{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69df502a-4c15-4a80-804d-258327cfab0b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### The case for distributed computing\n",
    "\n",
    "* What happens if the data to be analyzed is too large?\n",
    "  * e.g. cannot be stored on a single machine\n",
    "\n",
    "* What if the computation is too complex?\n",
    "  * e.g., in interactive mode, it is unacceptably slow\n",
    "\n",
    "* What if you have to deal with both situations?\n",
    "\n",
    "* Can you scale up? Can you scale out?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d14eef8e-7f19-4946-9d0f-20884122ba8b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Scaling Up: Local Machine\n",
    "\n",
    "* Scaling up occurs within the same system hosting the data and running the computaiton\n",
    "  * Simple to carry out from a physical standpoint\n",
    "  * from a programmatic standpoint, it's managed by the operating system and programming libraries; does not require additional frameworks\n",
    "* Scabaility is typically limited by the OS physical resources on the system.\n",
    "  * RAM upper bound is determined by the OS  or the number of slots available on the motherboard\n",
    "* The cost of a single machine at the highest configuration may be prohibitive\n",
    "\n",
    "* May not meet the demands of the workload at hand\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f127b161-c2c5-42de-bf9c-1d6aac2af7f9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Distributed Systems\n",
    "\n",
    "* The hardware specs may not be the same across machines, adding another layer of complexity if it doesn't\n",
    "\n",
    "<img src=\"https://www.dropbox.com/s/8mncw4ffe8uajol/networking.jpg?dl=1\" width=\"700\" height=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b4c13ab-adc4-4240-a9ed-0fb81aa7f829",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Key Requirements for Distributed Systems\n",
    "\n",
    "* A distributed system needs to meet several crucial criteria, including:\n",
    "\n",
    "  * Node Communication: Enabling seamless interaction among nodes in the network.\n",
    "  * Resilience to Faults: Ensuring that both data and operations remain unaffected by system failures.\n",
    "  * Scalability: The ability to scale computational resources to handle growing workloads.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf345b2d-f0eb-4e0f-81bb-04098a0a293e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Apache Spark\n",
    "\n",
    "* Apache Spark is an open-source, distributed processing system used for big data workloads.\n",
    "  * Runs on a cluster\n",
    "\n",
    "* It's an enhancement to Hadoop's MapReduce\n",
    "  * Processes and retains data in memory for subsequent steps\n",
    "  * For smaller workloads, Sparkâ€™s data processing speeds are up to 100x faster than Hadoop's MapReduce\n",
    "\n",
    "* Written in Scala and runs in the JVM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18f11d85-533e-4e25-90bf-9fcce5c39162",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Apache Spark\n",
    "\n",
    "* Ideal for real-time processing as it utilizes in-memory caching and optimized query execution for fast queries against data of any size.  \n",
    "\n",
    "* Provides a richer ecosystem of functionality\n",
    "  * Over 80 high level operators beyond Map and Reduce\n",
    "    * Tools for pipeline construction and evaluation\n",
    "  * compared to Hadoop, Spark provides more operators other than map and reduce\n",
    "    * Includes libraries to support SQL queries, machine learning (MLlib), graph data analysis (GraphX) and streaming data analysis\n",
    "  * Plethora of functions for SQL-like operation, ML and working with graph data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "048bfbef-984c-4209-a67e-13d5d2c639ac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### What is Apache Spark\n",
    "\n",
    "* [See Video](https://www.databricks.com/spark/about)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf96282e-53bf-4663-b90c-02918cfae9db",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Spark and Functional Programming\n",
    "\n",
    "* To manipulate data, Spark uses functional programming\n",
    "  * The functional programming paradigm is used in many popular languages including Common Lisp, Scheme, Clojure, OCaml, and Haskell\n",
    "  \n",
    "* Functional programming is a data oriented paradigm\n",
    "  * Decomposes a problem into a set of functions.\n",
    "  * Logic is implemented by applying and composing functions.\n",
    "\n",
    "* The idea is that functions should be able to manipulate data without maintaining any external state.\n",
    "  * No external or global variables\n",
    " \n",
    "* In functional programming, we need to always return new data instead of manipulating the data in-place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef54bd37-5133-436b-8da7-7e302c6ebd9c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Spark Operations and RDDs: An Overview\n",
    "\n",
    "What are RDDs? \n",
    " * Resilient Distributed Datasets is the data representation in Spark. \n",
    "   * an RDD is conceptually divided into one or more partitions. \n",
    "     Each partition is a self-contained unit of data that can be operated on independently and in parallel. \n",
    "     These partitions are distributed across the nodes in a Spark cluster for parallel computation.\n",
    "\n",
    "  * RDDs are read-only collections, partitioned across multiple computing nodes for optimized performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a53aff6-07df-4fc6-ace6-6a9a31351200",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Spark Operations and RDDs: An Overview\n",
    "  * Each partition is also replicated across nodes.\n",
    "    * Number of replicates is a configuration parameter.\n",
    "  * Partitioning enhances fault tolerance and boosts the efficiency of data operations.\n",
    "  * This allows RDDs to be accessed through parallel operations\n",
    "    * Data operations can be executed on all partitions at the same time, speeding up data tasks.\n",
    "    \n",
    "* In-Memory Caching\n",
    "  * RDDs are stored in memory -- if the RDD can fit into a node's memory -- facilitating quick iterations over the same dataset.\n",
    "    * Disk Spilling: If an RDD is too large Spark spills the what doesn't fit in RAM to disk\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82a09076-0a99-4e0b-a59a-b5a1a9c89419",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Overview of Apache Spark's Core Components\n",
    "\n",
    "* Spark Core: The foundation of the entire Spark ecosystem.\n",
    "  * Defines the basic data structure for (RDDs).\n",
    "  * Provides a set of operations (transformations) and actions to process RDDs.\n",
    "  * Enables distributed data processing, fault tolerance, and in-memory computations.\n",
    "\n",
    "* Spark SQL: Spark's SQL engine for structured data.\n",
    "  * Supports ANSI SQL standards for query language.\n",
    "  * Transforms SQL queries into Spark operations.\n",
    "  * Allows for SQL-like querying on large datasets, bridging the gap between traditional databases and big data processing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21b3e958-4c48-4874-a693-aef5b3c487a0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Overview of Apache Spark's Core Components - Cont'd\n",
    "\n",
    "* Spark Streaming: Real-time data processing module in Spark.\n",
    "  * Processes live streaming data.\n",
    "  * Offers seamless integration with other Spark components.\n",
    "  * Enables real-time analytics and data processing, vital for applications like fraud detection, monitoring, and recommendation systems.\n",
    "\n",
    "* MLlib: Spark's machine learning library.\n",
    "  * Implements machine learning algorithms on RDDs.\n",
    "  * Provides algorithms for classification, regression, clustering, and more.\n",
    "  * Allows for scalable machine learning tasks, leveraging Spark's distributed computing power.\n",
    "\n",
    "* GraphX: Spark's library for graph processing.\n",
    "  * Manages and manipulates graph structures.\n",
    "  * Performs parallel graph operations and computations.\n",
    "  * Enables graph analytics at scale, useful for social network analysis, recommendation systems, and more.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a83ee9cc-102d-4f0b-a3f8-ea04abe69c6f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Core Components of Spark\n",
    "<img src=\"https://www.dropbox.com/s/azebxe8nv5nsqne/spark_architecture.png?dl=1\" width=\"900\" height=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5cecd85f-8052-4735-9899-848545ba2573",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Spark High-level Components\n",
    "\n",
    "* Cluster manager: Manages the resources across the Spark cluster.\n",
    "  * Responsible for allocating resources like CPU and memory to Spark applications.\n",
    "  \n",
    "* Application driver: The central orchestrator of the Spark program, housing the main application logic.\n",
    "  * Contains an isntance of the spark context\n",
    "    * Requests resources from the Cluster Manager to launch executors.\n",
    "    * Coordinates the overall data processing workflow.\n",
    "  \n",
    "* Executors (workers): These are the worker nodes to which tasks are delegated.\n",
    "  * They execute the code sent by the driver, specifically focusing on their designated partitions of the dataset.\n",
    "  * Executors communicate with the Cluster Manager to report status and failures.\n",
    "\n",
    "\n",
    "![](https://spark.apache.org/docs/latest/img/cluster-overview.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c751e77-497a-4e72-b695-2e9894ce838e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Spark Program Flow\n",
    "\n",
    "* A typical Spark program adheres to the following structure:\n",
    "  * Application Driver: Central point of the Spark program, where the main application logic resides. \n",
    "    * Responsible for coordinating the entire data processing workflow.\n",
    "  * Executors (Workers): The worker nodes that tasks are delegated to.\n",
    "  \n",
    "    * Rxecuting the code sent by the driver, specifically focusing their designated partitions of the dataset.\n",
    "     * Driver send smaller, more specific operations that the executors can carry out.\n",
    "       * Referred to as a task plan.\n",
    "  * Result Aggregation: results sent by the executor to the application driver for aggregation\n",
    "    * Often a final layer of computation to produce the output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "caa78be3-1059-402b-9a4a-8aa64392708f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Spark Application Lifecycle\n",
    "\n",
    "1. Python Program Initialization\n",
    "    * The `SparkContext` object is created; it's a client to interact with the Spark cluster.\n",
    "  \n",
    "2. Resource Request to Cluster Manager:\n",
    "    * `SparkContext` contacts the Cluster Manager to request resources (CPU, memory) for your application.\n",
    "  \n",
    "3. Cluster Manager Allocates Resources:\n",
    "    * The Cluster Manager allocates the necessary resources for the application and decides where to place the executors across the cluster's nodes.\n",
    "  \n",
    "4. Application Driver Initialization\n",
    "    * The main logic of your Spark application is in the Application Driver.\n",
    "    * It becomes the master node for your application, coordinating tasks between the cluster and your Python program.\n",
    "5. Executor Launch\n",
    "    * Based on the resources allocated by the Cluster Manager, executors for the Spark application are launched on the worker nodes.\n",
    "6. Task Division and Execution Plan\n",
    "    * The Application Driver divides the job into tasks and builds an execution plan.\n",
    "7. Sending Task Plans to Executors\n",
    "    * Instead of sending raw code, the Application Driver sends the execution plan (or task plans) to the Executors.\n",
    "8. Task Execution on Executors\n",
    "    * Executors run the tasks on their designated partition of the dataset.\n",
    "9. Result Aggregation\n",
    "    * After task execution, the Executors send the results back to the Application Driver.\n",
    "10. Final Computation and Output Retrieval\n",
    "    * The Application Driver may perform some final computations.\n",
    "    * Results are returned to your Python program through the `SparkContext` object.\n",
    "11. Resource Release\n",
    "    * Once the application completes, the resources are released back to the Cluster Manager for use by other applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71997d80-647d-4a9b-80d7-9eed2733ac03",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Setting up a Docker Cluster\n",
    " \n",
    "* Installing Spark and all its components manually can be challenging and time-consuming.\n",
    "  * Configuring and optimizing a Spark cluster from scratch requires substantial effort.\n",
    "* Easier deployment options are available on cloud services, such as:\n",
    "  * [Amazon's EMR](https://aws.amazon.com/emr/features/spark/) for Spark support\n",
    "  * [Databricks' Community Edition](https://www.databricks.com/product/faq/community-edition) and paid offerings\n",
    "  * Other providers like Google's Dataproc, Microsoft's HDInsight, etc.\n",
    "\n",
    "* We'll utilize Databricks Community Edition.\n",
    "  [Brief Demo](www.databricks.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d13a01f5-e404-4c42-a6c7-cd1994c23166",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Installing Via Docker For ICS438 (Optional)\n",
    "\n",
    "* Note that the following is optional. All the work we will using pySpark will be done on a Databricks cluster in community edition. Also, this solutions assumes that you have docker running on your machine.\n",
    "\n",
    "* It is easy to use Docker to install locally. We will use the following Docker image\n",
    "  \n",
    "```  \n",
    "jupyter/all-spark-notebook\n",
    "```\n",
    "* There are other docker images, including (jupyter/pyspark-notebook), which does not include the jobs dashboard `http://localhost:4040`\n",
    "\n",
    "* We will run the infrastructure as follows:\n",
    "\n",
    "```\n",
    "docker run --rm -p 4040:4040 -p 8888:8888 -v $(pwd):/home/jovyan/work jupyter/all-spark-notebook\n",
    "```\n",
    "\n",
    "* This configuration created a master and compute nodes locally in a docker instance\n",
    " \n",
    "* While you're probably not going to need to, you can log into the running container using: `docker exec -it <CONTAINER_ID> bash`\n",
    "\n",
    " * where <CONTAINER_ID> of the container currently running the `jupyter/all-spark-notebook` image\n",
    "\n",
    "\n",
    "* The Docker instance has all the libraries installed and ready to go.\n",
    "\n",
    "* Make sure you run a Jupyer notebook on the Docker instnac\n",
    "  * If the code below fails, this means you're not running in the Docker instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "503e7db6-97e9-4c43-af09-c95fab6d7fca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=3007032361860494#setting/sparkui/0921-190855-7h5pqrfm/driver-4399418642160686734\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=3007032361860494#setting/sparkui/0921-190855-7h5pqrfm/driver-4399418642160686734\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# pip install pyspark\n",
    "\n",
    "from pyspark import SparkContext\n",
    "# sc = SparkContext()\n",
    "sc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e24d0ec7-0f4f-43ba-aa5b-e4c7b70fbc87",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# help(SparkContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46de1c98-8b14-4a85-91f9-bfbf8b3b8d88",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version is 3.4.1\n",
      "Python version is 3.10\n",
      "The name of the master is local[8]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Spark version is {sc.version}\")\n",
    "\n",
    "print(f\"Python version is {sc.pythonVer}\")\n",
    "\n",
    "print(f\"The name of the master is {sc.master}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67326381-b032-4b46-8659-92f9cf466937",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.databricks.preemption.enabled', 'true'),\n",
       " ('spark.sql.hive.metastore.jars', '/databricks/databricks-hive/*'),\n",
       " ('spark.driver.tempDirectory', '/local_disk0/tmp'),\n",
       " ('spark.sql.warehouse.dir', 'dbfs:/user/hive/warehouse'),\n",
       " ('spark.databricks.managedCatalog.clientClassName',\n",
       "  'com.databricks.managedcatalog.ManagedCatalogClientImpl'),\n",
       " ('spark.databricks.credential.scope.fs.gs.auth.access.tokenProviderClassName',\n",
       "  'com.databricks.backend.daemon.driver.credentials.CredentialScopeGCPTokenProvider'),\n",
       " ('spark.hadoop.fs.fcfs-s3.impl.disable.cache', 'true'),\n",
       " ('spark.sql.streaming.checkpointFileManagerClass',\n",
       "  'com.databricks.spark.sql.streaming.DatabricksCheckpointFileManager'),\n",
       " ('spark.databricks.service.dbutils.repl.backend',\n",
       "  'com.databricks.dbconnect.ReplDBUtils'),\n",
       " ('spark.hadoop.databricks.s3.verifyBucketExists.enabled', 'false'),\n",
       " ('spark.streaming.driver.writeAheadLog.allowBatching', 'true'),\n",
       " ('spark.databricks.clusterSource', 'UI'),\n",
       " ('spark.hadoop.hive.server2.transport.mode', 'http'),\n",
       " ('spark.executor.memory', '8278m'),\n",
       " ('spark.hadoop.fs.cpfs-adl.impl.disable.cache', 'true'),\n",
       " ('spark.databricks.clusterUsageTags.hailEnabled', 'false'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED --add-opens=java.management/sun.management=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=512m -XX:+UseCodeCacheFlushing -XX:PerMethodRecompilationCutoff=-1 -XX:PerBytecodeRecompilationCutoff=-1 -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:-UseContainerSupport -XX:+PrintFlagsFinal -XX:+PrintGCDateStamps -XX:+PrintGCDetails -verbose:gc -Xss4m -Djava.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Djavax.net.ssl.sessionCacheSize=10000 -Dscala.reflect.runtime.disable.typetag.cache=true -Dcom.google.cloud.spark.bigquery.repackaged.io.netty.tryReflectionSetAccessible=true -Dlog4j2.formatMsgNoLookups=true -verbose:gc -Xloggc:/dev/stdout -verbose:class -XX:+UnlockDiagnosticVMOptions -XX:+LogVMOutput -XX:-DisplayVMOutput -XX:LogFile=/databricks/databricks_vm_pipe -Ddatabricks.vmLog.pipe=/databricks/databricks_vm_pipe -Ddatabricks.serviceName=spark-executor-1'),\n",
       " ('spark.databricks.clusterUsageTags.clusterLogDeliveryEnabled', 'false'),\n",
       " ('spark.databricks.clusterUsageTags.containerType', 'LXC'),\n",
       " ('spark.hadoop.fs.s3a.assumed.role.credentials.provider',\n",
       "  'shaded.databricks.org.apache.hadoop.fs.s3a.DatabricksInstanceProfileCredentialsProvider'),\n",
       " ('spark.eventLog.enabled', 'false'),\n",
       " ('spark.databricks.clusterUsageTags.isIMv2Enabled', 'false'),\n",
       " ('spark.hadoop.fs.stage.impl.disable.cache', 'true'),\n",
       " ('spark.databricks.clusterUsageTags.containerZoneId', 'us-west-2a'),\n",
       " ('spark.hadoop.hive.hmshandler.retry.interval', '2000'),\n",
       " ('spark.executor.tempDirectory', '/local_disk0/tmp'),\n",
       " ('spark.hadoop.fs.azure.authorization.caching.enable', 'false'),\n",
       " ('spark.hadoop.fs.fcfs-abfss.impl',\n",
       "  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n",
       " ('spark.hadoop.mapred.output.committer.class',\n",
       "  'com.databricks.backend.daemon.data.client.DirectOutputCommitter'),\n",
       " ('spark.hadoop.hive.server2.thrift.http.port', '10000'),\n",
       " ('spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version', '2'),\n",
       " ('spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2S3', '0'),\n",
       " ('spark.sql.allowMultipleContexts', 'false'),\n",
       " ('spark.databricks.eventLog.enabled', 'true'),\n",
       " ('spark.home', '/databricks/spark'),\n",
       " ('spark.databricks.clusterUsageTags.clusterTargetWorkers', '0'),\n",
       " ('spark.hadoop.hive.server2.idle.operation.timeout', '7200000'),\n",
       " ('spark.task.reaper.enabled', 'true'),\n",
       " ('spark.databricks.clusterUsageTags.autoTerminationMinutes', '60'),\n",
       " ('spark.storage.memoryFraction', '0.5'),\n",
       " ('eventLog.rolloverIntervalSeconds', '900'),\n",
       " ('spark.databricks.clusterUsageTags.orgId', '3007032361860494'),\n",
       " ('spark.databricks.clusterUsageTags.clusterFirstOnDemand', '0'),\n",
       " ('spark.databricks.sql.configMapperClass',\n",
       "  'com.databricks.dbsql.config.SqlConfigMapperBridge'),\n",
       " ('spark.driver.maxResultSize', '4g'),\n",
       " ('spark.databricks.clusterUsageTags.sparkEnvVarContainsNewline', 'false'),\n",
       " ('spark.databricks.clusterUsageTags.driverPublicDns',\n",
       "  'ec2-35-87-178-103.us-west-2.compute.amazonaws.com'),\n",
       " ('spark.hadoop.fs.fcfs-s3.impl',\n",
       "  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n",
       " ('spark.databricks.delta.multiClusterWrites.enabled', 'true'),\n",
       " ('spark.databricks.clusterUsageTags.clusterAllTags',\n",
       "  '[{\"key\":\"Name\",\"value\":\"ce-worker\"},{\"key\":\"WorkspaceId\",\"value\":\"3007032361860494\"},{\"key\":\"ClusterId\",\"value\":\"0919-181904-pimvwda3\"}]'),\n",
       " ('spark.worker.cleanup.enabled', 'false'),\n",
       " ('spark.sql.legacy.createHiveTableByDefault', 'false'),\n",
       " ('spark.ui.port', '40001'),\n",
       " ('spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2File', '0'),\n",
       " ('spark.hadoop.fs.fcfs-s3a.impl.disable.cache', 'true'),\n",
       " ('spark.databricks.clusterUsageTags.driverContainerId',\n",
       "  '5fb25bb8daf34cd682e1ca99d06f6c88'),\n",
       " ('spark.hadoop.fs.s3a.attempts.maximum', '10'),\n",
       " ('spark.databricks.clusterUsageTags.enableCredentialPassthrough', 'false'),\n",
       " ('spark.databricks.clusterUsageTags.sparkEnvVarContainsDollarSign', 'false'),\n",
       " ('spark.databricks.clusterUsageTags.userProvidedRemoteVolumeType',\n",
       "  'ebs_volume_type: GENERAL_PURPOSE_SSD\\n'),\n",
       " ('spark.databricks.clusterUsageTags.enableJdbcAutoStart', 'true'),\n",
       " ('spark.hadoop.fs.azure.user.agent.prefix', ''),\n",
       " ('spark.hadoop.fs.s3n.impl.disable.cache', 'true'),\n",
       " ('spark.databricks.clusterUsageTags.enableGlueCatalogCredentialPassthrough',\n",
       "  'false'),\n",
       " ('spark.hadoop.fs.fcfs-s3n.impl',\n",
       "  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n",
       " ('spark.hadoop.fs.abfs.impl',\n",
       "  'com.databricks.common.filesystem.LokiFileSystem'),\n",
       " ('spark.hadoop.fs.s3a.retry.throttle.interval', '500ms'),\n",
       " ('spark.hadoop.fs.wasb.impl.disable.cache', 'true'),\n",
       " ('spark.databricks.clusterUsageTags.clusterLogDestination', ''),\n",
       " ('spark.databricks.wsfsPublicPreview', 'true'),\n",
       " ('spark.cleaner.referenceTracking.blocking', 'false'),\n",
       " ('spark.databricks.clusterUsageTags.isSingleUserCluster', 'false'),\n",
       " ('spark.databricks.clusterUsageTags.clusterState', 'Pending'),\n",
       " ('spark.databricks.clusterUsageTags.sparkEnvVarContainsSingleQuotes',\n",
       "  'false'),\n",
       " ('spark.app.startTime', '1695147842977'),\n",
       " ('spark.databricks.tahoe.logStore.azure.class',\n",
       "  'com.databricks.tahoe.store.AzureLogStore'),\n",
       " ('spark.hadoop.fs.azure.skip.metrics', 'true'),\n",
       " ('spark.hadoop.hive.hmshandler.retry.attempts', '10'),\n",
       " ('spark.hadoop.fs.wasb.impl',\n",
       "  'com.databricks.common.filesystem.LokiFileSystem'),\n",
       " ('spark.scheduler.mode', 'FAIR'),\n",
       " ('spark.sql.sources.default', 'delta'),\n",
       " ('spark.databricks.unityCatalog.credentialManager.tokenRefreshEnabled',\n",
       "  'true'),\n",
       " ('spark.hadoop.fs.cpfs-s3n.impl',\n",
       "  'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem'),\n",
       " ('spark.databricks.clusterUsageTags.clusterWorkers', '0'),\n",
       " ('spark.hadoop.fs.cpfs-adl.impl',\n",
       "  'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem'),\n",
       " ('spark.hadoop.fs.fcfs-s3n.impl.disable.cache', 'true'),\n",
       " ('spark.hadoop.fs.cpfs-abfss.impl',\n",
       "  'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem'),\n",
       " ('spark.databricks.rocksDB.fileManager.useCommitService', 'false'),\n",
       " ('spark.databricks.passthrough.oauth.refresher.impl',\n",
       "  'com.databricks.backend.daemon.driver.credentials.OAuthTokenRefresherClient'),\n",
       " ('spark.sql.hive.metastore.sharedPrefixes',\n",
       "  'org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks'),\n",
       " ('spark.databricks.io.directoryCommit.enableLogicalDelete', 'false'),\n",
       " ('spark.task.reaper.killTimeout', '60s'),\n",
       " ('spark.hadoop.parquet.block.size.row.check.min', '10'),\n",
       " ('spark.hadoop.hive.server2.use.SSL', 'true'),\n",
       " ('spark.hadoop.spark.databricks.metrics.filesystem_metrics', 'true'),\n",
       " ('spark.databricks.clusterUsageTags.clusterAvailability', 'ON_DEMAND'),\n",
       " ('spark.databricks.clusterUsageTags.userProvidedRemoteVolumeSizeGb', '0'),\n",
       " ('spark.hadoop.hive.server2.keystore.path',\n",
       "  '/databricks/keys/jetty-ssl-driver-keystore.jks'),\n",
       " ('spark.hadoop.fs.gs.impl',\n",
       "  'com.databricks.common.filesystem.LokiFileSystem'),\n",
       " ('spark.databricks.clusterUsageTags.driverInstanceId', 'i-061c720ef053a70cb'),\n",
       " ('spark.databricks.credential.redactor',\n",
       "  'com.databricks.logging.secrets.CredentialRedactorProxyImpl'),\n",
       " ('spark.app.id', 'local-1695147856246'),\n",
       " ('spark.databricks.clusterUsageTags.clusterPinned', 'false'),\n",
       " ('spark.databricks.acl.provider',\n",
       "  'com.databricks.sql.acl.ReflectionBackedAclProvider'),\n",
       " ('spark.databricks.mlflow.autologging.enabled', 'true'),\n",
       " ('spark.extraListeners',\n",
       "  'com.databricks.backend.daemon.driver.DBCEventLoggingListener'),\n",
       " ('spark.databricks.clusterUsageTags.driverInstancePrivateIp',\n",
       "  '10.172.173.68'),\n",
       " ('spark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.enabled',\n",
       "  'false'),\n",
       " ('spark.sql.parquet.cacheMetadata', 'true'),\n",
       " ('spark.databricks.clusterUsageTags.numPerGlobalInitScriptsV2', '0'),\n",
       " ('spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Abfss', '0'),\n",
       " ('spark.hadoop.parquet.abfs.readahead.optimization.enabled', 'true'),\n",
       " ('spark.hadoop.fs.cpfs-abfss.impl.disable.cache', 'true'),\n",
       " ('spark.databricks.clusterUsageTags.userProvidedSparkVersion',\n",
       "  '13.3.x-scala2.12'),\n",
       " ('spark.hadoop.fs.abfss.impl',\n",
       "  'com.databricks.common.filesystem.LokiFileSystem'),\n",
       " ('spark.databricks.clusterUsageTags.enableLocalDiskEncryption', 'false'),\n",
       " ('spark.databricks.tahoe.logStore.class',\n",
       "  'com.databricks.tahoe.store.DelegatingLogStore'),\n",
       " ('spark.hadoop.fs.s3.impl.disable.cache', 'true'),\n",
       " ('spark.hadoop.spark.hadoop.aws.glue.cache.db.ttl-mins', '30'),\n",
       " ('spark.hadoop.spark.hadoop.aws.glue.cache.table.ttl-mins', '30'),\n",
       " ('libraryDownload.sleepIntervalSeconds', '5'),\n",
       " ('spark.databricks.cloudProvider', 'AWS'),\n",
       " ('spark.sql.hive.convertMetastoreParquet', 'true'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.databricks.service.dbutils.server.backend',\n",
       "  'com.databricks.dbconnect.SparkServerDBUtils'),\n",
       " ('spark.databricks.clusterUsageTags.workerEnvironmentId',\n",
       "  'default-worker-env'),\n",
       " ('spark.databricks.repl.enableClassFileCleanup', 'true'),\n",
       " ('spark.hadoop.fs.s3a.multipart.size', '10485760'),\n",
       " ('spark.databricks.clusterUsageTags.cloudProvider', 'AWS'),\n",
       " ('spark.metrics.conf', '/databricks/spark/conf/metrics.properties'),\n",
       " ('spark.databricks.clusterUsageTags.effectiveSparkVersion',\n",
       "  '13.3.x-scala2.12'),\n",
       " ('spark.akka.frameSize', '256'),\n",
       " ('spark.hadoop.fs.s3a.fast.upload', 'true'),\n",
       " ('spark.sql.streaming.stopTimeout', '15s'),\n",
       " ('spark.hadoop.hive.server2.keystore.password', '[REDACTED]'),\n",
       " ('spark.databricks.clusterUsageTags.clusterName', 'TempCluster'),\n",
       " ('spark.databricks.clusterUsageTags.ignoreTerminationEventInAlerting',\n",
       "  'false'),\n",
       " ('spark.hadoop.fs.s3a.retry.interval', '250ms'),\n",
       " ('spark.databricks.clusterUsageTags.sparkEnvVarContainsEscape', 'false'),\n",
       " ('spark.databricks.overrideDefaultCommitProtocol',\n",
       "  'org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol'),\n",
       " ('spark.worker.aioaLazyConfig.dbfsReadinessCheckClientClass',\n",
       "  'com.databricks.backend.daemon.driver.NephosDbfsReadinessCheckClient'),\n",
       " ('spark.databricks.clusterUsageTags.clusterNoDriverDaemon', 'false'),\n",
       " ('spark.hadoop.fs.adl.impl',\n",
       "  'com.databricks.common.filesystem.LokiFileSystem'),\n",
       " ('libraryDownload.timeoutSeconds', '180'),\n",
       " ('spark.hadoop.parquet.memory.pool.ratio', '0.5'),\n",
       " ('spark.databricks.clusterUsageTags.clusterScalingType', 'fixed_size'),\n",
       " ('spark.databricks.passthrough.adls.gen2.tokenProviderClassName',\n",
       "  'com.databricks.backend.daemon.data.client.adl.AdlGen2CredentialContextTokenProvider'),\n",
       " ('spark.databricks.unityCatalog.legacy.enableCrossScopeCredCache', 'true'),\n",
       " ('spark.hadoop.fs.s3a.block.size', '67108864'),\n",
       " ('spark.databricks.sparkContextId', '7279778408176838596'),\n",
       " ('spark.databricks.tahoe.logStore.gcp.class',\n",
       "  'com.databricks.tahoe.store.GCPLogStore'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.databricks.clusterUsageTags.sparkMasterUrlType', 'None'),\n",
       " ('spark.sql.sources.commitProtocolClass',\n",
       "  'com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol'),\n",
       " ('spark.repl.class.outputDir',\n",
       "  '/local_disk0/tmp/repl/spark-7279778408176838596-0f64e236-36fb-4a2e-bf2f-895fe8fe8919'),\n",
       " ('spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Gcs', '0'),\n",
       " ('spark.hadoop.fs.fcfs-s3a.impl',\n",
       "  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n",
       " ('spark.databricks.clusterUsageTags.attribute_tag_budget', ''),\n",
       " ('spark.databricks.clusterUsageTags.clusterPythonVersion', '3'),\n",
       " ('spark.databricks.clusterUsageTags.enableDfAcls', 'false'),\n",
       " ('spark.databricks.clusterUsageTags.userProvidedRemoteVolumeCount', '0'),\n",
       " ('spark.hadoop.databricks.loki.fileSystemCache.enabled', 'true'),\n",
       " ('spark.shuffle.service.enabled', 'true'),\n",
       " ('spark.hadoop.fs.file.impl',\n",
       "  'com.databricks.backend.daemon.driver.WorkspaceLocalFileSystem'),\n",
       " ('spark.plugins', 'org.apache.spark.sql.connect.SparkConnectPlugin'),\n",
       " ('spark.hadoop.fs.fcfs-wasb.impl.disable.cache', 'true'),\n",
       " ('spark.hadoop.fs.cpfs-s3.impl',\n",
       "  'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem'),\n",
       " ('spark.databricks.clusterUsageTags.attribute_tag_dust_maintainer', ''),\n",
       " ('spark.hadoop.fs.s3a.multipart.threshold', '104857600'),\n",
       " ('spark.rpc.message.maxSize', '256'),\n",
       " ('spark.databricks.clusterUsageTags.attribute_tag_dust_suite', ''),\n",
       " ('spark.hadoop.fs.fcfs-wasbs.impl',\n",
       "  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n",
       " ('spark.databricks.clusterUsageTags.clusterId', '0919-181904-pimvwda3'),\n",
       " ('spark.databricks.clusterUsageTags.clusterMetastoreAccessType',\n",
       "  'RDS_DIRECT'),\n",
       " ('spark.databricks.driverNfs.enabled', 'true'),\n",
       " ('spark.databricks.clusterUsageTags.ngrokNpipEnabled', 'false'),\n",
       " ('spark.hadoop.parquet.page.metadata.validation.enabled', 'true'),\n",
       " ('spark.databricks.clusterUsageTags.instanceProfileUsed', 'false'),\n",
       " ('spark.databricks.unityCatalog.credentialManager.apiTokenProviderClassName',\n",
       "  'com.databricks.unity.TokenServiceApiTokenProvider'),\n",
       " ('spark.databricks.passthrough.glue.executorServiceFactoryClassName',\n",
       "  'com.databricks.backend.daemon.driver.credentials.GlueClientExecutorServiceFactory'),\n",
       " ('spark.databricks.clusterUsageTags.awsWorkspaceIMDSV2EnablementStatus',\n",
       "  'false'),\n",
       " ('spark.databricks.acl.scim.client',\n",
       "  'com.databricks.spark.sql.acl.client.DriverToWebappScimClient'),\n",
       " ('spark.driver.host', '10.172.185.113'),\n",
       " ('spark.databricks.clusterUsageTags.sparkEnvVarContainsBacktick', 'false'),\n",
       " ('spark.hadoop.fs.adl.impl.disable.cache', 'true'),\n",
       " ('spark.hadoop.parquet.block.size.row.check.max', '10'),\n",
       " ('spark.hadoop.fs.s3a.connection.maximum', '200'),\n",
       " ('spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2', '0'),\n",
       " ('spark.hadoop.fs.s3a.fast.upload.active.blocks', '32'),\n",
       " ('spark.shuffle.reduceLocality.enabled', 'false'),\n",
       " ('spark.databricks.clusterUsageTags.driverNodeType', 'dev-tier-node'),\n",
       " ('spark.hadoop.spark.sql.sources.outputCommitterClass',\n",
       "  'com.databricks.backend.daemon.data.client.MapReduceDirectOutputCommitter'),\n",
       " ('spark.hadoop.fs.fcfs-abfs.impl',\n",
       "  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n",
       " ('spark.hadoop.databricks.loki.fileStatusCache.enabled', 'true'),\n",
       " ('spark.databricks.clusterUsageTags.instanceBootstrapType', 'ssh'),\n",
       " ('spark.hadoop.fs.fcfs-abfss.impl.disable.cache', 'true'),\n",
       " ('spark.hadoop.hive.server2.thrift.http.cookie.auth.enabled', 'false'),\n",
       " ('spark.hadoop.spark.hadoop.aws.glue.cache.table.size', '1000'),\n",
       " ('spark.databricks.driverNodeTypeId', 'dev-tier-node'),\n",
       " ('spark.sql.parquet.compression.codec', 'snappy'),\n",
       " ('spark.repl.class.uri', 'spark://10.172.185.113:45001/classes'),\n",
       " ('spark.hadoop.fs.stage.impl',\n",
       "  'com.databricks.backend.daemon.driver.managedcatalog.PersonalStagingFileSystem'),\n",
       " ('spark.databricks.credential.scope.fs.s3a.tokenProviderClassName',\n",
       "  'com.databricks.backend.daemon.driver.credentials.CredentialScopeS3TokenProvider'),\n",
       " ('spark.r.sql.derby.temp.dir', '/tmp/RtmpjFFsm4'),\n",
       " ('spark.databricks.cloudfetch.hasRegionSupport', 'true'),\n",
       " ('spark.hadoop.databricks.s3.create.deleteUnnecessaryFakeDirectories',\n",
       "  'false'),\n",
       " ('spark.hadoop.spark.hadoop.aws.glue.cache.db.size', '1000'),\n",
       " ('spark.databricks.workerNodeTypeId', 'dev-tier-node'),\n",
       " ('spark.databricks.passthrough.glue.credentialsProviderFactoryClassName',\n",
       "  'com.databricks.backend.daemon.driver.credentials.DatabricksCredentialProviderFactory'),\n",
       " ('spark.databricks.clusterUsageTags.clusterEbsVolumeSize', '0'),\n",
       " ('spark.sparklyr-backend.threads', '1'),\n",
       " ('spark.hadoop.fs.fcfs-wasb.impl',\n",
       "  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n",
       " ('spark.databricks.passthrough.s3a.tokenProviderClassName',\n",
       "  'com.databricks.backend.daemon.driver.aws.AwsCredentialContextTokenProvider'),\n",
       " ('spark.databricks.session.share', 'false'),\n",
       " ('spark.databricks.clusterUsageTags.clusterResourceClass', 'default'),\n",
       " ('spark.hadoop.fs.idbfs.impl', 'com.databricks.io.idbfs.IdbfsFileSystem'),\n",
       " ('spark.hadoop.fs.dbfs.impl',\n",
       "  'com.databricks.backend.daemon.data.client.DbfsHadoop3'),\n",
       " ('spark.databricks.clusterUsageTags.clusterSku', 'STANDARD_SKU'),\n",
       " ('spark.hadoop.fs.gs.impl.disable.cache', 'true'),\n",
       " ('spark.databricks.privateLinkEnabled', 'false'),\n",
       " ('spark.databricks.clusterUsageTags.clusterUnityCatalogMode', 'CUSTOM'),\n",
       " ('spark.delta.sharing.profile.provider.class',\n",
       "  'io.delta.sharing.DeltaSharingCredentialsProvider'),\n",
       " ('spark.worker.aioaLazyConfig.iamReadinessCheckClientClass',\n",
       "  'com.databricks.backend.daemon.driver.NephosIamRoleCheckClient'),\n",
       " ('spark.hadoop.fs.wasbs.impl',\n",
       "  'com.databricks.common.filesystem.LokiFileSystem'),\n",
       " ('spark.databricks.clusterUsageTags.clusterEbsVolumeType',\n",
       "  'GENERAL_PURPOSE_SSD'),\n",
       " ('spark.databricks.clusterUsageTags.clusterOwnerUserId', '1801394777137432'),\n",
       " ('spark.databricks.automl.serviceEnabled', 'true'),\n",
       " ('spark.hadoop.parquet.page.size.check.estimate', 'false'),\n",
       " ('spark.databricks.clusterUsageTags.attribute_tag_service', ''),\n",
       " ('spark.databricks.passthrough.s3a.threadPoolExecutor.factory.class',\n",
       "  'com.databricks.backend.daemon.driver.aws.S3APassthroughThreadPoolExecutorFactory'),\n",
       " ('spark.driver.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED --add-opens=java.management/sun.management=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
       " ('spark.databricks.delta.preview.enabled', 'true'),\n",
       " ('spark.databricks.metrics.filesystem_io_metrics', 'true'),\n",
       " ('spark.hadoop.spark.driverproxy.customHeadersToProperties',\n",
       "  'X-Databricks-User-Token:spark.databricks.token,X-Databricks-Non-UC-User-Token:spark.databricks.non.uc.token,X-Databricks-Api-Url:spark.databricks.api.url,X-Databricks-ADLS-Gen1-Token:spark.databricks.adls.gen1.token,X-Databricks-ADLS-Gen2-Token:spark.databricks.adls.gen2.token,X-Databricks-Synapse-Token:spark.databricks.synapse.token,X-Databricks-AWS-Credentials:spark.databricks.aws.creds,X-Databricks-User-Id:spark.databricks.user.id,X-Databricks-User-Name:spark.databricks.user.name,X-Databricks-Oauth-Identity-Custom-Claim:spark.databricks.oauthCustomIdentityClaims'),\n",
       " ('spark.databricks.unityCatalog.credentialScope.enabled', 'true'),\n",
       " ('spark.databricks.cloudfetch.requesterClassName',\n",
       "  'com.databricks.spark.sql.cloudfetch.DataDaemonCloudPresignedUrlRequester'),\n",
       " ('spark.master', 'local[8]'),\n",
       " ('spark.databricks.delta.logStore.crossCloud.fatal', 'true'),\n",
       " ('spark.databricks.driverNfs.clusterWidePythonLibsEnabled', 'true'),\n",
       " ('spark.files.fetchFailure.unRegisterOutputOnHost', 'true'),\n",
       " ('spark.databricks.clusterUsageTags.enableSqlAclsOnly', 'false'),\n",
       " ('spark.databricks.clusterUsageTags.clusterEbsVolumeCount', '0'),\n",
       " ('spark.databricks.clusterUsageTags.clusterSizeType', 'VM_CONTAINER'),\n",
       " ('spark.hadoop.databricks.fs.perfMetrics.enable', 'true'),\n",
       " ('spark.databricks.clusterUsageTags.clusterNumSshKeys', '0'),\n",
       " ('spark.hadoop.fs.gs.outputstream.upload.chunk.size', '16777216'),\n",
       " ('spark.databricks.tahoe.logStore.aws.class',\n",
       "  'com.databricks.tahoe.store.S3LockBasedLogStore'),\n",
       " ('spark.speculation.quantile', '0.9'),\n",
       " ('spark.databricks.clusterUsageTags.privateLinkEnabled', 'false'),\n",
       " ('spark.shuffle.manager', 'SORT'),\n",
       " ('spark.files.overwrite', 'true'),\n",
       " ('spark.databricks.credential.aws.secretKey.redactor',\n",
       "  'com.databricks.spark.util.AWSSecretKeyRedactorProxy'),\n",
       " ('spark.databricks.clusterUsageTags.clusterNumCustomTags', '0'),\n",
       " ('spark.connect.extensions.command.classes',\n",
       "  'io.delta.connect.DeltaCommandPlugin'),\n",
       " ('spark.hadoop.fs.s3.impl',\n",
       "  'com.databricks.common.filesystem.LokiFileSystem'),\n",
       " ('spark.hadoop.fs.s3a.impl.disable.cache', 'true'),\n",
       " ('spark.databricks.clusterUsageTags.sparkEnvVarContainsDoubleQuotes',\n",
       "  'false'),\n",
       " ('spark.r.numRBackendThreads', '1'),\n",
       " ('spark.hadoop.fs.wasbs.impl.disable.cache', 'true'),\n",
       " ('spark.hadoop.fs.abfss.impl.disable.cache', 'true'),\n",
       " ('spark.hadoop.fs.azure.cache.invalidator.type',\n",
       "  'com.databricks.encryption.utils.CacheInvalidatorImpl'),\n",
       " ('spark.sql.hive.metastore.version', '0.13.0'),\n",
       " ('spark.shuffle.service.port', '4048'),\n",
       " ('spark.databricks.clusterUsageTags.instanceWorkerEnvNetworkType', 'default'),\n",
       " ('spark.databricks.acl.client',\n",
       "  'com.databricks.spark.sql.acl.client.SparkSqlAclClient'),\n",
       " ('spark.streaming.driver.writeAheadLog.closeFileAfterWrite', 'true'),\n",
       " ('spark.hadoop.hive.warehouse.subdir.inherit.perms', 'false'),\n",
       " ('spark.databricks.clusterUsageTags.runtimeEngine', 'STANDARD'),\n",
       " ('spark.databricks.clusterUsageTags.isServicePrincipalCluster', 'false'),\n",
       " ('spark.databricks.credential.scope.fs.impl',\n",
       "  'com.databricks.sql.acl.fs.CredentialScopeFileSystem'),\n",
       " ('spark.databricks.enablePublicDbfsFuse', 'false'),\n",
       " ('spark.databricks.clusterUsageTags.enableElasticDisk', 'false'),\n",
       " ('spark.hadoop.fs.fcfs-wasbs.impl.disable.cache', 'true'),\n",
       " ('spark.databricks.clusterUsageTags.clusterNodeType', 'dev-tier-node'),\n",
       " ('spark.databricks.clusterUsageTags.clusterOwnerOrgId', '3007032361860494'),\n",
       " ('spark.databricks.clusterUsageTags.driverContainerPrivateIp',\n",
       "  '10.172.185.113'),\n",
       " ('spark.databricks.passthrough.adls.tokenProviderClassName',\n",
       "  'com.databricks.backend.daemon.data.client.adl.AdlCredentialContextTokenProvider'),\n",
       " ('spark.app.name', 'Databricks Shell'),\n",
       " ('spark.driver.allowMultipleContexts', 'false'),\n",
       " ('spark.hadoop.fs.AbstractFileSystem.gs.impl',\n",
       "  'shaded.databricks.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS'),\n",
       " ('spark.databricks.secret.sparkConf.keys.toRedact', ''),\n",
       " ('spark.rdd.compress', 'true'),\n",
       " ('spark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.throwsException',\n",
       "  'false'),\n",
       " ('spark.databricks.python.defaultPythonRepl', 'ipykernel'),\n",
       " ('spark.hadoop.fs.s3a.retry.limit', '6'),\n",
       " ('spark.databricks.clusterUsageTags.attribute_tag_dust_execution_env', ''),\n",
       " ('spark.databricks.eventLog.dir', 'eventlogs'),\n",
       " ('spark.databricks.credential.scope.fs.adls.gen2.tokenProviderClassName',\n",
       "  'com.databricks.backend.daemon.driver.credentials.CredentialScopeADLSTokenProvider'),\n",
       " ('spark.databricks.clusterUsageTags.isDpCpPrivateLinkEnabled', 'false'),\n",
       " ('spark.databricks.driverNfs.pathSuffix', '.ephemeral_nfs'),\n",
       " ('spark.databricks.clusterUsageTags.clusterCreator', 'Webapp'),\n",
       " ('spark.speculation', 'false'),\n",
       " ('spark.hadoop.databricks.dbfs.client.version', 'v1'),\n",
       " ('spark.hadoop.hive.server2.session.check.interval', '60000'),\n",
       " ('spark.sql.hive.convertCTAS', 'true'),\n",
       " ('spark.connect.extensions.relation.classes',\n",
       "  'io.delta.connect.DeltaRelationPlugin'),\n",
       " ('spark.hadoop.spark.sql.parquet.output.committer.class',\n",
       "  'org.apache.spark.sql.parquet.DirectParquetOutputCommitter'),\n",
       " ('spark.hadoop.fs.s3a.max.total.tasks', '1000'),\n",
       " ('spark.driver.port', '45001'),\n",
       " ('spark.hadoop.fs.s3a.fast.upload.default', 'true'),\n",
       " ('spark.databricks.clusterUsageTags.clusterGeneration', '0'),\n",
       " ('spark.hadoop.fs.mlflowdbfs.impl',\n",
       "  'com.databricks.mlflowdbfs.MlflowdbfsFileSystem'),\n",
       " ('spark.databricks.eventLog.listenerClassName',\n",
       "  'com.databricks.backend.daemon.driver.DBCEventLoggingListener'),\n",
       " ('spark.hadoop.fs.abfs.impl.disable.cache', 'true'),\n",
       " ('spark.speculation.multiplier', '3'),\n",
       " ('spark.storage.blockManagerTimeoutIntervalMs', '300000'),\n",
       " ('spark.databricks.clusterUsageTags.instanceWorkerEnvId',\n",
       "  'default-worker-env'),\n",
       " ('spark.databricks.clusterUsageTags.sparkVersion', '13.3.x-scala2.12'),\n",
       " ('spark.sparkr.use.daemon', 'false'),\n",
       " ('spark.scheduler.listenerbus.eventqueue.capacity', '20000'),\n",
       " ('spark.hadoop.fs.s3a.impl',\n",
       "  'com.databricks.common.filesystem.LokiFileSystem'),\n",
       " ('spark.databricks.clusterUsageTags.clusterStateMessage', 'Starting Spark'),\n",
       " ('spark.hadoop.parquet.page.write-checksum.enabled', 'true'),\n",
       " ('spark.hadoop.databricks.s3commit.client.sslTrustAll', 'false'),\n",
       " ('spark.hadoop.fs.s3a.threads.max', '136'),\n",
       " ('spark.r.backendConnectionTimeout', '604800'),\n",
       " ('spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Dbfs', '0'),\n",
       " ('spark.hadoop.fs.s3n.impl',\n",
       "  'com.databricks.common.filesystem.LokiFileSystem'),\n",
       " ('spark.hadoop.hive.server2.idle.session.timeout', '900000'),\n",
       " ('spark.databricks.redactor',\n",
       "  'com.databricks.spark.util.DatabricksSparkLogRedactorProxy'),\n",
       " ('spark.executor.extraClassPath',\n",
       "  '/databricks/spark/dbconf/log4j/executor:/databricks/spark/dbconf/jets3t/:/databricks/spark/dbconf/hadoop:/databricks/hive/conf:/databricks/jars/*'),\n",
       " ('spark.databricks.autotune.maintenance.client.classname',\n",
       "  'com.databricks.maintenanceautocompute.MACClientImpl'),\n",
       " ('spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Volumes', '0'),\n",
       " ('spark.databricks.clusterUsageTags.sparkImageLabel',\n",
       "  'release__13.3.x-snapshot-scala2.12__databricks-universe__13.3.1__05e0d5c__bade798__jenkins__3594b86__format-3'),\n",
       " ('spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Workspace',\n",
       "  '0'),\n",
       " ('spark.hadoop.fs.fcfs-abfs.impl.disable.cache', 'true'),\n",
       " ('spark.hadoop.parquet.page.verify-checksum.enabled', 'true'),\n",
       " ('spark.databricks.clusterUsageTags.dataPlaneRegion', 'us-west-2'),\n",
       " ('spark.logConf', 'true'),\n",
       " ('spark.databricks.clusterUsageTags.enableJobsAutostart', 'true'),\n",
       " ('spark.hadoop.hive.server2.enable.doAs', 'false'),\n",
       " ('spark.hadoop.parquet.filter.columnindex.enabled', 'false'),\n",
       " ('spark.databricks.clusterUsageTags.userId', '1801394777137432'),\n",
       " ('spark.shuffle.memoryFraction', '0.2'),\n",
       " ('spark.databricks.unityCatalog.volumes.fuse.server.enabled', 'true'),\n",
       " ('spark.hadoop.fs.dbfsartifacts.impl',\n",
       "  'com.databricks.backend.daemon.data.client.DBFSV1'),\n",
       " ('spark.hadoop.fs.cpfs-s3a.impl',\n",
       "  'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem'),\n",
       " ('spark.hadoop.fs.s3a.connection.timeout', '50000'),\n",
       " ('spark.databricks.secret.envVar.keys.toRedact', ''),\n",
       " ('spark.databricks.clusterUsageTags.region', 'us-west-2'),\n",
       " ('spark.databricks.clusterUsageTags.clusterSpotBidPricePercent', '100'),\n",
       " ('spark.files.useFetchCache', 'false')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31b4617d-715e-46b2-9cb7-88bc37c8570b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Creating a Text RDD in Spark\n",
    "\n",
    "* Resilient Distributed Datasets (RDDs) can be created in various ways in Spark. Here are two commonly used methods:\n",
    "\n",
    "    * `parallelize()`: This function allows you to transform Python collections, such as lists or arrays, into an RDD.\n",
    "        * It distributes the elements of the passed collection across multiple nodes, making the RDD fault-tolerant.\n",
    "\n",
    "    * `textFile()`: This function reads in a text file and creates an RDD where each object corresponds to a line in the file.\n",
    "\n",
    "* Example using `parallelize()`:\n",
    "  ```python\n",
    "  from pyspark import SparkContext\n",
    "  sc = SparkContext()\n",
    "  my_list = [1, 2, 3, 4, 5]\n",
    "  my_rdd = sc.parallelize(my_list)\n",
    "  # or\n",
    "  my_text_rdd = sc.textFile(\"path/to/text/file.txt\")\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e953f692-1d30-4e0c-b4b8-4eb1c1545d04",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Fundamental Operations on RDDs\n",
    "\n",
    "* Several fundamental operations are available to manipulate and transform Spark RDDs. \n",
    "  * These operations are generally called 'transformations.'\n",
    "    * `map`: Applies a given function to each element of the RDD and returns a new RDD consisting of the results.\n",
    "    * `filter`: Returns a new RDD containing only the elements that satisfy a given predicate.\n",
    "    * `reduce`: Aggregates the elements of the RDD using a given function, \n",
    "      * The function should be commutative and associative so that it can be computed in parallel.\n",
    "\n",
    "* Each transformation on an RDD produces a new RDD without modifying the original one, making RDDs immutable.\n",
    "\n",
    "* `flatMap`: Another commonly used transformation, which first applies a function to all elements of the RDD and then flattens the results. It is conceptually equivalent to Python's `itertools.chain()`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2257e1ca-bf9a-4faf-a06c-178a85510340",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[10]: ParallelCollectionRDD[177] at readRDDFromInputStream at PythonRDD.scala:435"
     ]
    }
   ],
   "source": [
    "my_rdd = sc.parallelize([1,2,3,4,5,6,7,8,9,10])\n",
    "my_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "027c111e-443d-4ec9-8011-6c469fac67e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99af2868-30db-4236-be93-746a67bec28b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68ff42d0-be18-43c5-9387-40e423636c5d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf8052a8-d865-4d95-bc17-69a80168f634",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [2], [3], [4, 5], [6], [7], [8], [9, 10]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark's default behavior is sufficient and often near-optimal.\n",
    "partitions_data = my_rdd.glom().collect()\n",
    "partitions_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ce025ac-2e6f-4284-b89b-73ae73b1bbd5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[12]: PythonRDD[178] at RDD at PythonRDD.scala:58"
     ]
    }
   ],
   "source": [
    "doubled_rdd = my_rdd.map(lambda x: x * 2)\n",
    "doubled_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56ffc358-7e05-4980-ab72-fe122b8e4e1d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[13]: [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]"
     ]
    }
   ],
   "source": [
    "doubled_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8eb69bb4-1ac9-4450-9621-b3d2f3e3cd8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[15]: PythonRDD[179] at RDD at PythonRDD.scala:58"
     ]
    }
   ],
   "source": [
    "even_rdd = my_rdd.filter(lambda x: x % 2 == 0)\n",
    "even_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "250e3f98-1c9a-49a8-b8f4-b65d6b885df4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[16]: [2, 4, 6, 8, 10]"
     ]
    }
   ],
   "source": [
    "even_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b66618f5-6761-4530-8d9a-ba7aa532c46b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sum_of_elements = my_rdd.reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73ea0fde-7310-428d-b1f8-77ddc8338332",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[18]: 55"
     ]
    }
   ],
   "source": [
    "sum_of_elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c8f195b-fc6f-4465-897f-36dfc1b5ab63",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mapped_rdd = my_rdd.map(lambda x: (x, x * 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dc9f577-491e-46d3-91b3-1b4a2fa44db5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 3),\n",
       " (2, 6),\n",
       " (3, 9),\n",
       " (4, 12),\n",
       " (5, 15),\n",
       " (6, 18),\n",
       " (7, 21),\n",
       " (8, 24),\n",
       " (9, 27),\n",
       " (10, 30)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapped_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f7cf592-b14b-448e-8c0b-4289a06603b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "flat_rdd = my_rdd.flatMap(lambda x: (x, x * 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a980fa9-2805-405d-aee8-4d5ba4df0d43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 2, 6, 3, 9, 4, 12, 5, 15, 6, 18, 7, 21, 8, 24, 9, 27, 10, 30]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b891cffa-a0f8-4183-a37c-c848c370a0be",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Transformations and Actions in Spark\n",
    "\n",
    "* Transformations: These operations transform your data and produce a new RDD. They come in two types:\n",
    "    * Narrow-dependency transformations:\n",
    "        * Transformations where each partition of the parent RDD is used to build only one partition of the new RDD, e.g., `map` and `filter`.\n",
    "        * This means the operation can be performed independently on each partition, which allows for better parallelism and less data movement.\n",
    "    * Wide-dependency transformations:\n",
    "        * Transformations where a single partition of the parent RDD may be used to build multiple partitions of the child RDD, e.g., `groupBy` and `reduceByKey`.\n",
    "        * These operations are generally expensive in terms of performance as they typically require shuffling, or  redistributing, data across partitions\n",
    "\n",
    "* Actions: These operations trigger the computation and execute the job, producing a result.\n",
    "    * Each action initiates a Spark job, which may consist of multiple stages depending on the transformations involved.\n",
    "\n",
    "#### Why Distinguish Between Transformations and Actions?\n",
    "\n",
    "  * One reason is query optimization. For instance, performing a `filter` operation before a `groupBy` is usually more efficient than doing it afterward.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9bcc6c69-2122-43d5-aafa-91fedc5fb253",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "* Before Join\n",
    "\n",
    "* Partition 1 of RDD1: \n",
    "```\n",
    "(1, \"apple\")\n",
    "(2, \"banana\")\n",
    "```\n",
    "* Partition 2 of RDD1: \n",
    "```\n",
    "(3, \"cherry\")\n",
    "(4, \"date\")\n",
    "```\n",
    "  \n",
    "* Partition 1 of RDD2: \n",
    "```\n",
    "(3, \"red\")\n",
    "```\n",
    "* Partition 2 of RDD2: \n",
    "```\n",
    "(4, \"brown\")\n",
    "(2, \"yellow\")\n",
    "```\n",
    "\n",
    "Compute a jon between RDD1 and RDD2 based on the keys\n",
    "1 . shuffle the data around to group all occurrences of the same key together. \n",
    "* Data after shuffle\n",
    "\n",
    "  * Shuffle output targeting Partition 1 (from RDD1 and RDD2):\n",
    "    ```\n",
    "    (2, \"banana\")\n",
    "    (2, \"yellow\")\n",
    "    ```\n",
    "    \n",
    "  * Shuffle output targeting Partition 2 (from RDD1 and RDD2):\n",
    "    ```\n",
    "    (3, \"cherry\")\n",
    "    (3, \"red\")\n",
    "    (4, \"date\")\n",
    "    (4, \"brown\")\n",
    "    ```\n",
    "\n",
    "* Data Partitions After Join\n",
    "\n",
    "* Partition 1: \n",
    "```\n",
    "(2, (\"banana\", \"yellow\"))\n",
    "```\n",
    "* Partition 2\n",
    "```\n",
    "(3, (\"cherry\", \"red\")), (4, (\"date\", \"brown\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f8a7e3b-c7b3-4c80-8c02-b7216216b444",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Understanding the Concept of a Stage in Spark\n",
    "\n",
    "* A \"stage\" is a sequence of transformations that can be executed in parallel.\n",
    "* Stages are separated by operations that require data to be rearranged, known as \"wide dependencies.\" \n",
    "  * Managing this complexity is easier when tasks are grouped into stages.\n",
    "* Each stage either reads data, performs computations, or writes data.\n",
    "\n",
    "* Stages are executed in sequence, one after the other.\n",
    "  * Within each stage, tasks are executed in parallel.\n",
    "* Computation moves to where the data resides.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "140b46ff-e7c8-4e0c-882b-1496bd0265c1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Markdown block with example query: Decomposition into Stages\n",
    " * Note that the data is not provided to run this code.\n",
    "\n",
    "```python\n",
    "flights_df = spark.read.option(\"head\", \"true\").option(\"inferSchema\", \"true\").csv(\"flights_info.csv\")\n",
    "flights_data_partitioned_df = flights_data.repartition(minPartitions=4)\n",
    "\n",
    "counts_df = flights_df.where(\"duration > 120\")\n",
    "                                       .select(\"dep\", \"dest\", \"carrier\", \"durations\")\n",
    "                                       .groupBy(\"carrier\")\n",
    "                                       .count()\n",
    "counts_df.collect()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c640c588-cb2f-495d-ab73-1a207cdac2cd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Stages\n",
    "1. Reading Data\n",
    "* Reads the partitioned data into memory.\n",
    "  * A task for each partition.\n",
    "* This task has no dependency\n",
    "2. Filter, Select, and GroupBy\n",
    "* Applies .where(), .select(), and .groupBy().\n",
    "* Each task applies all these transformations on a partition.\n",
    "* GroupBy is \"Wide\" dependency (Needs to shuffle data between partitions for grouping)\n",
    "3. Count\n",
    "* Applies .count() to each group.\n",
    "* Each task calculates the count for groups in its partition.\n",
    "  * Rach group is guaranteed to be in the same partition\n",
    "* This task has no dependency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00631b18-045f-43e5-ac0b-e6c1f59848ce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### PySpark: Job, Stages and Tasks\n",
    "\n",
    "<img src=\"https://www.dropbox.com/s/5qa1fb7p867i787/Page5.jpg?dl=1\" width=\"900\" height=\"600\">\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a22c87cc-5fa2-497a-abd5-d945fbfe96f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001b[0m\n",
      "Collecting randomuser\n",
      "  Downloading randomuser-1.6.tar.gz (5.0 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: randomuser\n",
      "  Building wheel for randomuser (setup.py): started\n",
      "  Building wheel for randomuser (setup.py): finished with status 'done'\n",
      "  Created wheel for randomuser: filename=randomuser-1.6-py3-none-any.whl size=5067 sha256=43c83225c080737b09066ef07351699c7ccbecd59ad569b7e595d460a36b1525\n",
      "  Stored in directory: /root/.cache/pip/wheels/b8/f3/19/6a938647065b4bb2471a9d063647d14d4fcc3236731f4e2b53\n",
      "Successfully built randomuser\n",
      "Installing collected packages: randomuser\n",
      "Successfully installed randomuser-1.6\n",
      "\u001b[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "pip install randomuser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2d27bbd-6d2b-4359-86af-8ca61c661f71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user object  is <randomuser.RandomUser object at 0x7f64ae161b10>\n",
      "user json representation  is\n",
      " {'user_id': '331-30-2133', 'first_name': 'Layla', 'last_name': 'Wright', 'state': 'Wisconsin', 'zip': 59841, 'lat_long': {'latitude': '62.8794', 'longitude': '171.5591'}}\n"
     ]
    }
   ],
   "source": [
    "# Insstall using the following if not already installed \n",
    "\n",
    "from randomuser import RandomUser\n",
    "\n",
    "# # Generate a single user\n",
    "user = RandomUser({\"nat\": \"us\"})\n",
    "print(f\"user object  is {user}\")\n",
    "def get_user_info(u):\n",
    "\n",
    "    user_dict = {\n",
    "        \"user_id\": u.get_id()[\"number\"], \n",
    "        \"first_name\": u.get_first_name(), \n",
    "        \"last_name\": u.get_last_name(), \n",
    "        \"state\": u.get_state(),\n",
    "        \"zip\": u.get_zipcode(),\n",
    "        \"lat_long\": u.get_coordinates()\n",
    "    }\n",
    "    return user_dict\n",
    "\n",
    "user_json = get_user_info(user)\n",
    "print(f\"user json representation  is\\n {user_json}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17eeb182-7d8f-49d2-a00a-a73b7e92a7c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<randomuser.RandomUser at 0x7f64ae45b640>,\n",
       " <randomuser.RandomUser at 0x7f64ae45b5b0>,\n",
       " <randomuser.RandomUser at 0x7f64ae45b6a0>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_users = RandomUser.generate_users(500, {\"nat\": \"us\"})\n",
    "print(len(my_users))\n",
    "my_users[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a807114-710d-4c3f-ae16-a335257f7470",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'user_id': '378-40-5708',\n",
       "  'first_name': 'Julian',\n",
       "  'last_name': 'Griffin',\n",
       "  'state': 'New York',\n",
       "  'zip': 37964,\n",
       "  'lat_long': {'latitude': '-75.4630', 'longitude': '-8.8089'}},\n",
       " {'user_id': '834-80-5436',\n",
       "  'first_name': 'Joel',\n",
       "  'last_name': 'Vasquez',\n",
       "  'state': 'Montana',\n",
       "  'zip': 26392,\n",
       "  'lat_long': {'latitude': '29.0585', 'longitude': '69.3801'}},\n",
       " {'user_id': '611-47-0257',\n",
       "  'first_name': 'Lydia',\n",
       "  'last_name': 'Wallace',\n",
       "  'state': 'Missouri',\n",
       "  'zip': 33536,\n",
       "  'lat_long': {'latitude': '50.2295', 'longitude': '-16.1989'}}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a list of 10 random users\n",
    "\n",
    "user_dicts = list(map(get_user_info, my_users))\n",
    "\n",
    "user_dicts[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6817cbf9-f86e-4171-a4a4-284a90f01ca7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of objects in my RDD is: 500\n"
     ]
    }
   ],
   "source": [
    "users_rdd = sc.parallelize(user_dicts)\n",
    "users_rdd_size  = users_rdd.count()\n",
    "print(f\"The number of objects in my RDD is: {users_rdd_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5cfe97d-c065-453d-8ffb-91e55d5c562c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'user_id': '620-64-0507',\n",
       "  'first_name': 'Erik',\n",
       "  'last_name': 'Garcia',\n",
       "  'state': 'Hawaii',\n",
       "  'zip': 69548,\n",
       "  'lat_long': {'latitude': '66.3831', 'longitude': '135.7631'}},\n",
       " {'user_id': '783-39-9445',\n",
       "  'first_name': 'Reginald',\n",
       "  'last_name': 'Wheeler',\n",
       "  'state': 'Hawaii',\n",
       "  'zip': 25615,\n",
       "  'lat_long': {'latitude': '-40.4948', 'longitude': '59.3404'}},\n",
       " {'user_id': '946-19-2712',\n",
       "  'first_name': 'Scott',\n",
       "  'last_name': 'Peterson',\n",
       "  'state': 'Pennsylvania',\n",
       "  'zip': 14761,\n",
       "  'lat_long': {'latitude': '80.6580', 'longitude': '140.5090'}}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_rdd.takeSample(False, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33c2c47f-ee8e-4046-b02e-33151f5b352a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[84] at RDD at PythonRDD.scala:58"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_users_rdd = users_rdd.filter(lambda x: x['state'] in [\"Hawaii\", \"Idaho\"])\n",
    "select_users_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be241d1f-fe5d-4e9a-b3ab-36df42156b2e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'user_id': '353-47-7071',\n",
       "  'first_name': 'Claire',\n",
       "  'last_name': 'Beck',\n",
       "  'state': 'Hawaii',\n",
       "  'zip': 71522,\n",
       "  'lat_long': {'latitude': '-16.9948', 'longitude': '-167.7545'}},\n",
       " {'user_id': '528-05-8214',\n",
       "  'first_name': 'Josephine',\n",
       "  'last_name': 'Prescott',\n",
       "  'state': 'Hawaii',\n",
       "  'zip': 68836,\n",
       "  'lat_long': {'latitude': '38.2190', 'longitude': '22.3167'}},\n",
       " {'user_id': '785-01-3885',\n",
       "  'first_name': 'Alyssa',\n",
       "  'last_name': 'Stone',\n",
       "  'state': 'Idaho',\n",
       "  'zip': 49922,\n",
       "  'lat_long': {'latitude': '-63.8340', 'longitude': '-97.4699'}},\n",
       " {'user_id': '528-35-2493',\n",
       "  'first_name': 'Mario',\n",
       "  'last_name': 'Burton',\n",
       "  'state': 'Idaho',\n",
       "  'zip': 19149,\n",
       "  'lat_long': {'latitude': '-67.3983', 'longitude': '165.3953'}},\n",
       " {'user_id': '031-85-6639',\n",
       "  'first_name': 'Vanessa',\n",
       "  'last_name': 'Wells',\n",
       "  'state': 'Idaho',\n",
       "  'zip': 88568,\n",
       "  'lat_long': {'latitude': '46.3464', 'longitude': '-136.8160'}}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect the result means grab them from all the chunk nodes\n",
    "select_users_rdd.collect()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "630c8ced-716e-492b-86a3-85ab4786b5de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building an RDD from a text file.\n",
    "text = sc.textFile('dbfs:/FileStore/pride_and_prejudice.txt', minPartitions=4)\n",
    "### Number of items in the RDD\n",
    "text.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caecd573-8bbe-4634-a2d7-286917452a35",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of objects in the RDD is 14579\n"
     ]
    }
   ],
   "source": [
    "text_rdd_size = text.count()\n",
    "print(f\"number of objects in the RDD is {text_rdd_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73e058ba-39fb-45ae-a27e-f359e82f4d52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileInfo(path='dbfs:/FileStore/pride_and_prejudice.txt', name='pride_and_prejudice.txt', size=798774, modificationTime=1695151386000)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.ls(\"/FileStore/pride_and_prejudice.txt\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21364895-a949-46b5-a9bf-c7150ec21b5d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of subset_x is: 10\n",
      "\n",
      "type of subset_x is: <class 'list'>\n",
      "\n",
      "subset_x is:\n",
      "['The Project Gutenberg eBook of Pride and Prejudice, by Jane Austen', '', 'This eBook is for the use of anyone anywhere in the United States and', 'most other parts of the world at no cost and with almost no restrictions', 'whatsoever. You may copy it, give it away or re-use it under the terms', 'of the Project Gutenberg License included with this eBook or online at', 'www.gutenberg.org. If you are not located in the United States, you', 'will have to check the laws of the country where you are located before', 'using this eBook.', '']\n"
     ]
    }
   ],
   "source": [
    "subset_x = text.take(10)\n",
    "print(f\"len of subset_x is: {len(subset_x)}\\n\")\n",
    "print(f\"type of subset_x is: {type(subset_x)}\\n\")\n",
    "print(f\"subset_x is:\\n{subset_x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e5e91d7-50ef-4964-b453-cf77470b371b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['THE',\n",
       "  'PROJECT',\n",
       "  'GUTENBERG',\n",
       "  'EBOOK',\n",
       "  'OF',\n",
       "  'PRIDE',\n",
       "  'AND',\n",
       "  'PREJUDICE',\n",
       "  'BY',\n",
       "  'JANE',\n",
       "  'AUSTEN'],\n",
       " [],\n",
       " ['THIS',\n",
       "  'EBOOK',\n",
       "  'IS',\n",
       "  'FOR',\n",
       "  'THE',\n",
       "  'USE',\n",
       "  'OF',\n",
       "  'ANYONE',\n",
       "  'ANYWHERE',\n",
       "  'IN',\n",
       "  'THE',\n",
       "  'UNITED',\n",
       "  'STATES',\n",
       "  'AND'],\n",
       " ['MOST',\n",
       "  'OTHER',\n",
       "  'PARTS',\n",
       "  'OF',\n",
       "  'THE',\n",
       "  'WORLD',\n",
       "  'AT',\n",
       "  'NO',\n",
       "  'COST',\n",
       "  'AND',\n",
       "  'WITH',\n",
       "  'ALMOST',\n",
       "  'NO',\n",
       "  'RESTRICTIONS'],\n",
       " ['WHATSOEVER',\n",
       "  'YOU',\n",
       "  'MAY',\n",
       "  'COPY',\n",
       "  'IT',\n",
       "  'GIVE',\n",
       "  'IT',\n",
       "  'AWAY',\n",
       "  'OR',\n",
       "  'RE',\n",
       "  'USE',\n",
       "  'IT',\n",
       "  'UNDER',\n",
       "  'THE',\n",
       "  'TERMS'],\n",
       " ['OF',\n",
       "  'THE',\n",
       "  'PROJECT',\n",
       "  'GUTENBERG',\n",
       "  'LICENSE',\n",
       "  'INCLUDED',\n",
       "  'WITH',\n",
       "  'THIS',\n",
       "  'EBOOK',\n",
       "  'OR',\n",
       "  'ONLINE',\n",
       "  'AT'],\n",
       " ['WWW',\n",
       "  'GUTENBERG',\n",
       "  'ORG',\n",
       "  'IF',\n",
       "  'YOU',\n",
       "  'ARE',\n",
       "  'NOT',\n",
       "  'LOCATED',\n",
       "  'IN',\n",
       "  'THE',\n",
       "  'UNITED',\n",
       "  'STATES',\n",
       "  'YOU'],\n",
       " ['WILL',\n",
       "  'HAVE',\n",
       "  'TO',\n",
       "  'CHECK',\n",
       "  'THE',\n",
       "  'LAWS',\n",
       "  'OF',\n",
       "  'THE',\n",
       "  'COUNTRY',\n",
       "  'WHERE',\n",
       "  'YOU',\n",
       "  'ARE',\n",
       "  'LOCATED',\n",
       "  'BEFORE'],\n",
       " ['USING', 'THIS', 'EBOOK'],\n",
       " [],\n",
       " ['TITLE', 'PRIDE', 'AND', 'PREJUDICE'],\n",
       " [],\n",
       " ['AUTHOR', 'JANE', 'AUSTEN'],\n",
       " [],\n",
       " ['RELEASE', 'DATE', 'JUNE', 'EBOOK'],\n",
       " ['MOST', 'RECENTLY', 'UPDATED', 'AUGUST'],\n",
       " [],\n",
       " ['LANGUAGE', 'ENGLISH'],\n",
       " [],\n",
       " ['CHARACTER', 'SET', 'ENCODING', 'UTF'],\n",
       " [],\n",
       " ['PRODUCED', 'BY', 'ANONYMOUS', 'VOLUNTEERS', 'AND', 'DAVID', 'WIDGER'],\n",
       " [],\n",
       " ['START',\n",
       "  'OF',\n",
       "  'THE',\n",
       "  'PROJECT',\n",
       "  'GUTENBERG',\n",
       "  'EBOOK',\n",
       "  'PRIDE',\n",
       "  'AND',\n",
       "  'PREJUDICE'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['THERE',\n",
       "  'IS',\n",
       "  'AN',\n",
       "  'ILLUSTRATED',\n",
       "  'EDITION',\n",
       "  'OF',\n",
       "  'THIS',\n",
       "  'TITLE',\n",
       "  'WHICH',\n",
       "  'MAY',\n",
       "  'VIEWED',\n",
       "  'AT',\n",
       "  'EBOOK'],\n",
       " [],\n",
       " [],\n",
       " ['COVER'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['PRIDE', 'AND', 'PREJUDICE'],\n",
       " [],\n",
       " ['BY', 'JANE', 'AUSTEN'],\n",
       " [],\n",
       " ['CONTENTS'],\n",
       " [],\n",
       " ['CHAPTER'],\n",
       " [],\n",
       " ['CHAPTER'],\n",
       " [],\n",
       " ['CHAPTER'],\n",
       " [],\n",
       " ['CHAPTER'],\n",
       " [],\n",
       " ['CHAPTER'],\n",
       " [],\n",
       " ['CHAPTER'],\n",
       " [],\n",
       " ['CHAPTER'],\n",
       " [],\n",
       " ['CHAPTER'],\n",
       " [],\n",
       " ['CHAPTER'],\n",
       " []]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_split_line(line):\n",
    "    a = re.sub('\\d+', '', line)\n",
    "    b = re.sub('[\\W]+', ' ', a)\n",
    "    return b.upper().split()\n",
    "\n",
    "words = text.map(clean_split_line)\n",
    "words.take(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "378d00f7-542c-4a98-b78c-ad3fe02a5747",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['THE',\n",
       " 'PROJECT',\n",
       " 'GUTENBERG',\n",
       " 'EBOOK',\n",
       " 'OF',\n",
       " 'PRIDE',\n",
       " 'AND',\n",
       " 'PREJUDICE',\n",
       " 'BY',\n",
       " 'JANE',\n",
       " 'AUSTEN',\n",
       " 'THIS',\n",
       " 'EBOOK',\n",
       " 'IS',\n",
       " 'FOR',\n",
       " 'THE',\n",
       " 'USE',\n",
       " 'OF',\n",
       " 'ANYONE',\n",
       " 'ANYWHERE',\n",
       " 'IN',\n",
       " 'THE',\n",
       " 'UNITED',\n",
       " 'STATES',\n",
       " 'AND',\n",
       " 'MOST',\n",
       " 'OTHER',\n",
       " 'PARTS',\n",
       " 'OF',\n",
       " 'THE',\n",
       " 'WORLD',\n",
       " 'AT',\n",
       " 'NO',\n",
       " 'COST',\n",
       " 'AND',\n",
       " 'WITH',\n",
       " 'ALMOST',\n",
       " 'NO',\n",
       " 'RESTRICTIONS',\n",
       " 'WHATSOEVER',\n",
       " 'YOU',\n",
       " 'MAY',\n",
       " 'COPY',\n",
       " 'IT',\n",
       " 'GIVE',\n",
       " 'IT',\n",
       " 'AWAY',\n",
       " 'OR',\n",
       " 'RE',\n",
       " 'USE',\n",
       " 'IT',\n",
       " 'UNDER',\n",
       " 'THE',\n",
       " 'TERMS',\n",
       " 'OF',\n",
       " 'THE',\n",
       " 'PROJECT',\n",
       " 'GUTENBERG',\n",
       " 'LICENSE',\n",
       " 'INCLUDED']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_split_line(line):\n",
    "    a = re.sub('\\d+', '', line)\n",
    "    b = re.sub('[\\W]+', ' ', a)\n",
    "    return b.upper().split()\n",
    "\n",
    "words = text.flatMap(clean_split_line)\n",
    "words.take(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29c2185e-9ea0-48c4-98a9-8a4b303c52aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126018"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0dc92a9-cfe2-498a-839a-5945925f6ea0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['THE',\n",
       "   'PROJECT',\n",
       "   'GUTENBERG',\n",
       "   'EBOOK',\n",
       "   'OF',\n",
       "   'PRIDE',\n",
       "   'AND',\n",
       "   'PREJUDICE',\n",
       "   'BY',\n",
       "   'JANE',\n",
       "   'AUSTEN'],\n",
       "  1),\n",
       " ([], 1),\n",
       " (['THIS',\n",
       "   'EBOOK',\n",
       "   'IS',\n",
       "   'FOR',\n",
       "   'THE',\n",
       "   'USE',\n",
       "   'OF',\n",
       "   'ANYONE',\n",
       "   'ANYWHERE',\n",
       "   'IN',\n",
       "   'THE',\n",
       "   'UNITED',\n",
       "   'STATES',\n",
       "   'AND'],\n",
       "  1),\n",
       " (['MOST',\n",
       "   'OTHER',\n",
       "   'PARTS',\n",
       "   'OF',\n",
       "   'THE',\n",
       "   'WORLD',\n",
       "   'AT',\n",
       "   'NO',\n",
       "   'COST',\n",
       "   'AND',\n",
       "   'WITH',\n",
       "   'ALMOST',\n",
       "   'NO',\n",
       "   'RESTRICTIONS'],\n",
       "  1),\n",
       " (['WHATSOEVER',\n",
       "   'YOU',\n",
       "   'MAY',\n",
       "   'COPY',\n",
       "   'IT',\n",
       "   'GIVE',\n",
       "   'IT',\n",
       "   'AWAY',\n",
       "   'OR',\n",
       "   'RE',\n",
       "   'USE',\n",
       "   'IT',\n",
       "   'UNDER',\n",
       "   'THE',\n",
       "   'TERMS'],\n",
       "  1),\n",
       " (['OF',\n",
       "   'THE',\n",
       "   'PROJECT',\n",
       "   'GUTENBERG',\n",
       "   'LICENSE',\n",
       "   'INCLUDED',\n",
       "   'WITH',\n",
       "   'THIS',\n",
       "   'EBOOK',\n",
       "   'OR',\n",
       "   'ONLINE',\n",
       "   'AT'],\n",
       "  1),\n",
       " (['WWW',\n",
       "   'GUTENBERG',\n",
       "   'ORG',\n",
       "   'IF',\n",
       "   'YOU',\n",
       "   'ARE',\n",
       "   'NOT',\n",
       "   'LOCATED',\n",
       "   'IN',\n",
       "   'THE',\n",
       "   'UNITED',\n",
       "   'STATES',\n",
       "   'YOU'],\n",
       "  1),\n",
       " (['WILL',\n",
       "   'HAVE',\n",
       "   'TO',\n",
       "   'CHECK',\n",
       "   'THE',\n",
       "   'LAWS',\n",
       "   'OF',\n",
       "   'THE',\n",
       "   'COUNTRY',\n",
       "   'WHERE',\n",
       "   'YOU',\n",
       "   'ARE',\n",
       "   'LOCATED',\n",
       "   'BEFORE'],\n",
       "  1),\n",
       " (['USING', 'THIS', 'EBOOK'], 1),\n",
       " ([], 1)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We want to do something like the following\n",
    "# words_mapped = words.map(lambda x: (x,1))\n",
    "\n",
    "words_mapped = words.map(lambda x: (x,1))\n",
    "words_mapped.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b063e48d-b739-4678-8705-220eff5e2a54",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[21] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_map = words_mapped.sortByKey()\n",
    "sorted_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2da4743b-d5bf-4c11-9073-b28c911ac012",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 1),\n",
       " ('A', 1),\n",
       " ('A', 1),\n",
       " ('ALL', 1),\n",
       " ('AND', 1),\n",
       " ('AND', 1),\n",
       " ('AND', 1),\n",
       " ('ASSEMBLED', 1),\n",
       " ('AT', 1),\n",
       " ('BE', 1),\n",
       " ('BE', 1),\n",
       " ('BEFORE', 1),\n",
       " ('BEFORE', 1),\n",
       " ('BENNET', 1),\n",
       " ('BUT', 1),\n",
       " ('BUT', 1),\n",
       " ('BY', 1),\n",
       " ('CANDOUR', 1),\n",
       " ('COACH', 1),\n",
       " ('COLONEL', 1),\n",
       " ('COLONEL', 1),\n",
       " ('CONFIRMATION', 1),\n",
       " ('CONTRIVED', 1),\n",
       " ('CRIED', 1),\n",
       " ('DARCY', 1),\n",
       " ('DAY', 1),\n",
       " ('DESIRED', 1),\n",
       " ('DIFFIDENCE', 1),\n",
       " ('DON', 1),\n",
       " ('EBOOK', 1),\n",
       " ('EDWARD', 1),\n",
       " ('ELIZABETH', 1),\n",
       " ('EMBARRASSMENT', 1),\n",
       " ('EVERYTHING', 1),\n",
       " ('EXAGGERATION', 1),\n",
       " ('HAD', 1),\n",
       " ('HAD', 1),\n",
       " ('HAD', 1),\n",
       " ('HE', 1),\n",
       " ('HER', 1),\n",
       " ('HER', 1),\n",
       " ('HERTFORDSHIRE', 1),\n",
       " ('HIM', 1),\n",
       " ('HIS', 1),\n",
       " ('HIS', 1),\n",
       " ('HOPE', 1),\n",
       " ('IN', 1),\n",
       " ('IN', 1),\n",
       " ('INDEED', 1),\n",
       " ('INDUCEMENT', 1),\n",
       " ('IS', 1),\n",
       " ('IT', 1),\n",
       " ('IT', 1),\n",
       " ('ITS', 1),\n",
       " ('KNOW', 1),\n",
       " ('LAUGHED', 1),\n",
       " ('LINES', 1),\n",
       " ('LOOSE', 1),\n",
       " ('MANNERS', 1),\n",
       " ('ME', 1),\n",
       " ('MORE', 1),\n",
       " ('MUST', 1),\n",
       " ('MYSELF', 1),\n",
       " ('NO', 1),\n",
       " ('NOT', 1),\n",
       " ('OCCASION', 1),\n",
       " ('OF', 1),\n",
       " ('OF', 1),\n",
       " ('OF', 1),\n",
       " ('PUT', 1),\n",
       " ('QUICKNESS', 1),\n",
       " ('REPEATED', 1),\n",
       " ('RETURN', 1),\n",
       " ('S', 1),\n",
       " ('S', 1),\n",
       " ('SAID', 1),\n",
       " ('SAID', 1),\n",
       " ('SATURDAY', 1),\n",
       " ('SEE', 1),\n",
       " ('SHE', 1),\n",
       " ('SHE', 1),\n",
       " ('SOMETIMES', 1),\n",
       " ('SPREAD', 1),\n",
       " ('STRUCK', 1),\n",
       " ('STUDYING', 1),\n",
       " ('TELL', 1),\n",
       " ('TEN', 1),\n",
       " ('THAN', 1),\n",
       " ('THAN', 1),\n",
       " ('THAT', 1),\n",
       " ('THAT', 1),\n",
       " ('THE', 1),\n",
       " ('THE', 1),\n",
       " ('THE', 1),\n",
       " ('THE', 1),\n",
       " ('THE', 1),\n",
       " ('THEM', 1),\n",
       " ('THEM', 1),\n",
       " ('THERE', 1),\n",
       " ('THERE', 1),\n",
       " ('THOUGHT', 1),\n",
       " ('THROUGH', 1),\n",
       " ('TO', 1),\n",
       " ('TO', 1),\n",
       " ('TO', 1),\n",
       " ('VERY', 1),\n",
       " ('VOICE', 1),\n",
       " ('WALKED', 1),\n",
       " ('WERE', 1),\n",
       " ('WERE', 1),\n",
       " ('WHAT', 1),\n",
       " ('WILL', 1),\n",
       " ('YES', 1),\n",
       " ('YES', 1),\n",
       " ('YOU', 1),\n",
       " ('_WAS_', 1)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = sorted_map.sample(withReplacement=False, fraction= 0.001)\n",
    "sample.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec9d2a96-e1f1-42f0-aae6-7b967f9a9c8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PRIDE', 52),\n",
       " ('UNITED', 22),\n",
       " ('OTHER', 227),\n",
       " ('WORLD', 68),\n",
       " ('NO', 501),\n",
       " ('GIVE', 127),\n",
       " ('LICENSE', 18),\n",
       " ('WWW', 9),\n",
       " ('ARE', 361),\n",
       " ('TO', 4245),\n",
       " ('DATE', 5),\n",
       " ('UPDATED', 2),\n",
       " ('ENGLISH', 1),\n",
       " ('CHARACTER', 65),\n",
       " ('PRODUCED', 13),\n",
       " ('ILLUSTRATED', 1),\n",
       " ('THAT', 1555),\n",
       " ('POSSESSION', 10),\n",
       " ('LITTLE', 187),\n",
       " ('KNOWN', 58),\n",
       " ('VIEWS', 11),\n",
       " ('CONSIDERED', 23),\n",
       " ('AS', 1193),\n",
       " ('ONE', 273),\n",
       " ('THEIR', 439),\n",
       " ('MR', 784),\n",
       " ('JUST', 72),\n",
       " ('TOLD', 69),\n",
       " ('ME', 427),\n",
       " ('ANSWER', 65),\n",
       " ('WHO', 288),\n",
       " ('TELL', 71),\n",
       " ('HEARING', 24),\n",
       " ('ENOUGH', 106),\n",
       " ('WHY', 53),\n",
       " ('YOUNG', 130),\n",
       " ('MONDAY', 8),\n",
       " ('FOUR', 35),\n",
       " ('MUCH', 327),\n",
       " ('AGREED', 13),\n",
       " ('MICHAELMAS', 2),\n",
       " ('SERVANTS', 13),\n",
       " ('WEEK', 29),\n",
       " ('NAME', 34),\n",
       " ('BINGLEY', 307),\n",
       " ('OH', 96),\n",
       " ('FIVE', 32),\n",
       " ('YEAR', 29),\n",
       " ('FINE', 31),\n",
       " ('CAN', 223)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = words_mapped.reduceByKey(lambda x,y: x+y)\n",
    "counts.collect()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2e54ff0-687c-4e5d-82cc-344771c6fb36",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40.9 ms, sys: 7.41 ms, total: 48.4 ms\n",
      "Wall time: 2.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('PRIDE', 52),\n",
       " ('UNITED', 22),\n",
       " ('OTHER', 227),\n",
       " ('WORLD', 68),\n",
       " ('NO', 501),\n",
       " ('GIVE', 127),\n",
       " ('LICENSE', 18),\n",
       " ('WWW', 9),\n",
       " ('ARE', 361),\n",
       " ('TO', 4245),\n",
       " ('DATE', 5),\n",
       " ('UPDATED', 2),\n",
       " ('ENGLISH', 1),\n",
       " ('CHARACTER', 65),\n",
       " ('PRODUCED', 13),\n",
       " ('ILLUSTRATED', 1),\n",
       " ('THAT', 1555),\n",
       " ('POSSESSION', 10),\n",
       " ('LITTLE', 187),\n",
       " ('KNOWN', 58),\n",
       " ('VIEWS', 11),\n",
       " ('CONSIDERED', 23),\n",
       " ('AS', 1193),\n",
       " ('ONE', 273),\n",
       " ('THEIR', 439),\n",
       " ('MR', 784),\n",
       " ('JUST', 72),\n",
       " ('TOLD', 69),\n",
       " ('ME', 427),\n",
       " ('ANSWER', 65),\n",
       " ('WHO', 288),\n",
       " ('TELL', 71),\n",
       " ('HEARING', 24),\n",
       " ('ENOUGH', 106),\n",
       " ('WHY', 53),\n",
       " ('YOUNG', 130),\n",
       " ('MONDAY', 8),\n",
       " ('FOUR', 35),\n",
       " ('MUCH', 327),\n",
       " ('AGREED', 13),\n",
       " ('MICHAELMAS', 2),\n",
       " ('SERVANTS', 13),\n",
       " ('WEEK', 29),\n",
       " ('NAME', 34),\n",
       " ('BINGLEY', 307),\n",
       " ('OH', 96),\n",
       " ('FIVE', 32),\n",
       " ('YEAR', 29),\n",
       " ('FINE', 31),\n",
       " ('CAN', 223),\n",
       " ('NONSENSE', 8),\n",
       " ('THEREFORE', 75),\n",
       " ('VISIT', 53),\n",
       " ('PERHAPS', 76),\n",
       " ('PARTY', 58),\n",
       " ('THAN', 285),\n",
       " ('CONSIDER', 33),\n",
       " ('YOUR', 446),\n",
       " ('ESTABLISHMENT', 6),\n",
       " ('WILLIAM', 46),\n",
       " ('LUCAS', 70),\n",
       " ('DETERMINED', 32),\n",
       " ('THEY', 599),\n",
       " ('FEW', 72),\n",
       " ('HEARTY', 2),\n",
       " ('CONSENT', 13),\n",
       " ('DESIRE', 23),\n",
       " ('BIT', 3),\n",
       " ('OTHERS', 55),\n",
       " ('ALWAYS', 119),\n",
       " ('RECOMMEND', 12),\n",
       " ('SISTERS', 76),\n",
       " ('CHILDREN', 25),\n",
       " ('VEXING', 1),\n",
       " ('POOR', 38),\n",
       " ('NERVES', 4),\n",
       " ('HIGH', 17),\n",
       " ('OLD', 15),\n",
       " ('YEARS', 36),\n",
       " ('SINCE', 59),\n",
       " ('CAPRICE', 4),\n",
       " ('INSUFFICIENT', 10),\n",
       " ('UNDERSTANDING', 21),\n",
       " ('FANCIED', 7),\n",
       " ('NERVOUS', 4),\n",
       " ('VISITING', 5),\n",
       " ('WAITED', 7),\n",
       " ('ASSURING', 4),\n",
       " ('TILL', 93),\n",
       " ('EVENING', 74),\n",
       " ('AFTER', 200),\n",
       " ('FOLLOWING', 27),\n",
       " ('TRIMMING', 1),\n",
       " ('SUDDENLY', 16),\n",
       " ('ADDRESSED', 17),\n",
       " ('WE', 255),\n",
       " ('FORGET', 17),\n",
       " ('SHALL', 164),\n",
       " ('PROMISED', 21),\n",
       " ('TWO', 131)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As functional programming always returns new data instead of manipulating the data in-place, we can rewrite the above as:\n",
    "\n",
    "%%time\n",
    "counts_test_2 = text.flatMap(clean_split_line).map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y)\n",
    "counts_test_2.take(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9206831c-cf2e-4cb1-a65b-ca42c0480a1c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Project Gutenberg eBook of Pride and Prejudice, by Jane Austen']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.take(1)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark_introduction_1",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
