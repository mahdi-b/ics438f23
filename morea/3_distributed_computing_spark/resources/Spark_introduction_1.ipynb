{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69df502a-4c15-4a80-804d-258327cfab0b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### The case for distributed computing\n",
    "\n",
    "* What happens if the data to be analyzed is too large?\n",
    "  * e.g. cannot be stored on a single machine\n",
    "\n",
    "* What if the computation is too complex?\n",
    "  * e.g., in interactive mode, it is unacceptably slow\n",
    "\n",
    "* What if you have to deal with both situations?\n",
    "\n",
    "* Can you scale up? Can you scale out?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d14eef8e-7f19-4946-9d0f-20884122ba8b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Scaling Up: Local Machine\n",
    "\n",
    "* Scaling up occurs within the same system hosting the data and running the computaiton\n",
    "  * Simple to carry out from a physical standpoint\n",
    "  * from a programmatic standpoint, it's managed by the operating system and programming libraries; does not require additional frameworks\n",
    "* Scabaility is typically limited by the OS physical resources on the system.\n",
    "  * RAM upper bound is determined by the OS  or the number of slots available on the motherboard\n",
    "* The cost of a single machine at the highest configuration may be prohibitive\n",
    "\n",
    "* May not meet the demands of the workload at hand\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f127b161-c2c5-42de-bf9c-1d6aac2af7f9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Distributed Systems\n",
    "\n",
    "* The hardware specs may not be the same across machines, adding another layer of complexity if it doesn't\n",
    "\n",
    "<img src=\"https://www.dropbox.com/s/8mncw4ffe8uajol/networking.jpg?dl=1\" width=\"700\" height=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b4c13ab-adc4-4240-a9ed-0fb81aa7f829",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Key Requirements for Distributed Systems\n",
    "\n",
    "* A distributed system needs to meet several crucial criteria, including:\n",
    "\n",
    "  * Node Communication: Enabling seamless interaction among nodes in the network.\n",
    "  * Resilience to Faults: Ensuring that both data and operations remain unaffected by system failures.\n",
    "  * Scalability: The ability to scale computational resources to handle growing workloads.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf345b2d-f0eb-4e0f-81bb-04098a0a293e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Apache Spark\n",
    "\n",
    "* Apache Spark is an open-source, distributed processing system used for big data workloads.\n",
    "  * Runs on a cluster\n",
    "\n",
    "* It's an enhancement to Hadoop's MapReduce\n",
    "  * Processes and retains data in memory for subsequent steps\n",
    "  * For smaller workloads, Sparkâ€™s data processing speeds are up to 100x faster than Hadoop's MapReduce\n",
    "\n",
    "* Written in Scala and runs in the JVM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18f11d85-533e-4e25-90bf-9fcce5c39162",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Apache Spark\n",
    "\n",
    "* Ideal for real-time processing as it utilizes in-memory caching and optimized query execution for fast queries against data of any size.  \n",
    "\n",
    "* Provides a richer ecosystem of functionality\n",
    "  * Over 80 high level operators beyond Map and Reduce\n",
    "    * Tools for pipeline construction and evaluation\n",
    "  * compared to Hadoop, Spark provides more operators other than map and reduce\n",
    "    * Includes libraries to support SQL queries, machine learning (MLlib), graph data analysis (GraphX) and streaming data analysis\n",
    "  * Plethora of functions for SQL-like operation, ML and working with graph data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "048bfbef-984c-4209-a67e-13d5d2c639ac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### What is Apache Spark\n",
    "\n",
    "* [See Video](https://www.databricks.com/spark/about)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf96282e-53bf-4663-b90c-02918cfae9db",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Spark and Functional Programming\n",
    "\n",
    "* To manipulate data, Spark uses functional programming\n",
    "  * The functional programming paradigm is used in many popular languages including Common Lisp, Scheme, Clojure, OCaml, and Haskell\n",
    "  \n",
    "* Functional programming is a data oriented paradigm\n",
    "  * Decomposes a problem into a set of functions.\n",
    "  * Logic is implemented by applying and composing functions.\n",
    "\n",
    "* The idea is that functions should be able to manipulate data without maintaining any external state.\n",
    "  * No external or global variables\n",
    " \n",
    "* In functional programming, we need to always return new data instead of manipulating the data in-place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef54bd37-5133-436b-8da7-7e302c6ebd9c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Spark Operations and RDDs: An Overview\n",
    "\n",
    "What are RDDs? \n",
    " * Resilient Distributed Datasets is the data representation in Spark. \n",
    "   * an RDD is conceptually divided into one or more partitions. \n",
    "     Each partition is a self-contained unit of data that can be operated on independently and in parallel. \n",
    "     These partitions are distributed across the nodes in a Spark cluster for parallel computation.\n",
    "\n",
    "  * RDDs are read-only collections, partitioned across multiple computing nodes for optimized performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a53aff6-07df-4fc6-ace6-6a9a31351200",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Spark Operations and RDDs: An Overview\n",
    "  * Each partition is also replicated across nodes.\n",
    "    * Number of replicates is a configuration parameter.\n",
    "  * Partitioning enhances fault tolerance and boosts the efficiency of data operations.\n",
    "  * This allows RDDs to be accessed through parallel operations\n",
    "    * Data operations can be executed on all partitions at the same time, speeding up data tasks.\n",
    "    \n",
    "* In-Memory Caching\n",
    "  * RDDs are stored in memory -- if the RDD can fit into a node's memory -- facilitating quick iterations over the same dataset.\n",
    "    * Disk Spilling: If an RDD is too large Spark spills the what doesn't fit in RAM to disk\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82a09076-0a99-4e0b-a59a-b5a1a9c89419",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Overview of Apache Spark's Core Components\n",
    "\n",
    "* Spark Core: The foundation of the entire Spark ecosystem.\n",
    "  * Defines the basic data structure for (RDDs).\n",
    "  * Provides a set of operations (transformations) and actions to process RDDs.\n",
    "  * Enables distributed data processing, fault tolerance, and in-memory computations.\n",
    "\n",
    "* Spark SQL: Spark's SQL engine for structured data.\n",
    "  * Supports ANSI SQL standards for query language.\n",
    "  * Transforms SQL queries into Spark operations.\n",
    "  * Allows for SQL-like querying on large datasets, bridging the gap between traditional databases and big data processing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21b3e958-4c48-4874-a693-aef5b3c487a0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Overview of Apache Spark's Core Components - Cont'd\n",
    "\n",
    "* Spark Streaming: Real-time data processing module in Spark.\n",
    "  * Processes live streaming data.\n",
    "  * Offers seamless integration with other Spark components.\n",
    "  * Enables real-time analytics and data processing, vital for applications like fraud detection, monitoring, and recommendation systems.\n",
    "\n",
    "* MLlib: Spark's machine learning library.\n",
    "  * Implements machine learning algorithms on RDDs.\n",
    "  * Provides algorithms for classification, regression, clustering, and more.\n",
    "  * Allows for scalable machine learning tasks, leveraging Spark's distributed computing power.\n",
    "\n",
    "* GraphX: Spark's library for graph processing.\n",
    "  * Manages and manipulates graph structures.\n",
    "  * Performs parallel graph operations and computations.\n",
    "  * Enables graph analytics at scale, useful for social network analysis, recommendation systems, and more.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a83ee9cc-102d-4f0b-a3f8-ea04abe69c6f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Core Components of Spark\n",
    "<img src=\"https://www.dropbox.com/s/azebxe8nv5nsqne/spark_architecture.png?dl=1\" width=\"900\" height=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cecd85f-8052-4735-9899-848545ba2573",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Spark High-level Components\n",
    "\n",
    "* Cluster manager: Manages the resources across the Spark cluster.\n",
    "  * Responsible for allocating resources like CPU and memory to Spark applications.\n",
    "  \n",
    "* Application driver: The central orchestrator of the Spark program, housing the main application logic.\n",
    "  * Contains an isntance of the spark context\n",
    "    * Requests resources from the Cluster Manager to launch executors.\n",
    "    * Coordinates the overall data processing workflow.\n",
    "  \n",
    "* Executors (workers): These are the worker nodes to which tasks are delegated.\n",
    "  * They execute the code sent by the driver, specifically focusing on their designated partitions of the dataset.\n",
    "  * Executors communicate with the Cluster Manager to report status and failures.\n",
    "\n",
    "\n",
    "![](https://spark.apache.org/docs/latest/img/cluster-overview.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c751e77-497a-4e72-b695-2e9894ce838e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Spark Program Flow\n",
    "\n",
    "* A typical Spark program adheres to the following structure:\n",
    "  * Application Driver: Central point of the Spark program, where the main application logic resides. \n",
    "    * Responsible for coordinating the entire data processing workflow.\n",
    "  * Executors (Workers): The worker nodes that tasks are delegated to.\n",
    "  \n",
    "    * Rxecuting the code sent by the driver, specifically focusing their designated partitions of the dataset.\n",
    "     * Driver send smaller, more specific operations that the executors can carry out.\n",
    "       * Referred to as a task plan.\n",
    "  * Result Aggregation: results sent by the executor to the application driver for aggregation\n",
    "    * Often a final layer of computation to produce the output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caa78be3-1059-402b-9a4a-8aa64392708f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Spark Application Lifecycle\n",
    "\n",
    "1. Python Program Initialization\n",
    "    * The `SparkContext` object is created; it's a client to interact with the Spark cluster.\n",
    "  \n",
    "2. Resource Request to Cluster Manager:\n",
    "    * `SparkContext` contacts the Cluster Manager to request resources (CPU, memory) for your application.\n",
    "  \n",
    "3. Cluster Manager Allocates Resources:\n",
    "    * The Cluster Manager allocates the necessary resources for the application and decides where to place the executors across the cluster's nodes.\n",
    "  \n",
    "4. Application Driver Initialization\n",
    "    * The main logic of your Spark application is in the Application Driver.\n",
    "    * It becomes the master node for your application, coordinating tasks between the cluster and your Python program.\n",
    "5. Executor Launch\n",
    "    * Based on the resources allocated by the Cluster Manager, executors for the Spark application are launched on the worker nodes.\n",
    "6. Task Division and Execution Plan\n",
    "    * The Application Driver divides the job into tasks and builds an execution plan.\n",
    "7. Sending Task Plans to Executors\n",
    "    * Instead of sending raw code, the Application Driver sends the execution plan (or task plans) to the Executors.\n",
    "8. Task Execution on Executors\n",
    "    * Executors run the tasks on their designated partition of the dataset.\n",
    "9. Result Aggregation\n",
    "    * After task execution, the Executors send the results back to the Application Driver.\n",
    "10. Final Computation and Output Retrieval\n",
    "    * The Application Driver may perform some final computations.\n",
    "    * Results are returned to your Python program through the `SparkContext` object.\n",
    "11. Resource Release\n",
    "    * Once the application completes, the resources are released back to the Cluster Manager for use by other applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71997d80-647d-4a9b-80d7-9eed2733ac03",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Setting up a Docker Cluster\n",
    " \n",
    "* Installing Spark and all its components manually can be challenging and time-consuming.\n",
    "  * Configuring and optimizing a Spark cluster from scratch requires substantial effort.\n",
    "* Easier deployment options are available on cloud services, such as:\n",
    "  * [Amazon's EMR](https://aws.amazon.com/emr/features/spark/) for Spark support\n",
    "  * [Databricks' Community Edition](https://www.databricks.com/product/faq/community-edition) and paid offerings\n",
    "  * Other providers like Google's Dataproc, Microsoft's HDInsight, etc.\n",
    "\n",
    "* We'll utilize Databricks Community Edition.\n",
    "  [Brief Demo](www.databricks.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d13a01f5-e404-4c42-a6c7-cd1994c23166",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Installing Via Docker For ICS438 (Optional)\n",
    "\n",
    "* Note that the following is optional. All the work we will using pySpark will be done on a Databricks cluster in community edition. Also, this solutions assumes that you have docker running on your machine.\n",
    "\n",
    "* It is easy to use Docker to install locally. We will use the following Docker image\n",
    "  \n",
    "```  \n",
    "jupyter/all-spark-notebook\n",
    "```\n",
    "* There are other docker images, including (jupyter/pyspark-notebook), which does not include the jobs dashboard `http://localhost:4040`\n",
    "\n",
    "* We will run the infrastructure as follows:\n",
    "\n",
    "```\n",
    "docker run --rm -p 4040:4040 -p 8888:8888 -v $(pwd):/home/jovyan/work jupyter/all-spark-notebook\n",
    "```\n",
    "\n",
    "* This configuration created a master and compute nodes locally in a docker instance\n",
    " \n",
    "* While you're probably not going to need to, you can log into the running container using: `docker exec -it <CONTAINER_ID> bash`\n",
    "\n",
    " * where <CONTAINER_ID> of the container currently running the `jupyter/all-spark-notebook` image\n",
    "\n",
    "\n",
    "* The Docker instance has all the libraries installed and ready to go.\n",
    "\n",
    "* Make sure you run a Jupyer notebook on the Docker instnac\n",
    "  * If the code below fails, this means you're not running in the Docker instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "503e7db6-97e9-4c43-af09-c95fab6d7fca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=3007032361860494#setting/sparkui/0926-185017-qmz3pexl/driver-1577583029937858185\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=3007032361860494#setting/sparkui/0926-185017-qmz3pexl/driver-1577583029937858185\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# pip install pyspark\n",
    "\n",
    "from pyspark import SparkContext\n",
    "# sc = SparkContext()\n",
    "sc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e24d0ec7-0f4f-43ba-aa5b-e4c7b70fbc87",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class SparkContext in module pyspark.context:\n\nclass SparkContext(builtins.object)\n |  SparkContext(master: Optional[str] = None, appName: Optional[str] = None, sparkHome: Optional[str] = None, pyFiles: Optional[List[str]] = None, environment: Optional[Dict[str, Any]] = None, batchSize: int = 0, serializer: 'Serializer' = CloudPickleSerializer(), conf: Optional[pyspark.conf.SparkConf] = None, gateway: Optional[py4j.java_gateway.JavaGateway] = None, jsc: Optional[py4j.java_gateway.JavaObject] = None, profiler_cls: Type[pyspark.profiler.BasicProfiler] = <class 'pyspark.profiler.BasicProfiler'>, udf_profiler_cls: Type[pyspark.profiler.UDFBasicProfiler] = <class 'pyspark.profiler.UDFBasicProfiler'>, memory_profiler_cls: Type[pyspark.profiler.MemoryProfiler] = <class 'pyspark.profiler.MemoryProfiler'>)\n |  \n |  Main entry point for Spark functionality. A SparkContext represents the\n |  connection to a Spark cluster, and can be used to create :class:`RDD` and\n |  broadcast variables on that cluster.\n |  \n |  When you create a new SparkContext, at least the master and app name should\n |  be set, either through the named parameters here or through `conf`.\n |  \n |  Parameters\n |  ----------\n |  master : str, optional\n |      Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]).\n |  appName : str, optional\n |      A name for your job, to display on the cluster web UI.\n |  sparkHome : str, optional\n |      Location where Spark is installed on cluster nodes.\n |  pyFiles : list, optional\n |      Collection of .zip or .py files to send to the cluster\n |      and add to PYTHONPATH.  These can be paths on the local file\n |      system or HDFS, HTTP, HTTPS, or FTP URLs.\n |  environment : dict, optional\n |      A dictionary of environment variables to set on\n |      worker nodes.\n |  batchSize : int, optional, default 0\n |      The number of Python objects represented as a single\n |      Java object. Set 1 to disable batching, 0 to automatically choose\n |      the batch size based on object sizes, or -1 to use an unlimited\n |      batch size\n |  serializer : :class:`Serializer`, optional, default :class:`CPickleSerializer`\n |      The serializer for RDDs.\n |  conf : :class:`SparkConf`, optional\n |      An object setting Spark properties.\n |  gateway : class:`py4j.java_gateway.JavaGateway`,  optional\n |      Use an existing gateway and JVM, otherwise a new JVM\n |      will be instantiated. This is only used internally.\n |  jsc : class:`py4j.java_gateway.JavaObject`, optional\n |      The JavaSparkContext instance. This is only used internally.\n |  profiler_cls : type, optional, default :class:`BasicProfiler`\n |      A class of custom Profiler used to do profiling\n |  udf_profiler_cls : type, optional, default :class:`UDFBasicProfiler`\n |      A class of custom Profiler used to do udf profiling\n |  \n |  Notes\n |  -----\n |  Only one :class:`SparkContext` should be active per JVM. You must `stop()`\n |  the active :class:`SparkContext` before creating a new one.\n |  \n |  :class:`SparkContext` instance is not supported to share across multiple\n |  processes out of the box, and PySpark does not guarantee multi-processing execution.\n |  Use threads instead for concurrent processing purpose.\n |  \n |  Examples\n |  --------\n |  >>> from pyspark.context import SparkContext\n |  >>> sc = SparkContext('local', 'test')\n |  >>> sc2 = SparkContext('local', 'test2') # doctest: +IGNORE_EXCEPTION_DETAIL\n |  Traceback (most recent call last):\n |      ...\n |  ValueError: ...\n |  \n |  Methods defined here:\n |  \n |  __enter__(self) -> 'SparkContext'\n |      Enable 'with SparkContext(...) as sc: app(sc)' syntax.\n |  \n |  __exit__(self, type: Optional[Type[BaseException]], value: Optional[BaseException], trace: Optional[traceback]) -> None\n |      Enable 'with SparkContext(...) as sc: app' syntax.\n |      \n |      Specifically stop the context on exit of the with block.\n |  \n |  __getnewargs__(self) -> NoReturn\n |  \n |  __init__(self, master: Optional[str] = None, appName: Optional[str] = None, sparkHome: Optional[str] = None, pyFiles: Optional[List[str]] = None, environment: Optional[Dict[str, Any]] = None, batchSize: int = 0, serializer: 'Serializer' = CloudPickleSerializer(), conf: Optional[pyspark.conf.SparkConf] = None, gateway: Optional[py4j.java_gateway.JavaGateway] = None, jsc: Optional[py4j.java_gateway.JavaObject] = None, profiler_cls: Type[pyspark.profiler.BasicProfiler] = <class 'pyspark.profiler.BasicProfiler'>, udf_profiler_cls: Type[pyspark.profiler.UDFBasicProfiler] = <class 'pyspark.profiler.UDFBasicProfiler'>, memory_profiler_cls: Type[pyspark.profiler.MemoryProfiler] = <class 'pyspark.profiler.MemoryProfiler'>)\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  __repr__(self) -> str\n |      Return repr(self).\n |  \n |  accumulator(self, value: ~T, accum_param: Optional[ForwardRef('AccumulatorParam[T]')] = None) -> 'Accumulator[T]'\n |      Create an :class:`Accumulator` with the given initial value, using a given\n |      :class:`AccumulatorParam` helper object to define how to add values of the\n |      data type if provided. Default AccumulatorParams are used for integers\n |      and floating-point numbers if you do not provide one. For other types,\n |      a custom AccumulatorParam can be used.\n |      \n |      .. versionadded:: 0.7.0\n |      \n |      Parameters\n |      ----------\n |      value : T\n |          initialized value\n |      accum_param : :class:`pyspark.AccumulatorParam`, optional\n |          helper object to define how to add values\n |      \n |      Returns\n |      -------\n |      :class:`Accumulator`\n |          `Accumulator` object, a shared variable that can be accumulated\n |      \n |      Examples\n |      --------\n |      >>> acc = sc.accumulator(9)\n |      >>> acc.value\n |      9\n |      >>> acc += 1\n |      >>> acc.value\n |      10\n |      \n |      Accumulator object can be accumulated in RDD operations:\n |      \n |      >>> rdd = sc.range(5)\n |      >>> def f(x):\n |      ...     global acc\n |      ...     acc += 1\n |      >>> rdd.foreach(f)\n |      >>> acc.value\n |      15\n |  \n |  addArchive(self, path: str) -> None\n |      Add an archive to be downloaded with this Spark job on every node.\n |      The `path` passed can be either a local file, a file in HDFS\n |      (or other Hadoop-supported filesystems), or an HTTP, HTTPS or\n |      FTP URI.\n |      \n |      To access the file in Spark jobs, use :meth:`SparkFiles.get` with the\n |      filename to find its download/unpacked location. The given path should\n |      be one of .zip, .tar, .tar.gz, .tgz and .jar.\n |      \n |      .. versionadded:: 3.3.0\n |      \n |      Parameters\n |      ----------\n |      path : str\n |          can be either a local file, a file in HDFS (or other Hadoop-supported\n |          filesystems), or an HTTP, HTTPS or FTP URI. To access the file in Spark jobs,\n |          use :meth:`SparkFiles.get` to find its download location.\n |      \n |      See Also\n |      --------\n |      :meth:`SparkFiles.get`\n |      \n |      Notes\n |      -----\n |      A path can be added only once. Subsequent additions of the same path are ignored.\n |      This API is experimental.\n |      \n |      Examples\n |      --------\n |      Creates a zipped file that contains a text file written '100'.\n |      \n |      >>> import os\n |      >>> import tempfile\n |      >>> import zipfile\n |      >>> from pyspark import SparkFiles\n |      \n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     path = os.path.join(d, \"test.txt\")\n |      ...     with open(path, \"w\") as f:\n |      ...         _ = f.write(\"100\")\n |      ...\n |      ...     zip_path1 = os.path.join(d, \"test1.zip\")\n |      ...     with zipfile.ZipFile(zip_path1, \"w\", zipfile.ZIP_DEFLATED) as z:\n |      ...         z.write(path, os.path.basename(path))\n |      ...\n |      ...     sc.addArchive(zip_path1)\n |      ...\n |      ...     def func(iterator):\n |      ...         with open(\"%s/test.txt\" % SparkFiles.get(\"test1.zip\")) as f:\n |      ...             mul = int(f.readline())\n |      ...             return [x * mul for x in iterator]\n |      ...\n |      ...     collected = sc.parallelize([1, 2, 3, 4]).mapPartitions(func).collect()\n |      \n |      >>> collected\n |      [100, 200, 300, 400]\n |  \n |  addClusterWideLibraryToPath(self, libname: str) -> None\n |      # add cluster wide library to the system path. This is called by libraries installed\n |      # through addPyFile, library installation UI or command line (databricks libraries install)\n |  \n |  addFile(self, path: str, recursive: bool = False) -> None\n |      Add a file to be downloaded with this Spark job on every node.\n |      The `path` passed can be either a local file, a file in HDFS\n |      (or other Hadoop-supported filesystems), or an HTTP, HTTPS or\n |      FTP URI.\n |      \n |      To access the file in Spark jobs, use :meth:`SparkFiles.get` with the\n |      filename to find its download location.\n |      \n |      A directory can be given if the recursive option is set to True.\n |      Currently directories are only supported for Hadoop-supported filesystems.\n |      \n |      .. versionadded:: 0.7.0\n |      \n |      Parameters\n |      ----------\n |      path : str\n |          can be either a local file, a file in HDFS (or other Hadoop-supported\n |          filesystems), or an HTTP, HTTPS or FTP URI. To access the file in Spark jobs,\n |          use :meth:`SparkFiles.get` to find its download location.\n |      recursive : bool, default False\n |          whether to recursively add files in the input directory\n |      \n |      See Also\n |      --------\n |      :meth:`SparkContext.addPyFile`\n |      :meth:`SparkFiles.get`\n |      \n |      Notes\n |      -----\n |      A path can be added only once. Subsequent additions of the same path are ignored.\n |      \n |      Examples\n |      --------\n |      >>> import os\n |      >>> import tempfile\n |      >>> from pyspark import SparkFiles\n |      \n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     path1 = os.path.join(d, \"test1.txt\")\n |      ...     with open(path1, \"w\") as f:\n |      ...         _ = f.write(\"100\")\n |      ...\n |      ...     sc.addFile(path1)\n |      ...\n |      ...     def func(iterator):\n |      ...         with open(SparkFiles.get(\"test1.txt\")) as f:\n |      ...             mul = int(f.readline())\n |      ...             return [x * mul for x in iterator]\n |      ...\n |      ...     collected = sc.parallelize([1, 2, 3, 4]).mapPartitions(func).collect()\n |      >>> collected\n |      [100, 200, 300, 400]\n |  \n |  addIsolatedLibraryPath(self, libname: str) -> None\n |      For a .egg, .zip, or .jar library, add the path to sys.path, and add it to\n |      _python_includes for passing it to executor.\n |  \n |  addPyFile(self, path: str) -> None\n |      Add a .py or .zip dependency for all tasks to be executed on this\n |      SparkContext in the future.  The `path` passed can be either a local\n |      file, a file in HDFS (or other Hadoop-supported filesystems), or an\n |      HTTP, HTTPS or FTP URI.\n |      \n |      .. versionadded:: 0.7.0\n |      \n |      Parameters\n |      ----------\n |      path : str\n |          can be either a .py file or .zip dependency.\n |      \n |      See Also\n |      --------\n |      :meth:`SparkContext.addFile`\n |      \n |      Notes\n |      -----\n |      A path can be added only once. Subsequent additions of the same path are ignored.\n |  \n |  binaryFiles(self, path: str, minPartitions: Optional[int] = None) -> pyspark.rdd.RDD[typing.Tuple[str, bytes]]\n |      Read a directory of binary files from HDFS, a local file system\n |      (available on all nodes), or any Hadoop-supported file system URI\n |      as a byte array. Each file is read as a single record and returned\n |      in a key-value pair, where the key is the path of each file, the\n |      value is the content of each file.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Parameters\n |      ----------\n |      path : str\n |          directory to the input data files, the path can be comma separated\n |          paths as a list of inputs\n |      minPartitions : int, optional\n |          suggested minimum number of partitions for the resulting RDD\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          RDD representing path-content pairs from the file(s).\n |      \n |      Notes\n |      -----\n |      Small files are preferred, large file is also allowable, but may cause bad performance.\n |      \n |      See Also\n |      --------\n |      :meth:`SparkContext.binaryRecords`\n |      \n |      Examples\n |      --------\n |      >>> import os\n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a temporary binary file\n |      ...     with open(os.path.join(d, \"1.bin\"), \"wb\") as f1:\n |      ...         _ = f1.write(b\"binary data I\")\n |      ...\n |      ...     # Write another temporary binary file\n |      ...     with open(os.path.join(d, \"2.bin\"), \"wb\") as f2:\n |      ...         _ = f2.write(b\"binary data II\")\n |      ...\n |      ...     collected = sorted(sc.binaryFiles(d).collect())\n |      \n |      >>> collected\n |      [('.../1.bin', b'binary data I'), ('.../2.bin', b'binary data II')]\n |  \n |  binaryRecords(self, path: str, recordLength: int) -> pyspark.rdd.RDD[bytes]\n |      Load data from a flat binary file, assuming each record is a set of numbers\n |      with the specified numerical format (see ByteBuffer), and the number of\n |      bytes per record is constant.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Parameters\n |      ----------\n |      path : str\n |          Directory to the input data files\n |      recordLength : int\n |          The length at which to split the records\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          RDD of data with values, represented as byte arrays\n |      \n |      See Also\n |      --------\n |      :meth:`SparkContext.binaryFiles`\n |      \n |      Examples\n |      --------\n |      >>> import os\n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a temporary file\n |      ...     with open(os.path.join(d, \"1.bin\"), \"w\") as f:\n |      ...         for i in range(3):\n |      ...             _ = f.write(\"%04d\" % i)\n |      ...\n |      ...     # Write another file\n |      ...     with open(os.path.join(d, \"2.bin\"), \"w\") as f:\n |      ...         for i in [-1, -2, -10]:\n |      ...             _ = f.write(\"%04d\" % i)\n |      ...\n |      ...     collected = sorted(sc.binaryRecords(d, 4).collect())\n |      \n |      >>> collected\n |      [b'-001', b'-002', b'-010', b'0000', b'0001', b'0002']\n |  \n |  broadcast(self, value: ~T) -> 'Broadcast[T]'\n |      Broadcast a read-only variable to the cluster, returning a :class:`Broadcast`\n |      object for reading it in distributed functions. The variable will\n |      be sent to each cluster only once.\n |      \n |      .. versionadded:: 0.7.0\n |      \n |      Parameters\n |      ----------\n |      value : T\n |          value to broadcast to the Spark nodes\n |      \n |      Returns\n |      -------\n |      :class:`Broadcast`\n |          :class:`Broadcast` object, a read-only variable cached on each machine\n |      \n |      Examples\n |      --------\n |      >>> mapping = {1: 10001, 2: 10002}\n |      >>> bc = sc.broadcast(mapping)\n |      \n |      >>> rdd = sc.range(5)\n |      >>> rdd2 = rdd.map(lambda i: bc.value[i] if i in bc.value else -1)\n |      >>> rdd2.collect()  # doctest: +SKIP\n |      [-1, 10001, 10002, -1, -1]\n |      \n |      >>> bc.destroy()  # doctest: +SKIP\n |  \n |  cancelAllJobs(self) -> None\n |      Cancel all jobs that have been scheduled or are running.\n |      \n |      .. versionadded:: 1.1.0\n |      \n |      See Also\n |      --------\n |      :meth:`SparkContext.cancelJobGroup`\n |      :meth:`SparkContext.runJob`\n |  \n |  cancelJobGroup(self, groupId: str) -> None\n |      Cancel active jobs for the specified group. See :meth:`SparkContext.setJobGroup`.\n |      for more information.\n |      \n |      .. versionadded:: 1.1.0\n |      \n |      Parameters\n |      ----------\n |      groupId : str\n |          The group ID to cancel the job.\n |      \n |      See Also\n |      --------\n |      :meth:`SparkContext.setJobGroup`\n |      :meth:`SparkContext.cancelJobGroup`\n |  \n |  dump_profiles(self, path: str) -> None\n |      Dump the profile stats into directory `path`\n |      \n |      .. versionadded:: 1.2.0\n |      \n |      See Also\n |      --------\n |      :meth:`SparkContext.show_profiles`\n |  \n |  emptyRDD(self) -> pyspark.rdd.RDD[typing.Any]\n |      Create an :class:`RDD` that has no partitions or elements.\n |      \n |      .. versionadded:: 1.5.0\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          An empty RDD\n |      \n |      Examples\n |      --------\n |      >>> sc.emptyRDD()\n |      EmptyRDD...\n |      >>> sc.emptyRDD().count()\n |      0\n |  \n |  getCheckpointDir(self) -> Optional[str]\n |      Return the directory where RDDs are checkpointed. Returns None if no\n |      checkpoint directory has been set.\n |      \n |      .. versionadded:: 3.1.0\n |      \n |      See Also\n |      --------\n |      :meth:`SparkContext.setCheckpointDir`\n |      :meth:`RDD.checkpoint`\n |      :meth:`RDD.getCheckpointFile`\n |  \n |  getConf(self) -> pyspark.conf.SparkConf\n |      Return a copy of this SparkContext's configuration :class:`SparkConf`.\n |      \n |      .. versionadded:: 2.1.0\n |  \n |  getJobGroupId(self) -> Optional[str]\n |      Get a job group ID in this thread, or `None` if it is missing.\n |  \n |  getLocalProperty(self, key: str) -> Optional[str]\n |      Get a local property set in this thread, or null if it is missing. See\n |      :meth:`setLocalProperty`.\n |      \n |      .. versionadded:: 1.0.0\n |      \n |      See Also\n |      --------\n |      :meth:`SparkContext.setLocalProperty`\n |  \n |  hadoopFile(self, path: str, inputFormatClass: str, keyClass: str, valueClass: str, keyConverter: Optional[str] = None, valueConverter: Optional[str] = None, conf: Optional[Dict[str, str]] = None, batchSize: int = 0) -> pyspark.rdd.RDD[typing.Tuple[~T, ~U]]\n |      Read an 'old' Hadoop InputFormat with arbitrary key and value class from HDFS,\n |      a local file system (available on all nodes), or any Hadoop-supported file system URI.\n |      The mechanism is the same as for meth:`SparkContext.sequenceFile`.\n |      \n |      .. versionadded:: 1.1.0\n |      \n |      A Hadoop configuration can be passed in as a Python dict. This will be converted into a\n |      Configuration in Java.\n |      \n |      Parameters\n |      ----------\n |      path : str\n |          path to Hadoop file\n |      inputFormatClass : str\n |          fully qualified classname of Hadoop InputFormat\n |          (e.g. \"org.apache.hadoop.mapreduce.lib.input.TextInputFormat\")\n |      keyClass : str\n |          fully qualified classname of key Writable class (e.g. \"org.apache.hadoop.io.Text\")\n |      valueClass : str\n |          fully qualified classname of value Writable class\n |          (e.g. \"org.apache.hadoop.io.LongWritable\")\n |      keyConverter : str, optional\n |          fully qualified name of a function returning key WritableConverter\n |      valueConverter : str, optional\n |          fully qualified name of a function returning value WritableConverter\n |      conf : dict, optional\n |          Hadoop configuration, passed in as a dict\n |      batchSize : int, optional, default 0\n |          The number of Python objects represented as a single\n |          Java object. (default 0, choose batchSize automatically)\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          RDD of tuples of key and corresponding value\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.saveAsSequenceFile`\n |      :meth:`RDD.saveAsNewAPIHadoopFile`\n |      :meth:`RDD.saveAsHadoopFile`\n |      :meth:`SparkContext.newAPIHadoopFile`\n |      :meth:`SparkContext.hadoopRDD`\n |      \n |      Examples\n |      --------\n |      >>> import os\n |      >>> import tempfile\n |      \n |      Set the related classes\n |      \n |      >>> output_format_class = \"org.apache.hadoop.mapred.TextOutputFormat\"\n |      >>> input_format_class = \"org.apache.hadoop.mapred.TextInputFormat\"\n |      >>> key_class = \"org.apache.hadoop.io.IntWritable\"\n |      >>> value_class = \"org.apache.hadoop.io.Text\"\n |      \n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     path = os.path.join(d, \"old_hadoop_file\")\n |      ...\n |      ...     # Write a temporary Hadoop file\n |      ...     rdd = sc.parallelize([(1, \"\"), (1, \"a\"), (3, \"x\")])\n |      ...     rdd.saveAsHadoopFile(path, output_format_class, key_class, value_class)\n |      ...\n |      ...     loaded = sc.hadoopFile(path, input_format_class, key_class, value_class)\n |      ...     collected = sorted(loaded.collect())\n |      \n |      >>> collected\n |      [(0, '1\\t'), (0, '1\\ta'), (0, '3\\tx')]\n |  \n |  hadoopRDD(self, inputFormatClass: str, keyClass: str, valueClass: str, keyConverter: Optional[str] = None, valueConverter: Optional[str] = None, conf: Optional[Dict[str, str]] = None, batchSize: int = 0) -> pyspark.rdd.RDD[typing.Tuple[~T, ~U]]\n |      Read an 'old' Hadoop InputFormat with arbitrary key and value class, from an arbitrary\n |      Hadoop configuration, which is passed in as a Python dict.\n |      This will be converted into a Configuration in Java.\n |      The mechanism is the same as for meth:`SparkContext.sequenceFile`.\n |      \n |      .. versionadded:: 1.1.0\n |      \n |      Parameters\n |      ----------\n |      inputFormatClass : str\n |          fully qualified classname of Hadoop InputFormat\n |          (e.g. \"org.apache.hadoop.mapreduce.lib.input.TextInputFormat\")\n |      keyClass : str\n |          fully qualified classname of key Writable class (e.g. \"org.apache.hadoop.io.Text\")\n |      valueClass : str\n |          fully qualified classname of value Writable class\n |          (e.g. \"org.apache.hadoop.io.LongWritable\")\n |      keyConverter : str, optional\n |          fully qualified name of a function returning key WritableConverter\n |      valueConverter : str, optional\n |          fully qualified name of a function returning value WritableConverter\n |      conf : dict, optional\n |          Hadoop configuration, passed in as a dict\n |      batchSize : int, optional, default 0\n |          The number of Python objects represented as a single\n |          Java object. (default 0, choose batchSize automatically)\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          RDD of tuples of key and corresponding value\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.saveAsNewAPIHadoopDataset`\n |      :meth:`RDD.saveAsHadoopDataset`\n |      :meth:`SparkContext.newAPIHadoopRDD`\n |      :meth:`SparkContext.hadoopFile`\n |      \n |      Examples\n |      --------\n |      >>> import os\n |      >>> import tempfile\n |      \n |      Set the related classes\n |      \n |      >>> output_format_class = \"org.apache.hadoop.mapred.TextOutputFormat\"\n |      >>> input_format_class = \"org.apache.hadoop.mapred.TextInputFormat\"\n |      >>> key_class = \"org.apache.hadoop.io.IntWritable\"\n |      >>> value_class = \"org.apache.hadoop.io.Text\"\n |      \n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     path = os.path.join(d, \"old_hadoop_file\")\n |      ...\n |      ...     # Create the conf for writing\n |      ...     write_conf = {\n |      ...         \"mapred.output.format.class\": output_format_class,\n |      ...         \"mapreduce.job.output.key.class\": key_class,\n |      ...         \"mapreduce.job.output.value.class\": value_class,\n |      ...         \"mapreduce.output.fileoutputformat.outputdir\": path,\n |      ...     }\n |      ...\n |      ...     # Write a temporary Hadoop file\n |      ...     rdd = sc.parallelize([(1, \"\"), (1, \"a\"), (3, \"x\")])\n |      ...     rdd.saveAsHadoopDataset(conf=write_conf)\n |      ...\n |      ...     # Create the conf for reading\n |      ...     read_conf = {\"mapreduce.input.fileinputformat.inputdir\": path}\n |      ...\n |      ...     loaded = sc.hadoopRDD(input_format_class, key_class, value_class, conf=read_conf)\n |      ...     collected = sorted(loaded.collect())\n |      \n |      >>> collected\n |      [(0, '1\\t'), (0, '1\\ta'), (0, '3\\tx')]\n |  \n |  init_batched_serializer(self, data: Iterable[~T], numSlices: Optional[int] = None) -> 'BatchedSerializer'\n |      Init the batched serializer with the data to parallelize into an RDD and numSlices for the\n |      batch size.\n |  \n |  newAPIHadoopFile(self, path: str, inputFormatClass: str, keyClass: str, valueClass: str, keyConverter: Optional[str] = None, valueConverter: Optional[str] = None, conf: Optional[Dict[str, str]] = None, batchSize: int = 0) -> pyspark.rdd.RDD[typing.Tuple[~T, ~U]]\n |      Read a 'new API' Hadoop InputFormat with arbitrary key and value class fr\n\n*** WARNING: max output size exceeded, skipping output. ***\n\n>> value_class = \"org.apache.hadoop.io.Text\"\n |      \n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     path = os.path.join(d, \"new_hadoop_file\")\n |      ...\n |      ...     # Create the conf for writing\n |      ...     write_conf = {\n |      ...         \"mapreduce.job.outputformat.class\": (output_format_class),\n |      ...         \"mapreduce.job.output.key.class\": key_class,\n |      ...         \"mapreduce.job.output.value.class\": value_class,\n |      ...         \"mapreduce.output.fileoutputformat.outputdir\": path,\n |      ...     }\n |      ...\n |      ...     # Write a temporary Hadoop file\n |      ...     rdd = sc.parallelize([(1, \"\"), (1, \"a\"), (3, \"x\")])\n |      ...     rdd.saveAsNewAPIHadoopDataset(conf=write_conf)\n |      ...\n |      ...     # Create the conf for reading\n |      ...     read_conf = {\"mapreduce.input.fileinputformat.inputdir\": path}\n |      ...\n |      ...     loaded = sc.newAPIHadoopRDD(input_format_class,\n |      ...         key_class, value_class, conf=read_conf)\n |      ...     collected = sorted(loaded.collect())\n |      \n |      >>> collected\n |      [(1, ''), (1, 'a'), (3, 'x')]\n |  \n |  parallelize(self, c: Iterable[~T], numSlices: Optional[int] = None) -> pyspark.rdd.RDD[~T]\n |      Distribute a local Python collection to form an RDD. Using range\n |      is recommended if the input represents a range for performance.\n |      \n |      .. versionadded:: 0.7.0\n |      \n |      Parameters\n |      ----------\n |      c : :class:`collections.abc.Iterable`\n |          iterable collection to distribute\n |      numSlices : int, optional\n |          the number of partitions of the new RDD\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          RDD representing distributed collection.\n |      \n |      Examples\n |      --------\n |      >>> sc.parallelize([0, 2, 3, 4, 6], 5).glom().collect()\n |      [[0], [2], [3], [4], [6]]\n |      >>> sc.parallelize(range(0, 6, 2), 5).glom().collect()\n |      [[], [0], [], [2], [4]]\n |      \n |      Deal with a list of strings.\n |      \n |      >>> strings = [\"a\", \"b\", \"c\"]\n |      >>> sc.parallelize(strings, 2).glom().collect()\n |      [['a'], ['b', 'c']]\n |  \n |  pickleFile(self, name: str, minPartitions: Optional[int] = None) -> pyspark.rdd.RDD[typing.Any]\n |      Load an RDD previously saved using :meth:`RDD.saveAsPickleFile` method.\n |      \n |      .. versionadded:: 1.1.0\n |      \n |      Parameters\n |      ----------\n |      name : str\n |          directory to the input data files, the path can be comma separated\n |          paths as a list of inputs\n |      minPartitions : int, optional\n |          suggested minimum number of partitions for the resulting RDD\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          RDD representing unpickled data from the file(s).\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.saveAsPickleFile`\n |      \n |      Examples\n |      --------\n |      >>> import os\n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a temporary pickled file\n |      ...     path1 = os.path.join(d, \"pickled1\")\n |      ...     sc.parallelize(range(10)).saveAsPickleFile(path1, 3)\n |      ...\n |      ...     # Write another temporary pickled file\n |      ...     path2 = os.path.join(d, \"pickled2\")\n |      ...     sc.parallelize(range(-10, -5)).saveAsPickleFile(path2, 3)\n |      ...\n |      ...     # Load picked file\n |      ...     collected1 = sorted(sc.pickleFile(path1, 3).collect())\n |      ...     collected2 = sorted(sc.pickleFile(path2, 4).collect())\n |      ...\n |      ...     # Load two picked files together\n |      ...     collected3 = sorted(sc.pickleFile('{},{}'.format(path1, path2), 5).collect())\n |      \n |      >>> collected1\n |      [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n |      >>> collected2\n |      [-10, -9, -8, -7, -6]\n |      >>> collected3\n |      [-10, -9, -8, -7, -6, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n |  \n |  range(self, start: int, end: Optional[int] = None, step: int = 1, numSlices: Optional[int] = None) -> pyspark.rdd.RDD[int]\n |      Create a new RDD of int containing elements from `start` to `end`\n |      (exclusive), increased by `step` every element. Can be called the same\n |      way as python's built-in range() function. If called with a single argument,\n |      the argument is interpreted as `end`, and `start` is set to 0.\n |      \n |      .. versionadded:: 1.5.0\n |      \n |      Parameters\n |      ----------\n |      start : int\n |          the start value\n |      end : int, optional\n |          the end value (exclusive)\n |      step : int, optional, default 1\n |          the incremental step\n |      numSlices : int, optional\n |          the number of partitions of the new RDD\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          An RDD of int\n |      \n |      See Also\n |      --------\n |      :meth:`pyspark.sql.SparkSession.range`\n |      \n |      Examples\n |      --------\n |      >>> sc.range(5).collect()\n |      [0, 1, 2, 3, 4]\n |      >>> sc.range(2, 4).collect()\n |      [2, 3]\n |      >>> sc.range(1, 7, 2).collect()\n |      [1, 3, 5]\n |      \n |      Generate RDD with a negative step\n |      \n |      >>> sc.range(5, 0, -1).collect()\n |      [5, 4, 3, 2, 1]\n |      >>> sc.range(0, 5, -1).collect()\n |      []\n |      \n |      Control the number of partitions\n |      \n |      >>> sc.range(5, numSlices=1).getNumPartitions()\n |      1\n |      >>> sc.range(5, numSlices=10).getNumPartitions()\n |      10\n |  \n |  runJob(self, rdd: pyspark.rdd.RDD[~T], partitionFunc: Callable[[Iterable[~T]], Iterable[~U]], partitions: Optional[Sequence[int]] = None, allowLocal: bool = False) -> List[~U]\n |      Executes the given partitionFunc on the specified set of partitions,\n |      returning the result as an array of elements.\n |      \n |      If 'partitions' is not specified, this will run over all partitions.\n |      \n |      .. versionadded:: 1.1.0\n |      \n |      Parameters\n |      ----------\n |      rdd : :class:`RDD`\n |          target RDD to run tasks on\n |      partitionFunc : function\n |          a function to run on each partition of the RDD\n |      partitions : list, optional\n |          set of partitions to run on; some jobs may not want to compute on all\n |          partitions of the target RDD, e.g. for operations like `first`\n |      allowLocal : bool, default False\n |          this parameter takes no effect\n |      \n |      Returns\n |      -------\n |      list\n |          results of specified partitions\n |      \n |      See Also\n |      --------\n |      :meth:`SparkContext.cancelAllJobs`\n |      \n |      Examples\n |      --------\n |      >>> myRDD = sc.parallelize(range(6), 3)\n |      >>> sc.runJob(myRDD, lambda part: [x * x for x in part])\n |      [0, 1, 4, 9, 16, 25]\n |      \n |      >>> myRDD = sc.parallelize(range(6), 3)\n |      >>> sc.runJob(myRDD, lambda part: [x * x for x in part], [0, 2], True)\n |      [0, 1, 16, 25]\n |  \n |  sequenceFile(self, path: str, keyClass: Optional[str] = None, valueClass: Optional[str] = None, keyConverter: Optional[str] = None, valueConverter: Optional[str] = None, minSplits: Optional[int] = None, batchSize: int = 0) -> pyspark.rdd.RDD[typing.Tuple[~T, ~U]]\n |      Read a Hadoop SequenceFile with arbitrary key and value Writable class from HDFS,\n |      a local file system (available on all nodes), or any Hadoop-supported file system URI.\n |      The mechanism is as follows:\n |      \n |          1. A Java RDD is created from the SequenceFile or other InputFormat, and the key\n |             and value Writable classes\n |          2. Serialization is attempted via Pickle pickling\n |          3. If this fails, the fallback is to call 'toString' on each key and value\n |          4. :class:`CPickleSerializer` is used to deserialize pickled objects on the Python side\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Parameters\n |      ----------\n |      path : str\n |          path to sequencefile\n |      keyClass: str, optional\n |          fully qualified classname of key Writable class (e.g. \"org.apache.hadoop.io.Text\")\n |      valueClass : str, optional\n |          fully qualified classname of value Writable class\n |          (e.g. \"org.apache.hadoop.io.LongWritable\")\n |      keyConverter : str, optional\n |          fully qualified name of a function returning key WritableConverter\n |      valueConverter : str, optional\n |          fully qualifiedname of a function returning value WritableConverter\n |      minSplits : int, optional\n |          minimum splits in dataset (default min(2, sc.defaultParallelism))\n |      batchSize : int, optional, default 0\n |          The number of Python objects represented as a single\n |          Java object. (default 0, choose batchSize automatically)\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          RDD of tuples of key and corresponding value\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.saveAsSequenceFile`\n |      :meth:`RDD.saveAsNewAPIHadoopFile`\n |      :meth:`RDD.saveAsHadoopFile`\n |      :meth:`SparkContext.newAPIHadoopFile`\n |      :meth:`SparkContext.hadoopFile`\n |      \n |      Examples\n |      --------\n |      >>> import os\n |      >>> import tempfile\n |      \n |      Set the class of output format\n |      \n |      >>> output_format_class = \"org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat\"\n |      \n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     path = os.path.join(d, \"hadoop_file\")\n |      ...\n |      ...     # Write a temporary Hadoop file\n |      ...     rdd = sc.parallelize([(1, {3.0: \"bb\"}), (2, {1.0: \"aa\"}), (3, {2.0: \"dd\"})])\n |      ...     rdd.saveAsNewAPIHadoopFile(path, output_format_class)\n |      ...\n |      ...     collected = sorted(sc.sequenceFile(path).collect())\n |      \n |      >>> collected\n |      [(1, {3.0: 'bb'}), (2, {1.0: 'aa'}), (3, {2.0: 'dd'})]\n |  \n |  setCheckpointDir(self, dirName: str) -> None\n |      Set the directory under which RDDs are going to be checkpointed. The\n |      directory must be an HDFS path if running on a cluster.\n |      \n |      .. versionadded:: 0.7.0\n |      \n |      Parameters\n |      ----------\n |      dirName : str\n |          path to the directory where checkpoint files will be stored\n |          (must be HDFS path if running in cluster)\n |      \n |      See Also\n |      --------\n |      :meth:`SparkContext.getCheckpointDir`\n |      :meth:`RDD.checkpoint`\n |      :meth:`RDD.getCheckpointFile`\n |  \n |  setJobDescription(self, value: str) -> None\n |      Set a human readable description of the current job.\n |      \n |      .. versionadded:: 2.3.0\n |      \n |      Parameters\n |      ----------\n |      value : str\n |          The job description to set.\n |      \n |      Notes\n |      -----\n |      If you run jobs in parallel, use :class:`pyspark.InheritableThread` for thread\n |      local inheritance.\n |  \n |  setJobGroup(self, groupId: str, description: str, interruptOnCancel: bool = False) -> None\n |      Assigns a group ID to all the jobs started by this thread until the group ID is set to a\n |      different value or cleared.\n |      \n |      Often, a unit of execution in an application consists of multiple Spark actions or jobs.\n |      Application programmers can use this method to group all those jobs together and give a\n |      group description. Once set, the Spark web UI will associate such jobs with this group.\n |      \n |      The application can use :meth:`SparkContext.cancelJobGroup` to cancel all\n |      running jobs in this group.\n |      \n |      .. versionadded:: 1.0.0\n |      \n |      Parameters\n |      ----------\n |      groupId : str\n |          The group ID to assign.\n |      description : str\n |          The description to set for the job group.\n |      interruptOnCancel : bool, optional, default False\n |          whether to interrupt jobs on job cancellation.\n |      \n |      Notes\n |      -----\n |      If interruptOnCancel is set to true for the job group, then job cancellation will result\n |      in Thread.interrupt() being called on the job's executor threads. This is useful to help\n |      ensure that the tasks are actually stopped in a timely manner, but is off by default due\n |      to HDFS-1208, where HDFS may respond to Thread.interrupt() by marking nodes as dead.\n |      \n |      If you run jobs in parallel, use :class:`pyspark.InheritableThread` for thread\n |      local inheritance.\n |      \n |      See Also\n |      --------\n |      :meth:`SparkContext.cancelJobGroup`\n |      \n |      Examples\n |      --------\n |      >>> import threading\n |      >>> from time import sleep\n |      >>> from pyspark import InheritableThread\n |      >>> result = \"Not Set\"\n |      >>> lock = threading.Lock()\n |      >>> def map_func(x):\n |      ...     sleep(100)\n |      ...     raise RuntimeError(\"Task should have been cancelled\")\n |      >>> def start_job(x):\n |      ...     global result\n |      ...     try:\n |      ...         sc.setJobGroup(\"job_to_cancel\", \"some description\")\n |      ...         result = sc.parallelize(range(x)).map(map_func).collect()\n |      ...     except Exception as e:\n |      ...         result = \"Cancelled\"\n |      ...     lock.release()\n |      >>> def stop_job():\n |      ...     sleep(5)\n |      ...     sc.cancelJobGroup(\"job_to_cancel\")\n |      >>> suppress = lock.acquire()\n |      >>> suppress = InheritableThread(target=start_job, args=(10,)).start()\n |      >>> suppress = InheritableThread(target=stop_job).start()\n |      >>> suppress = lock.acquire()\n |      >>> print(result)\n |      Cancelled\n |  \n |  setLocalProperty(self, key: str, value: str) -> None\n |      Set a local property that affects jobs submitted from this thread, such as the\n |      Spark fair scheduler pool.\n |      \n |      .. versionadded:: 1.0.0\n |      \n |      Parameters\n |      ----------\n |      key : str\n |          The key of the local property to set.\n |      value : str\n |          The value of the local property to set.\n |      \n |      See Also\n |      --------\n |      :meth:`SparkContext.getLocalProperty`\n |      \n |      Notes\n |      -----\n |      If you run jobs in parallel, use :class:`pyspark.InheritableThread` for thread\n |      local inheritance.\n |  \n |  setLogLevel(self, logLevel: str) -> None\n |      Control our logLevel. This overrides any user-defined log settings.\n |      Valid log levels include: ALL, DEBUG, ERROR, FATAL, INFO, OFF, TRACE, WARN\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      Parameters\n |      ----------\n |      logLevel : str\n |          The desired log level as a string.\n |      \n |      Examples\n |      --------\n |      >>> sc.setLogLevel(\"WARN\")  # doctest :+SKIP\n |  \n |  show_profiles(self) -> None\n |      Print the profile stats to stdout\n |      \n |      .. versionadded:: 1.2.0\n |      \n |      See Also\n |      --------\n |      :meth:`SparkContext.dump_profiles`\n |  \n |  sparkUser(self) -> str\n |      Get SPARK_USER for user who is running SparkContext.\n |      \n |      .. versionadded:: 1.0.0\n |  \n |  statusTracker(self) -> pyspark.status.StatusTracker\n |      Return :class:`StatusTracker` object\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  stop(self) -> None\n |      Shut down the :class:`SparkContext`.\n |      \n |      .. versionadded:: 0.7.0\n |  \n |  textFile(self, name: str, minPartitions: Optional[int] = None, use_unicode: bool = True) -> pyspark.rdd.RDD[str]\n |      Read a text file from HDFS, a local file system (available on all\n |      nodes), or any Hadoop-supported file system URI, and return it as an\n |      RDD of Strings. The text files must be encoded as UTF-8.\n |      \n |      .. versionadded:: 0.7.0\n |      \n |      Parameters\n |      ----------\n |      name : str\n |          directory to the input data files, the path can be comma separated\n |          paths as a list of inputs\n |      minPartitions : int, optional\n |          suggested minimum number of partitions for the resulting RDD\n |      use_unicode : bool, default True\n |          If `use_unicode` is False, the strings will be kept as `str` (encoding\n |          as `utf-8`), which is faster and smaller than unicode.\n |      \n |          .. versionadded:: 1.2.0\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          RDD representing text data from the file(s).\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.saveAsTextFile`\n |      :meth:`SparkContext.wholeTextFiles`\n |      \n |      Examples\n |      --------\n |      >>> import os\n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     path1 = os.path.join(d, \"text1\")\n |      ...     path2 = os.path.join(d, \"text2\")\n |      ...\n |      ...     # Write a temporary text file\n |      ...     sc.parallelize([\"x\", \"y\", \"z\"]).saveAsTextFile(path1)\n |      ...\n |      ...     # Write another temporary text file\n |      ...     sc.parallelize([\"aa\", \"bb\", \"cc\"]).saveAsTextFile(path2)\n |      ...\n |      ...     # Load text file\n |      ...     collected1 = sorted(sc.textFile(path1, 3).collect())\n |      ...     collected2 = sorted(sc.textFile(path2, 4).collect())\n |      ...\n |      ...     # Load two text files together\n |      ...     collected3 = sorted(sc.textFile('{},{}'.format(path1, path2), 5).collect())\n |      \n |      >>> collected1\n |      ['x', 'y', 'z']\n |      >>> collected2\n |      ['aa', 'bb', 'cc']\n |      >>> collected3\n |      ['aa', 'bb', 'cc', 'x', 'y', 'z']\n |  \n |  union(self, rdds: List[pyspark.rdd.RDD[~T]]) -> pyspark.rdd.RDD[~T]\n |      Build the union of a list of RDDs.\n |      \n |      This supports unions() of RDDs with different serialized formats,\n |      although this forces them to be reserialized using the default\n |      serializer:\n |      \n |      .. versionadded:: 0.7.0\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.union`\n |      \n |      Examples\n |      --------\n |      >>> import os\n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # generate a text RDD\n |      ...     with open(os.path.join(d, \"union-text.txt\"), \"w\") as f:\n |      ...         _ = f.write(\"Hello\")\n |      ...     text_rdd = sc.textFile(d)\n |      ...\n |      ...     # generate another RDD\n |      ...     parallelized = sc.parallelize([\"World!\"])\n |      ...\n |      ...     unioned = sorted(sc.union([text_rdd, parallelized]).collect())\n |      \n |      >>> unioned\n |      ['Hello', 'World!']\n |  \n |  wholeTextFiles(self, path: str, minPartitions: Optional[int] = None, use_unicode: bool = True) -> pyspark.rdd.RDD[typing.Tuple[str, str]]\n |      Read a directory of text files from HDFS, a local file system\n |      (available on all nodes), or any  Hadoop-supported file system\n |      URI. Each file is read as a single record and returned in a\n |      key-value pair, where the key is the path of each file, the\n |      value is the content of each file.\n |      The text files must be encoded as UTF-8.\n |      \n |      .. versionadded:: 1.0.0\n |      \n |      For example, if you have the following files:\n |      \n |      .. code-block:: text\n |      \n |          hdfs://a-hdfs-path/part-00000\n |          hdfs://a-hdfs-path/part-00001\n |          ...\n |          hdfs://a-hdfs-path/part-nnnnn\n |      \n |      Do ``rdd = sparkContext.wholeTextFiles(\"hdfs://a-hdfs-path\")``,\n |      then ``rdd`` contains:\n |      \n |      .. code-block:: text\n |      \n |          (a-hdfs-path/part-00000, its content)\n |          (a-hdfs-path/part-00001, its content)\n |          ...\n |          (a-hdfs-path/part-nnnnn, its content)\n |      \n |      Parameters\n |      ----------\n |      path : str\n |          directory to the input data files, the path can be comma separated\n |          paths as a list of inputs\n |      minPartitions : int, optional\n |          suggested minimum number of partitions for the resulting RDD\n |      use_unicode : bool, default True\n |          If `use_unicode` is False, the strings will be kept as `str` (encoding\n |          as `utf-8`), which is faster and smaller than unicode.\n |      \n |          .. versionadded:: 1.2.0\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          RDD representing path-content pairs from the file(s).\n |      \n |      Notes\n |      -----\n |      Small files are preferred, as each file will be loaded fully in memory.\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.saveAsTextFile`\n |      :meth:`SparkContext.textFile`\n |      \n |      Examples\n |      --------\n |      >>> import os\n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a temporary text file\n |      ...     with open(os.path.join(d, \"1.txt\"), \"w\") as f:\n |      ...         _ = f.write(\"123\")\n |      ...\n |      ...     # Write another temporary text file\n |      ...     with open(os.path.join(d, \"2.txt\"), \"w\") as f:\n |      ...         _ = f.write(\"xyz\")\n |      ...\n |      ...     collected = sorted(sc.wholeTextFiles(d).collect())\n |      >>> collected\n |      [('.../1.txt', '123'), ('.../2.txt', 'xyz')]\n |  \n |  ----------------------------------------------------------------------\n |  Class methods defined here:\n |  \n |  getOrCreate(conf: Optional[pyspark.conf.SparkConf] = None) -> 'SparkContext' from builtins.type\n |      Get or instantiate a :class:`SparkContext` and register it as a singleton object.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      Parameters\n |      ----------\n |      conf : :class:`SparkConf`, optional\n |          :class:`SparkConf` that will be used for initialization of the :class:`SparkContext`.\n |      \n |      Returns\n |      -------\n |      :class:`SparkContext`\n |          current :class:`SparkContext`, or a new one if it wasn't created before the function\n |          call.\n |      \n |      Examples\n |      --------\n |      >>> SparkContext.getOrCreate()\n |      <SparkContext ...>\n |  \n |  setSystemProperty(key: str, value: str) -> None from builtins.type\n |      Set a Java system property, such as `spark.executor.memory`. This must\n |      be invoked before instantiating :class:`SparkContext`.\n |      \n |      .. versionadded:: 0.9.0\n |      \n |      Parameters\n |      ----------\n |      key : str\n |          The key of a new Java system property.\n |      value : str\n |          The value of a new Java system property.\n |  \n |  ----------------------------------------------------------------------\n |  Readonly properties defined here:\n |  \n |  applicationId\n |      A unique identifier for the Spark application.\n |      Its format depends on the scheduler implementation.\n |      \n |      * in case of local spark app something like 'local-1433865536131'\n |      * in case of YARN something like 'application_1433865536131_34483'\n |      \n |      .. versionadded:: 1.5.0\n |      \n |      Examples\n |      --------\n |      >>> sc.applicationId  # doctest: +ELLIPSIS\n |      'local-...'\n |  \n |  defaultMinPartitions\n |      Default min number of partitions for Hadoop RDDs when not given by user\n |      \n |      .. versionadded:: 1.1.0\n |      \n |      Examples\n |      --------\n |      >>> sc.defaultMinPartitions > 0\n |      True\n |  \n |  defaultParallelism\n |      Default level of parallelism to use when not given by user (e.g. for reduce tasks)\n |      \n |      .. versionadded:: 0.7.0\n |      \n |      Examples\n |      --------\n |      >>> sc.defaultParallelism > 0\n |      True\n |  \n |  resources\n |      Return the resource information of this :class:`SparkContext`.\n |      A resource could be a GPU, FPGA, etc.\n |      \n |      .. versionadded:: 3.0.0\n |  \n |  startTime\n |      Return the epoch time when the :class:`SparkContext` was started.\n |      \n |      .. versionadded:: 1.5.0\n |      \n |      Examples\n |      --------\n |      >>> _ = sc.startTime\n |  \n |  uiWebUrl\n |      Return the URL of the SparkUI instance started by this :class:`SparkContext`\n |      \n |      .. versionadded:: 2.1.0\n |      \n |      Notes\n |      -----\n |      When the web ui is disabled, e.g., by ``spark.ui.enabled`` set to ``False``,\n |      it returns ``None``.\n |      \n |      Examples\n |      --------\n |      >>> sc.uiWebUrl\n |      'http://...'\n |  \n |  version\n |      The version of Spark on which this application is running.\n |      \n |      .. versionadded:: 1.1.0\n |      \n |      Examples\n |      --------\n |      >>> _ = sc.version\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes defined here:\n |  \n |  PACKAGE_EXTENSIONS = ('.zip', '.egg', '.jar')\n |  \n |  __annotations__ = {'PACKAGE_EXTENSIONS': typing.Iterable[str], '_activ...\n\n"
     ]
    }
   ],
   "source": [
    "help(SparkContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46de1c98-8b14-4a85-91f9-bfbf8b3b8d88",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version is 3.3.2\nPython version is 3.9\nThe name of the master is local[8]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Spark version is {sc.version}\")\n",
    "\n",
    "print(f\"Python version is {sc.pythonVer}\")\n",
    "\n",
    "print(f\"The name of the master is {sc.master}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67326381-b032-4b46-8659-92f9cf466937",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[20]: [('spark.databricks.preemption.enabled', 'true'),\n ('spark.sql.hive.metastore.jars', '/databricks/databricks-hive/*'),\n ('spark.driver.tempDirectory', '/local_disk0/tmp'),\n ('spark.sql.warehouse.dir', 'dbfs:/user/hive/warehouse'),\n ('spark.databricks.managedCatalog.clientClassName',\n  'com.databricks.managedcatalog.ManagedCatalogClientImpl'),\n ('spark.databricks.credential.scope.fs.gs.auth.access.tokenProviderClassName',\n  'com.databricks.backend.daemon.driver.credentials.CredentialScopeGCPTokenProvider'),\n ('spark.hadoop.fs.fcfs-s3.impl.disable.cache', 'true'),\n ('spark.sql.streaming.checkpointFileManagerClass',\n  'com.databricks.spark.sql.streaming.DatabricksCheckpointFileManager'),\n ('spark.databricks.service.dbutils.repl.backend',\n  'com.databricks.dbconnect.ReplDBUtils'),\n ('spark.hadoop.databricks.s3.verifyBucketExists.enabled', 'false'),\n ('spark.streaming.driver.writeAheadLog.allowBatching', 'true'),\n ('spark.databricks.clusterSource', 'UI'),\n ('spark.databricks.clusterUsageTags.driverContainerId',\n  '2f72e9da136e46b785a6e5bbb9c25505'),\n ('spark.hadoop.hive.server2.transport.mode', 'http'),\n ('spark.executor.memory', '8278m'),\n ('spark.hadoop.fs.cpfs-adl.impl.disable.cache', 'true'),\n ('spark.databricks.clusterUsageTags.hailEnabled', 'false'),\n ('spark.databricks.clusterUsageTags.clusterLogDeliveryEnabled', 'false'),\n ('spark.databricks.clusterUsageTags.containerType', 'LXC'),\n ('spark.hadoop.fs.s3a.assumed.role.credentials.provider',\n  'shaded.databricks.org.apache.hadoop.fs.s3a.DatabricksInstanceProfileCredentialsProvider'),\n ('spark.eventLog.enabled', 'false'),\n ('spark.databricks.clusterUsageTags.isIMv2Enabled', 'false'),\n ('spark.hadoop.fs.stage.impl.disable.cache', 'true'),\n ('spark.databricks.clusterUsageTags.containerZoneId', 'us-west-2a'),\n ('spark.hadoop.hive.hmshandler.retry.interval', '2000'),\n ('spark.executor.tempDirectory', '/local_disk0/tmp'),\n ('spark.hadoop.fs.azure.authorization.caching.enable', 'false'),\n ('spark.hadoop.fs.fcfs-abfss.impl',\n  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n ('spark.hadoop.mapred.output.committer.class',\n  'com.databricks.backend.daemon.data.client.DirectOutputCommitter'),\n ('spark.hadoop.hive.server2.thrift.http.port', '10000'),\n ('spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version', '2'),\n ('spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2S3', '0'),\n ('spark.sql.allowMultipleContexts', 'false'),\n ('spark.databricks.eventLog.enabled', 'true'),\n ('spark.home', '/databricks/spark'),\n ('spark.databricks.clusterUsageTags.clusterTargetWorkers', '0'),\n ('spark.hadoop.hive.server2.idle.operation.timeout', '7200000'),\n ('spark.task.reaper.enabled', 'true'),\n ('spark.databricks.clusterUsageTags.autoTerminationMinutes', '60'),\n ('spark.storage.memoryFraction', '0.5'),\n ('eventLog.rolloverIntervalSeconds', '900'),\n ('spark.databricks.clusterUsageTags.orgId', '3007032361860494'),\n ('spark.databricks.clusterUsageTags.clusterFirstOnDemand', '0'),\n ('spark.databricks.sql.configMapperClass',\n  'com.databricks.dbsql.config.SqlConfigMapperBridge'),\n ('spark.driver.maxResultSize', '4g'),\n ('spark.databricks.clusterUsageTags.sparkEnvVarContainsNewline', 'false'),\n ('spark.hadoop.fs.fcfs-s3.impl',\n  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n ('spark.databricks.delta.multiClusterWrites.enabled', 'true'),\n ('spark.worker.cleanup.enabled', 'false'),\n ('spark.sql.legacy.createHiveTableByDefault', 'false'),\n ('spark.ui.port', '40001'),\n ('spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2File', '0'),\n ('spark.hadoop.fs.fcfs-s3a.impl.disable.cache', 'true'),\n ('spark.hadoop.fs.s3a.attempts.maximum', '10'),\n ('spark.databricks.clusterUsageTags.enableCredentialPassthrough', 'false'),\n ('spark.databricks.clusterUsageTags.sparkEnvVarContainsDollarSign', 'false'),\n ('spark.databricks.clusterUsageTags.userProvidedRemoteVolumeType',\n  'ebs_volume_type: GENERAL_PURPOSE_SSD\\n'),\n ('spark.databricks.clusterUsageTags.enableJdbcAutoStart', 'true'),\n ('spark.hadoop.fs.azure.user.agent.prefix', ''),\n ('spark.hadoop.fs.s3n.impl.disable.cache', 'true'),\n ('spark.databricks.clusterUsageTags.enableGlueCatalogCredentialPassthrough',\n  'false'),\n ('spark.hadoop.fs.fcfs-s3n.impl',\n  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n ('spark.hadoop.fs.abfs.impl',\n  'com.databricks.common.filesystem.LokiFileSystem'),\n ('spark.hadoop.fs.s3a.retry.throttle.interval', '500ms'),\n ('spark.hadoop.fs.wasb.impl.disable.cache', 'true'),\n ('spark.databricks.clusterUsageTags.clusterLogDestination', ''),\n ('spark.databricks.wsfsPublicPreview', 'true'),\n ('spark.cleaner.referenceTracking.blocking', 'false'),\n ('spark.databricks.clusterUsageTags.isSingleUserCluster', 'false'),\n ('spark.databricks.clusterUsageTags.clusterState', 'Pending'),\n ('spark.databricks.clusterUsageTags.sparkEnvVarContainsSingleQuotes',\n  'false'),\n ('spark.databricks.tahoe.logStore.azure.class',\n  'com.databricks.tahoe.store.AzureLogStore'),\n ('spark.hadoop.fs.azure.skip.metrics', 'true'),\n ('spark.hadoop.hive.hmshandler.retry.attempts', '10'),\n ('spark.scheduler.mode', 'FAIR'),\n ('spark.sql.sources.default', 'delta'),\n ('spark.databricks.unityCatalog.credentialManager.tokenRefreshEnabled',\n  'true'),\n ('spark.hadoop.fs.cpfs-s3n.impl',\n  'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem'),\n ('spark.databricks.clusterUsageTags.clusterWorkers', '0'),\n ('spark.hadoop.fs.cpfs-adl.impl',\n  'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem'),\n ('spark.hadoop.fs.fcfs-s3n.impl.disable.cache', 'true'),\n ('spark.hadoop.fs.cpfs-abfss.impl',\n  'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem'),\n ('spark.databricks.rocksDB.fileManager.useCommitService', 'false'),\n ('spark.databricks.passthrough.oauth.refresher.impl',\n  'com.databricks.backend.daemon.driver.credentials.OAuthTokenRefresherClient'),\n ('spark.hadoop.databricks.loki.fileStatusCache.gcs.enabled', 'false'),\n ('spark.sql.hive.metastore.sharedPrefixes',\n  'org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks'),\n ('spark.databricks.io.directoryCommit.enableLogicalDelete', 'false'),\n ('spark.task.reaper.killTimeout', '60s'),\n ('spark.hadoop.parquet.block.size.row.check.min', '10'),\n ('spark.repl.class.uri', 'spark://10.172.176.211:41799/classes'),\n ('spark.hadoop.hive.server2.use.SSL', 'true'),\n ('spark.databricks.clusterUsageTags.clusterAvailability', 'ON_DEMAND'),\n ('spark.databricks.clusterUsageTags.userProvidedRemoteVolumeSizeGb', '0'),\n ('spark.hadoop.hive.server2.keystore.path',\n  '/databricks/keys/jetty-ssl-driver-keystore.jks'),\n ('spark.hadoop.fs.gs.impl',\n  'com.databricks.common.filesystem.LokiFileSystem'),\n ('spark.databricks.credential.redactor',\n  'com.databricks.logging.secrets.CredentialRedactorProxyImpl'),\n ('spark.databricks.clusterUsageTags.clusterPinned', 'false'),\n ('spark.databricks.acl.provider',\n  'com.databricks.sql.acl.ReflectionBackedAclProvider'),\n ('spark.databricks.mlflow.autologging.enabled', 'true'),\n ('spark.extraListeners',\n  'com.databricks.backend.daemon.driver.DBCEventLoggingListener'),\n ('spark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.enabled',\n  'false'),\n ('spark.sql.parquet.cacheMetadata', 'true'),\n ('spark.databricks.clusterUsageTags.numPerGlobalInitScriptsV2', '0'),\n ('spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Abfss', '0'),\n ('spark.hadoop.parquet.abfs.readahead.optimization.enabled', 'true'),\n ('spark.hadoop.fs.adl.impl', 'com.databricks.adl.AdlFileSystem'),\n ('spark.hadoop.fs.cpfs-abfss.impl.disable.cache', 'true'),\n ('spark.hadoop.fs.abfss.impl',\n  'com.databricks.common.filesystem.LokiFileSystem'),\n ('spark.databricks.clusterUsageTags.enableLocalDiskEncryption', 'false'),\n ('spark.databricks.tahoe.logStore.class',\n  'com.databricks.tahoe.store.DelegatingLogStore'),\n ('spark.hadoop.fs.s3.impl.disable.cache', 'true'),\n ('spark.hadoop.spark.hadoop.aws.glue.cache.db.ttl-mins', '30'),\n ('spark.hadoop.spark.hadoop.aws.glue.cache.table.ttl-mins', '30'),\n ('spark.databricks.clusterUsageTags.driverPublicDns',\n  'ec2-35-86-134-254.us-west-2.compute.amazonaws.com'),\n ('spark.databricks.cloudProvider', 'AWS'),\n ('spark.sql.hive.convertMetastoreParquet', 'true'),\n ('spark.executor.id', 'driver'),\n ('spark.databricks.service.dbutils.server.backend',\n  'com.databricks.dbconnect.SparkServerDBUtils'),\n ('libraryDownload.sleepIntervalSeconds', '5'),\n ('spark.databricks.clusterUsageTags.workerEnvironmentId',\n  'default-worker-env'),\n ('spark.databricks.repl.enableClassFileCleanup', 'true'),\n ('spark.hadoop.fs.s3a.multipart.size', '10485760'),\n ('spark.databricks.clusterUsageTags.cloudProvider', 'AWS'),\n ('spark.databricks.clusterUsageTags.sparkImageLabel',\n  'release__12.2.x-snapshot-scala2.12__databricks-universe__12.2.13__53a9dec__49d07c2__jenkins__af58696__format-3'),\n ('spark.metrics.conf', '/databricks/spark/conf/metrics.properties'),\n ('spark.akka.frameSize', '256'),\n ('spark.hadoop.fs.s3a.fast.upload', 'true'),\n ('spark.databricks.clusterUsageTags.driverInstanceId', 'i-07de04a35a09d6799'),\n ('spark.hadoop.fs.wasbs.impl',\n  'shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem'),\n ('spark.sql.streaming.stopTimeout', '15s'),\n ('spark.hadoop.hive.server2.keystore.password', '[REDACTED]'),\n ('spark.databricks.clusterUsageTags.clusterName', 'TempCluster'),\n ('spark.databricks.clusterUsageTags.ignoreTerminationEventInAlerting',\n  'false'),\n ('spark.hadoop.fs.s3a.retry.interval', '250ms'),\n ('spark.databricks.clusterUsageTags.sparkEnvVarContainsEscape', 'false'),\n ('spark.databricks.overrideDefaultCommitProtocol',\n  'org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol'),\n ('spark.worker.aioaLazyConfig.dbfsReadinessCheckClientClass',\n  'com.databricks.backend.daemon.driver.NephosDbfsReadinessCheckClient'),\n ('spark.databricks.clusterUsageTags.clusterNoDriverDaemon', 'false'),\n ('libraryDownload.timeoutSeconds', '180'),\n ('spark.hadoop.parquet.memory.pool.ratio', '0.5'),\n ('spark.databricks.clusterUsageTags.clusterScalingType', 'fixed_size'),\n ('spark.hadoop.databricks.loki.fileStatusCache.abfs.enabled', 'false'),\n ('spark.databricks.passthrough.adls.gen2.tokenProviderClassName',\n  'com.databricks.backend.daemon.data.client.adl.AdlGen2CredentialContextTokenProvider'),\n ('spark.hadoop.fs.s3a.block.size', '67108864'),\n ('spark.databricks.tahoe.logStore.gcp.class',\n  'com.databricks.tahoe.store.GCPLogStore'),\n ('spark.serializer.objectStreamReset', '100'),\n ('spark.databricks.clusterUsageTags.sparkMasterUrlType', 'None'),\n ('spark.sql.sources.commitProtocolClass',\n  'com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol'),\n ('spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Gcs', '0'),\n ('spark.databricks.clusterUsageTags.effectiveSparkVersion',\n  '12.2.x-scala2.12'),\n ('spark.databricks.clusterUsageTags.attribute_tag_budget', ''),\n ('spark.hadoop.fs.fcfs-s3a.impl',\n  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n ('spark.databricks.clusterUsageTags.clusterPythonVersion', '3'),\n ('spark.databricks.clusterUsageTags.enableDfAcls', 'false'),\n ('spark.databricks.clusterUsageTags.userProvidedRemoteVolumeCount', '0'),\n ('spark.hadoop.databricks.loki.fileSystemCache.enabled', 'true'),\n ('spark.shuffle.service.enabled', 'true'),\n ('spark.hadoop.fs.file.impl',\n  'com.databricks.backend.daemon.driver.WorkspaceLocalFileSystem'),\n ('spark.plugins', 'org.apache.spark.sql.connect.SparkConnectPlugin'),\n ('spark.hadoop.fs.fcfs-wasb.impl.disable.cache', 'true'),\n ('spark.hadoop.fs.cpfs-s3.impl',\n  'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem'),\n ('spark.databricks.clusterUsageTags.attribute_tag_dust_maintainer', ''),\n ('spark.hadoop.fs.s3a.multipart.threshold', '104857600'),\n ('spark.rpc.message.maxSize', '256'),\n ('spark.databricks.clusterUsageTags.attribute_tag_dust_suite', ''),\n ('spark.hadoop.fs.fcfs-wasbs.impl',\n  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n ('spark.databricks.driverNfs.enabled', 'true'),\n ('spark.databricks.clusterUsageTags.clusterMetastoreAccessType',\n  'RDS_DIRECT'),\n ('spark.databricks.clusterUsageTags.ngrokNpipEnabled', 'false'),\n ('spark.hadoop.parquet.page.metadata.validation.enabled', 'true'),\n ('spark.databricks.clusterUsageTags.instanceProfileUsed', 'false'),\n ('spark.databricks.unityCatalog.credentialManager.apiTokenProviderClassName',\n  'com.databricks.unity.TokenServiceApiTokenProvider'),\n ('spark.databricks.passthrough.glue.executorServiceFactoryClassName',\n  'com.databricks.backend.daemon.driver.credentials.GlueClientExecutorServiceFactory'),\n ('spark.databricks.sparkContextId', '1577583029937858185'),\n ('spark.databricks.clusterUsageTags.awsWorkspaceIMDSV2EnablementStatus',\n  'false'),\n ('spark.databricks.clusterUsageTags.clusterId', '0926-185017-qmz3pexl'),\n ('spark.databricks.acl.scim.client',\n  'com.databricks.spark.sql.acl.client.DriverToWebappScimClient'),\n ('spark.databricks.clusterUsageTags.sparkEnvVarContainsBacktick', 'false'),\n ('spark.r.sql.derby.temp.dir', '/tmp/RtmpzANk6F'),\n ('spark.hadoop.fs.adl.impl.disable.cache', 'true'),\n ('spark.hadoop.parquet.block.size.row.check.max', '10'),\n ('spark.hadoop.fs.s3a.connection.maximum', '200'),\n ('spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2', '0'),\n ('spark.hadoop.fs.s3a.fast.upload.active.blocks', '32'),\n ('spark.shuffle.reduceLocality.enabled', 'false'),\n ('spark.databricks.clusterUsageTags.driverNodeType', 'dev-tier-node'),\n ('spark.hadoop.spark.sql.sources.outputCommitterClass',\n  'com.databricks.backend.daemon.data.client.MapReduceDirectOutputCommitter'),\n ('spark.hadoop.fs.fcfs-abfs.impl',\n  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n ('spark.databricks.clusterUsageTags.instanceBootstrapType', 'ssh'),\n ('spark.hadoop.fs.fcfs-abfss.impl.disable.cache', 'true'),\n ('spark.app.id', 'local-1695754273214'),\n ('spark.hadoop.hive.server2.thrift.http.cookie.auth.enabled', 'false'),\n ('spark.hadoop.spark.hadoop.aws.glue.cache.table.size', '1000'),\n ('spark.databricks.driverNodeTypeId', 'dev-tier-node'),\n ('spark.sql.parquet.compression.codec', 'snappy'),\n ('spark.hadoop.fs.stage.impl',\n  'com.databricks.backend.daemon.driver.managedcatalog.PersonalStagingFileSystem'),\n ('spark.databricks.clusterUsageTags.driverContainerPrivateIp',\n  '10.172.176.211'),\n ('spark.databricks.credential.scope.fs.s3a.tokenProviderClassName',\n  'com.databricks.backend.daemon.driver.credentials.CredentialScopeS3TokenProvider'),\n ('spark.databricks.cloudfetch.hasRegionSupport', 'true'),\n ('spark.hadoop.databricks.s3.create.deleteUnnecessaryFakeDirectories',\n  'false'),\n ('spark.hadoop.fs.wasb.impl',\n  'shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem'),\n ('spark.hadoop.spark.hadoop.aws.glue.cache.db.size', '1000'),\n ('spark.databricks.workerNodeTypeId', 'dev-tier-node'),\n ('spark.databricks.clusterUsageTags.clusterAllTags',\n  '[{\"key\":\"Name\",\"value\":\"ce-worker\"},{\"key\":\"WorkspaceId\",\"value\":\"3007032361860494\"},{\"key\":\"ClusterId\",\"value\":\"0926-185017-qmz3pexl\"}]'),\n ('spark.databricks.passthrough.glue.credentialsProviderFactoryClassName',\n  'com.databricks.backend.daemon.driver.credentials.DatabricksCredentialProviderFactory'),\n ('spark.databricks.clusterUsageTags.clusterEbsVolumeSize', '0'),\n ('spark.sparklyr-backend.threads', '1'),\n ('spark.hadoop.fs.fcfs-wasb.impl',\n  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n ('spark.databricks.passthrough.s3a.tokenProviderClassName',\n  'com.databricks.backend.daemon.driver.aws.AwsCredentialContextTokenProvider'),\n ('spark.databricks.session.share', 'false'),\n ('spark.databricks.clusterUsageTags.clusterResourceClass', 'default'),\n ('spark.databricks.clusterUsageTags.driverInstancePrivateIp',\n  '10.172.191.247'),\n ('spark.hadoop.fs.idbfs.impl', 'com.databricks.io.idbfs.IdbfsFileSystem'),\n ('spark.driver.extraJavaOptions',\n  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n ('spark.hadoop.fs.dbfs.impl',\n  'com.databricks.backend.daemon.data.client.DbfsHadoop3'),\n ('spark.databricks.clusterUsageTags.clusterSku', 'STANDARD_SKU'),\n ('spark.repl.class.outputDir',\n  '/local_disk0/tmp/repl/spark-1577583029937858185-65c47c32-c1de-400f-b71b-6bf8e97795b1'),\n ('spark.hadoop.fs.gs.impl.disable.cache', 'true'),\n ('spark.databricks.privateLinkEnabled', 'false'),\n ('spark.databricks.clusterUsageTags.clusterUnityCatalogMode', 'CUSTOM'),\n ('spark.delta.sharing.profile.provider.class',\n  'io.delta.sharing.DeltaSharingCredentialsProvider'),\n ('spark.executor.extraJavaOptions',\n  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=512m -XX:+UseCodeCacheFlushing -XX:PerMethodRecompilationCutoff=-1 -XX:PerBytecodeRecompilationCutoff=-1 -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:-UseContainerSupport -XX:+PrintFlagsFinal -XX:+PrintGCDateStamps -XX:+PrintGCDetails -verbose:gc -Xss4m -Djava.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Djavax.net.ssl.sessionCacheSize=10000 -Dscala.reflect.runtime.disable.typetag.cache=true -Dcom.google.cloud.spark.bigquery.repackaged.io.netty.tryReflectionSetAccessible=true -Dlog4j2.formatMsgNoLookups=true -Ddatabricks.serviceName=spark-executor-1'),\n ('spark.app.startTime', '1695754264537'),\n ('spark.worker.aioaLazyConfig.iamReadinessCheckClientClass',\n  'com.databricks.backend.daemon.driver.NephosIamRoleCheckClient'),\n ('spark.databricks.clusterUsageTags.clusterEbsVolumeType',\n  'GENERAL_PURPOSE_SSD'),\n ('spark.databricks.clusterUsageTags.clusterOwnerUserId', '1801394777137432'),\n ('spark.databricks.automl.serviceEnabled', 'true'),\n ('spark.hadoop.parquet.page.size.check.estimate', 'false'),\n ('spark.databricks.clusterUsageTags.attribute_tag_service', ''),\n ('spark.databricks.passthrough.s3a.threadPoolExecutor.factory.class',\n  'com.databricks.backend.daemon.driver.aws.S3APassthroughThreadPoolExecutorFactory'),\n ('spark.databricks.delta.preview.enabled', 'true'),\n ('spark.databricks.metrics.filesystem_io_metrics', 'true'),\n ('spark.hadoop.spark.driverproxy.customHeadersToProperties',\n  'X-Databricks-User-Token:spark.databricks.token,X-Databricks-Non-UC-User-Token:spark.databricks.non.uc.token,X-Databricks-Api-Url:spark.databricks.api.url,X-Databricks-ADLS-Gen1-Token:spark.databricks.adls.gen1.token,X-Databricks-ADLS-Gen2-Token:spark.databricks.adls.gen2.token,X-Databricks-Synapse-Token:spark.databricks.synapse.token,X-Databricks-AWS-Credentials:spark.databricks.aws.creds,X-Databricks-User-Id:spark.databricks.user.id,X-Databricks-User-Name:spark.databricks.user.name,X-Databricks-Oauth-Identity-Custom-Claim:spark.databricks.oauthCustomIdentityClaims'),\n ('spark.databricks.cloudfetch.requesterClassName',\n  'com.databricks.spark.sql.cloudfetch.DataDaemonCloudPresignedUrlRequester'),\n ('spark.master', 'local[8]'),\n ('spark.databricks.delta.logStore.crossCloud.fatal', 'true'),\n ('spark.databricks.driverNfs.clusterWidePythonLibsEnabled', 'true'),\n ('spark.files.fetchFailure.unRegisterOutputOnHost', 'true'),\n ('spark.databricks.clusterUsageTags.enableSqlAclsOnly', 'false'),\n ('spark.databricks.clusterUsageTags.clusterEbsVolumeCount', '0'),\n ('spark.databricks.clusterUsageTags.clusterSizeType', 'VM_CONTAINER'),\n ('spark.hadoop.databricks.fs.perfMetrics.enable', 'true'),\n ('spark.databricks.clusterUsageTags.clusterNumSshKeys', '0'),\n ('spark.hadoop.fs.gs.outputstream.upload.chunk.size', '16777216'),\n ('spark.databricks.tahoe.logStore.aws.class',\n  'com.databricks.tahoe.store.S3LockBasedLogStore'),\n ('spark.speculation.quantile', '0.9'),\n ('spark.databricks.clusterUsageTags.privateLinkEnabled', 'false'),\n ('spark.shuffle.manager', 'SORT'),\n ('spark.files.overwrite', 'true'),\n ('spark.databricks.credential.aws.secretKey.redactor',\n  'com.databricks.spark.util.AWSSecretKeyRedactorProxy'),\n ('spark.databricks.clusterUsageTags.clusterNumCustomTags', '0'),\n ('spark.hadoop.fs.s3.impl',\n  'com.databricks.common.filesystem.LokiFileSystem'),\n ('spark.hadoop.fs.s3a.impl.disable.cache', 'true'),\n ('spark.databricks.clusterUsageTags.sparkEnvVarContainsDoubleQuotes',\n  'false'),\n ('spark.r.numRBackendThreads', '1'),\n ('spark.hadoop.fs.wasbs.impl.disable.cache', 'true'),\n ('spark.hadoop.fs.abfss.impl.disable.cache', 'true'),\n ('spark.hadoop.fs.azure.cache.invalidator.type',\n  'com.databricks.encryption.utils.CacheInvalidatorImpl'),\n ('spark.sql.hive.metastore.version', '0.13.0'),\n ('spark.shuffle.service.port', '4048'),\n ('spark.databricks.clusterUsageTags.instanceWorkerEnvNetworkType', 'default'),\n ('spark.databricks.acl.client',\n  'com.databricks.spark.sql.acl.client.SparkSqlAclClient'),\n ('spark.streaming.driver.writeAheadLog.closeFileAfterWrite', 'true'),\n ('spark.hadoop.hive.warehouse.subdir.inherit.perms', 'false'),\n ('spark.driver.host', '10.172.176.211'),\n ('spark.databricks.clusterUsageTags.runtimeEngine', 'STANDARD'),\n ('spark.databricks.clusterUsageTags.isServicePrincipalCluster', 'false'),\n ('spark.driver.port', '41799'),\n ('spark.databricks.credential.scope.fs.impl',\n  'com.databricks.sql.acl.fs.CredentialScopeFileSystem'),\n ('spark.hadoop.databricks.loki.fileStatusCache.s3a.enabled', 'false'),\n ('spark.databricks.enablePublicDbfsFuse', 'false'),\n ('spark.databricks.clusterUsageTags.enableElasticDisk', 'false'),\n ('spark.hadoop.fs.fcfs-wasbs.impl.disable.cache', 'true'),\n ('spark.databricks.clusterUsageTags.userProvidedSparkVersion',\n  '12.2.x-scala2.12'),\n ('spark.databricks.clusterUsageTags.clusterNodeType', 'dev-tier-node'),\n ('spark.databricks.clusterUsageTags.clusterOwnerOrgId', '3007032361860494'),\n ('spark.databricks.passthrough.adls.tokenProviderClassName',\n  'com.databricks.backend.daemon.data.client.adl.AdlCredentialContextTokenProvider'),\n ('spark.app.name', 'Databricks Shell'),\n ('spark.driver.allowMultipleContexts', 'false'),\n ('spark.hadoop.fs.AbstractFileSystem.gs.impl',\n  'shaded.databricks.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS'),\n ('spark.databricks.secret.sparkConf.keys.toRedact', ''),\n ('spark.rdd.compress', 'true'),\n ('spark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.throwsException',\n  'false'),\n ('spark.databricks.python.defaultPythonRepl', 'ipykernel'),\n ('spark.hadoop.fs.s3a.retry.limit', '6'),\n ('spark.databricks.clusterUsageTags.attribute_tag_dust_execution_env', ''),\n ('spark.databricks.eventLog.dir', 'eventlogs'),\n ('spark.databricks.credential.scope.fs.adls.gen2.tokenProviderClassName',\n  'com.databricks.backend.daemon.driver.credentials.CredentialScopeADLSTokenProvider'),\n ('spark.databricks.clusterUsageTags.isDpCpPrivateLinkEnabled', 'false'),\n ('spark.databricks.driverNfs.pathSuffix', '.ephemeral_nfs'),\n ('spark.databricks.clusterUsageTags.clusterCreator', 'Webapp'),\n ('spark.speculation', 'false'),\n ('spark.hadoop.databricks.dbfs.client.version', 'v1'),\n ('spark.hadoop.hive.server2.session.check.interval', '60000'),\n ('spark.sql.hive.convertCTAS', 'true'),\n ('spark.hadoop.fs.s3a.max.total.tasks', '1000'),\n ('spark.hadoop.spark.sql.parquet.output.committer.class',\n  'org.apache.spark.sql.parquet.DirectParquetOutputCommitter'),\n ('spark.databricks.clusterUsageTags.sparkVersion', '12.2.x-scala2.12'),\n ('spark.hadoop.fs.s3a.fast.upload.default', 'true'),\n ('spark.databricks.clusterUsageTags.clusterGeneration', '0'),\n ('spark.hadoop.fs.mlflowdbfs.impl',\n  'com.databricks.mlflowdbfs.MlflowdbfsFileSystem'),\n ('spark.databricks.eventLog.listenerClassName',\n  'com.databricks.backend.daemon.driver.DBCEventLoggingListener'),\n ('spark.hadoop.fs.abfs.impl.disable.cache', 'true'),\n ('spark.speculation.multiplier', '3'),\n ('spark.storage.blockManagerTimeoutIntervalMs', '300000'),\n ('spark.databricks.clusterUsageTags.instanceWorkerEnvId',\n  'default-worker-env'),\n ('spark.sparkr.use.daemon', 'false'),\n ('spark.scheduler.listenerbus.eventqueue.capacity', '20000'),\n ('spark.hadoop.fs.s3a.impl',\n  'com.databricks.common.filesystem.LokiFileSystem'),\n ('spark.databricks.clusterUsageTags.clusterStateMessage', 'Starting Spark'),\n ('spark.hadoop.parquet.page.write-checksum.enabled', 'true'),\n ('spark.hadoop.databricks.s3commit.client.sslTrustAll', 'false'),\n ('spark.hadoop.fs.s3a.threads.max', '136'),\n ('spark.r.backendConnectionTimeout', '604800'),\n ('spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Dbfs', '0'),\n ('spark.hadoop.fs.s3n.impl',\n  'com.databricks.common.filesystem.LokiFileSystem'),\n ('spark.hadoop.hive.server2.idle.session.timeout', '900000'),\n ('spark.databricks.redactor',\n  'com.databricks.spark.util.DatabricksSparkLogRedactorProxy'),\n ('spark.executor.extraClassPath',\n  '/databricks/spark/dbconf/log4j/executor:/databricks/spark/dbconf/jets3t/:/databricks/spark/dbconf/hadoop:/databricks/hive/conf:/databricks/jars/*'),\n ('spark.databricks.autotune.maintenance.client.classname',\n  'com.databricks.maintenanceautocompute.MACClientImpl'),\n ('spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Volumes', '0'),\n ('spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Workspace',\n  '0'),\n ('spark.hadoop.fs.fcfs-abfs.impl.disable.cache', 'true'),\n ('spark.hadoop.parquet.page.verify-checksum.enabled', 'true'),\n ('spark.databricks.clusterUsageTags.dataPlaneRegion', 'us-west-2'),\n ('spark.logConf', 'true'),\n ('spark.databricks.clusterUsageTags.enableJobsAutostart', 'true'),\n ('spark.hadoop.hive.server2.enable.doAs', 'false'),\n ('spark.hadoop.parquet.filter.columnindex.enabled', 'false'),\n ('spark.databricks.clusterUsageTags.userId', '1801394777137432'),\n ('spark.shuffle.memoryFraction', '0.2'),\n ('spark.hadoop.fs.dbfsartifacts.impl',\n  'com.databricks.backend.daemon.data.client.DBFSV1'),\n ('spark.hadoop.fs.cpfs-s3a.impl',\n  'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem'),\n ('spark.hadoop.fs.s3a.connection.timeout', '50000'),\n ('spark.databricks.secret.envVar.keys.toRedact', ''),\n ('spark.databricks.clusterUsageTags.region', 'us-west-2'),\n ('spark.databricks.clusterUsageTags.clusterSpotBidPricePercent', '100'),\n ('spark.files.useFetchCache', 'false')]"
     ]
    }
   ],
   "source": [
    "sc.getConf().getAll()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31b4617d-715e-46b2-9cb7-88bc37c8570b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Creating a Text RDD in Spark\n",
    "\n",
    "* Resilient Distributed Datasets (RDDs) can be created in various ways in Spark. Here are two commonly used methods:\n",
    "\n",
    "    * `parallelize()`: This function allows you to transform Python collections, such as lists or arrays, into an RDD.\n",
    "        * It distributes the elements of the passed collection across multiple nodes, making the RDD fault-tolerant.\n",
    "\n",
    "    * `textFile()`: This function reads in a text file and creates an RDD where each object corresponds to a line in the file.\n",
    "\n",
    "* Example using `parallelize()`:\n",
    "  ```python\n",
    "  from pyspark import SparkContext\n",
    "  sc = SparkContext()\n",
    "  my_list = [1, 2, 3, 4, 5]\n",
    "  my_rdd = sc.parallelize(my_list)\n",
    "  # or\n",
    "  my_text_rdd = sc.textFile(\"path/to/text/file.txt\")\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e953f692-1d30-4e0c-b4b8-4eb1c1545d04",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Fundamental Operations on RDDs\n",
    "\n",
    "* Several fundamental operations are available to manipulate and transform Spark RDDs. \n",
    "  * These operations are generally called 'transformations.'\n",
    "    * `map`: Applies a given function to each element of the RDD and returns a new RDD consisting of the results.\n",
    "    * `filter`: Returns a new RDD containing only the elements that satisfy a given predicate.\n",
    "    * `reduce`: Aggregates the elements of the RDD using a given function, \n",
    "      * The function should be commutative and associative so that it can be computed in parallel.\n",
    "\n",
    "* Each transformation on an RDD produces a new RDD without modifying the original one, making RDDs immutable.\n",
    "\n",
    "* `flatMap`: Another commonly used transformation, which first applies a function to all elements of the RDD and then flattens the results. It is conceptually equivalent to Python's `itertools.chain()`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2257e1ca-bf9a-4faf-a06c-178a85510340",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[21]: ParallelCollectionRDD[9] at readRDDFromInputStream at PythonRDD.scala:435"
     ]
    }
   ],
   "source": [
    "my_rdd = sc.parallelize([1,2,3,4,5,6,7,8,9,10])\n",
    "my_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "027c111e-443d-4ec9-8011-6c469fac67e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[22]: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
     ]
    }
   ],
   "source": [
    "my_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68ff42d0-be18-43c5-9387-40e423636c5d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[23]: 8"
     ]
    }
   ],
   "source": [
    "my_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf8052a8-d865-4d95-bc17-69a80168f634",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[24]: [[1], [2], [3], [4, 5], [6], [7], [8], [9, 10]]"
     ]
    }
   ],
   "source": [
    "# Spark's default behavior is sufficient and often near-optimal.\n",
    "partitions_data = my_rdd.glom().collect()\n",
    "partitions_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ce025ac-2e6f-4284-b89b-73ae73b1bbd5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[25]: PythonRDD[11] at RDD at PythonRDD.scala:58"
     ]
    }
   ],
   "source": [
    "doubled_rdd = my_rdd.map(lambda x: x * 2)\n",
    "doubled_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56ffc358-7e05-4980-ab72-fe122b8e4e1d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[26]: [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]"
     ]
    }
   ],
   "source": [
    "doubled_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8eb69bb4-1ac9-4450-9621-b3d2f3e3cd8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[27]: PythonRDD[12] at RDD at PythonRDD.scala:58"
     ]
    }
   ],
   "source": [
    "even_rdd = my_rdd.filter(lambda x: x % 2 == 0)\n",
    "even_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "250e3f98-1c9a-49a8-b8f4-b65d6b885df4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[28]: [2, 4, 6, 8, 10]"
     ]
    }
   ],
   "source": [
    "even_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b66618f5-6761-4530-8d9a-ba7aa532c46b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sum_of_elements = my_rdd.reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73ea0fde-7310-428d-b1f8-77ddc8338332",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[30]: 55"
     ]
    }
   ],
   "source": [
    "sum_of_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c8f195b-fc6f-4465-897f-36dfc1b5ab63",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mapped_rdd = my_rdd.map(lambda x: (x, x * 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dc9f577-491e-46d3-91b3-1b4a2fa44db5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[32]: [(1, 3),\n (2, 6),\n (3, 9),\n (4, 12),\n (5, 15),\n (6, 18),\n (7, 21),\n (8, 24),\n (9, 27),\n (10, 30)]"
     ]
    }
   ],
   "source": [
    "mapped_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f7cf592-b14b-448e-8c0b-4289a06603b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "flat_rdd = my_rdd.flatMap(lambda x: (x, x * 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a980fa9-2805-405d-aee8-4d5ba4df0d43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[34]: [1, 3, 2, 6, 3, 9, 4, 12, 5, 15, 6, 18, 7, 21, 8, 24, 9, 27, 10, 30]"
     ]
    }
   ],
   "source": [
    "flat_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b891cffa-a0f8-4183-a37c-c848c370a0be",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Transformations and Actions in Spark\n",
    "\n",
    "* Transformations: These operations transform your data and produce a new RDD. They come in two types:\n",
    "    * Narrow-dependency transformations:\n",
    "        * Transformations where each partition of the parent RDD is used to build only one partition of the new RDD, e.g., `map` and `filter`.\n",
    "        * This means the operation can be performed independently on each partition, which allows for better parallelism and less data movement.\n",
    "    * Wide-dependency transformations:\n",
    "        * Transformations where a single partition of the parent RDD may be used to build multiple partitions of the child RDD, e.g., `groupBy` and `reduceByKey`.\n",
    "        * These operations are generally expensive in terms of performance as they typically require shuffling, or  redistributing, data across partitions\n",
    "\n",
    "* Actions: These operations trigger the computation and execute the job, producing a result.\n",
    "    * Each action initiates a Spark job, which may consist of multiple stages depending on the transformations involved.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a63e3042-964e-4e52-a893-34e668d8fcac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Why Distinguish Between Transformations and Actions?\n",
    "* The original data in PySpark is immutable, and there are certain transformations that don't justify creating a new copy of the data. \n",
    "  * E.g., whe performing a groupby operation followed by an aggregation like taking the group's minimum or maximum value, there's no need to generate and persist an RDD that displays the intermediate steps.\n",
    "* By separating these two types of operations, PySpark uses lazy evaluation\n",
    "  * It postpones the actual computation until an action is called. This allows PySpark to optimize the execution plan and minimize unnecessary computations.\n",
    "* Execution plan can reorder, combining, or pruning transformations before executing them. \n",
    "  * This optimization is possible because PySpark knows that transformations will not be executed until an action is called.\n",
    "  * If all operations were executed immediately, it would be challenging to optimize the processing efficiently, leading to potentially slower performance.  \n",
    "* During the execution of transformations, PySpark may persist intermediate results as needed for fault tolerance. \n",
    "  * If a node fails during the execution of these transformations, PySpark can recover by recomputing only the lost partitions of the RDD, starting from the original data.\n",
    "* PySpark may persist intermediate results for fault tolerance. This is done only for the data that is needed for the specific action being executed, which helps minimize the amount of data persisted and enhances fault tolerance.\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bcc6c69-2122-43d5-aafa-91fedc5fb253",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Example of a Wide-dependency Transformation: Join\n",
    "\n",
    "* Before Join on key (field 1 in the tuple) between RDD1 and RDD 2\n",
    "\n",
    "* Partition 1 of RDD1: \n",
    "```\n",
    "(1, \"apple\")\n",
    "(2, \"banana\")\n",
    "```\n",
    "* Partition 2 of RDD1: \n",
    "```\n",
    "(3, \"cherry\")\n",
    "(4, \"date\")\n",
    "```\n",
    "  \n",
    "* Partition 1 of RDD2: \n",
    "```\n",
    "(3, \"red\")\n",
    "```\n",
    "* Partition 2 of RDD2: \n",
    "```\n",
    "(4, \"brown\")\n",
    "(2, \"yellow\")\n",
    "```\n",
    "\n",
    "Compute a jon between RDD1 and RDD2 based on the keys\n",
    "1 . shuffle the data around to group all occurrences of the same key together. \n",
    "* Data after shuffle\n",
    "\n",
    "  * Shuffle output targeting Partition 1 (from RDD1 and RDD2):\n",
    "    ```\n",
    "    (2, \"banana\")\n",
    "    (2, \"yellow\")\n",
    "    ```\n",
    "    \n",
    "  * Shuffle output targeting Partition 2 (from RDD1 and RDD2):\n",
    "    ```\n",
    "    (3, \"cherry\")\n",
    "    (3, \"red\")\n",
    "    (4, \"date\")\n",
    "    (4, \"brown\")\n",
    "    ```\n",
    "\n",
    "* Data Partitions After Join\n",
    "\n",
    "* Partition 1: \n",
    "```\n",
    "(2, (\"banana\", \"yellow\"))\n",
    "```\n",
    "* Partition 2\n",
    "```\n",
    "(3, (\"cherry\", \"red\")), (4, (\"date\", \"brown\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f8a7e3b-c7b3-4c80-8c02-b7216216b444",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Understanding the Concept of a Stage in Spark\n",
    "\n",
    "* A \"stage\" is a sequence of transformations that can be executed in parallel.\n",
    " * A stage represents a set of tasks that can be executed in parallel without shuffling data between them.\n",
    " \n",
    "* Stages are separated by operations that require data to be rearranged, known as \"wide dependencies.\" \n",
    "  * Those include actions like `reduceByKey`, `groupByKey`, and `join`, where data needs to be redistributed across the cluster\n",
    "  * These operations typically trigger the boundary between stages.\n",
    "\n",
    "\n",
    "* Each stage either reads data, performs computations, or writes data.\n",
    "\n",
    "* Stages are executed in sequence, one after the other.\n",
    "  * Within each stage, tasks are executed in parallel.\n",
    "* Computation moves to where the data resides.\n",
    "\n",
    "* Complexity can be managed with stages\n",
    "  * Within a stage, tasks can be executed in parallel on different partitions of the data. \n",
    "    * However, tasks within different stages cannot be executed in parallel until the preceding stage is complete.\n",
    "  * Stages also help with fault tolerance because Spark can recompute a specific stage if some tasks fail without having to recompute the entire job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "140b46ff-e7c8-4e0c-882b-1496bd0265c1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Markdown block with example query: Decomposition into Stages\n",
    " * Note that the data is not provided to run this code.\n",
    "\n",
    "```python\n",
    "flights_df = spark.read.option(\"head\", \"true\").option(\"inferSchema\", \"true\").csv(\"flights_info.csv\") # (1)\n",
    "flights_data_partitioned_df = flights_data.repartition(minPartitions=4)   # (1)\n",
    "\n",
    "counts_df = flights_df.where(\"duration > 120\")                                          # (2)\n",
    "                                       .select(\"dep\", \"dest\", \"carrier\", \"durations\")   # (2)\n",
    "                                       .groupBy(\"carrier\")                              # (2')\n",
    "                                       .count()                                         # (3)\n",
    "counts_df.collect()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c640c588-cb2f-495d-ab73-1a207cdac2cd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Stages\n",
    "1. Reading Data\n",
    "  * Reads the partitioned data into memory.\n",
    "    * A task for each partition.\n",
    "  * This task has no dependency\n",
    "2. Filter, Select, and GroupBy\n",
    "   * Applies .where(), .select(), and .groupBy().\n",
    "   * Each task applies all these transformations on a partition.\n",
    "   * GroupBy is \"Wide\" dependency (Needs to shuffle data between partitions for grouping) \n",
    "     * Delineates the end of the stage\n",
    "3. Count\n",
    "  * Applies .count() to each group.\n",
    "   * Each task calculates the count for groups in its partition.\n",
    "    * Each group is guaranteed to be in the same partition\n",
    "  * This task has no dependency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00631b18-045f-43e5-ac0b-e6c1f59848ce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### PySpark: Job, Stages and Tasks\n",
    "\n",
    "<img src=\"https://www.dropbox.com/s/5qa1fb7p867i787/Page5.jpg?dl=1\" width=\"900\" height=\"600\">\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a22c87cc-5fa2-497a-abd5-d945fbfe96f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting randomuser\r\n  Downloading randomuser-1.6.tar.gz (5.0 kB)\r\nBuilding wheels for collected packages: randomuser\r\n  Building wheel for randomuser (setup.py) ... \u001B[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \bdone\r\n\u001B[?25h  Created wheel for randomuser: filename=randomuser-1.6-py3-none-any.whl size=5082 sha256=110188b69345c9ca0746b3d2ec3a23e2a25b27bac92dae30bee71f77ede0a7a1\r\n  Stored in directory: /root/.cache/pip/wheels/41/6f/23/878c103a235dc2d4e85a3965c124aae8a28470c541b81aa2ba\r\nSuccessfully built randomuser\r\nInstalling collected packages: randomuser\r\nSuccessfully installed randomuser-1.6\r\n\u001B[33mWARNING: You are using pip version 21.2.4; however, version 23.2.1 is available.\r\nYou should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-48857c09-ed83-4abb-8d1e-21ffed4f54c9/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install randomuser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2d27bbd-6d2b-4359-86af-8ca61c661f71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user object  is <randomuser.RandomUser object at 0x7fcd3c6a44c0>\n"
     ]
    }
   ],
   "source": [
    "from randomuser import RandomUser\n",
    "\n",
    "# # Generate a single user\n",
    "user = RandomUser({\"nat\": \"us\"})\n",
    "print(f\"user object  is {user}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44931b51-103f-4e9f-bd54-c03f4daad95f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user json representation  is\n {'user_id': '386-03-3670', 'first_name': 'Misty', 'last_name': 'Watts', 'state': 'Virginia', 'zip': 59791, 'lat_long': {'latitude': '68.6293', 'longitude': '147.0340'}}\n"
     ]
    }
   ],
   "source": [
    "def get_user_info(u):\n",
    "\n",
    "    user_dict = {\n",
    "        \"user_id\": u.get_id()[\"number\"], \n",
    "        \"first_name\": u.get_first_name(), \n",
    "        \"last_name\": u.get_last_name(), \n",
    "        \"state\": u.get_state(),\n",
    "        \"zip\": u.get_zipcode(),\n",
    "        \"lat_long\": u.get_coordinates()\n",
    "    }\n",
    "    return user_dict\n",
    "\n",
    "user_json = get_user_info(user)\n",
    "print(f\"user json representation  is\\n {user_json}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17eeb182-7d8f-49d2-a00a-a73b7e92a7c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\nOut[39]: [<randomuser.RandomUser at 0x7fcd2c8ad790>,\n <randomuser.RandomUser at 0x7fcd2c8ad130>,\n <randomuser.RandomUser at 0x7fcd2c8ad070>]"
     ]
    }
   ],
   "source": [
    "my_users = RandomUser.generate_users(500, {\"nat\": \"us\"})\n",
    "print(len(my_users))\n",
    "my_users[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a807114-710d-4c3f-ae16-a335257f7470",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[40]: [{'user_id': '957-20-9308',\n  'first_name': 'Shane',\n  'last_name': 'Diaz',\n  'state': 'Arkansas',\n  'zip': 90419,\n  'lat_long': {'latitude': '24.3314', 'longitude': '-173.0931'}},\n {'user_id': '153-62-0894',\n  'first_name': 'Lewis',\n  'last_name': 'Kelly',\n  'state': 'Alaska',\n  'zip': 78969,\n  'lat_long': {'latitude': '-81.9182', 'longitude': '73.8634'}},\n {'user_id': '486-16-7047',\n  'first_name': 'Melanie',\n  'last_name': 'Mcdonalid',\n  'state': 'Illinois',\n  'zip': 62347,\n  'lat_long': {'latitude': '19.6323', 'longitude': '146.5552'}}]"
     ]
    }
   ],
   "source": [
    "# Generate a list of 10 random users\n",
    "\n",
    "user_dicts = list(map(get_user_info, my_users))\n",
    "\n",
    "user_dicts[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6817cbf9-f86e-4171-a4a4-284a90f01ca7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of objects in my RDD is: 500\n"
     ]
    }
   ],
   "source": [
    "users_rdd = sc.parallelize(user_dicts)\n",
    "users_rdd_size  = users_rdd.count()\n",
    "print(f\"The number of objects in my RDD is: {users_rdd_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5cfe97d-c065-453d-8ffb-91e55d5c562c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[42]: [{'user_id': '402-75-4332',\n  'first_name': 'Margie',\n  'last_name': 'Taylor',\n  'state': 'South Dakota',\n  'zip': 84922,\n  'lat_long': {'latitude': '-18.7350', 'longitude': '-113.8008'}},\n {'user_id': '993-56-9952',\n  'first_name': 'Kylie',\n  'last_name': 'Hopkins',\n  'state': 'Pennsylvania',\n  'zip': 98931,\n  'lat_long': {'latitude': '-1.5840', 'longitude': '72.0011'}},\n {'user_id': '900-86-1218',\n  'first_name': 'Addison',\n  'last_name': 'Ruiz',\n  'state': 'Iowa',\n  'zip': 24346,\n  'lat_long': {'latitude': '-17.8507', 'longitude': '152.1308'}}]"
     ]
    }
   ],
   "source": [
    "users_rdd.takeSample(False, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33c2c47f-ee8e-4046-b02e-33151f5b352a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[43]: PythonRDD[20] at RDD at PythonRDD.scala:58"
     ]
    }
   ],
   "source": [
    "select_users_rdd = users_rdd.filter(lambda x: x['state'] in [\"Hawaii\", \"Idaho\"])\n",
    "select_users_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be241d1f-fe5d-4e9a-b3ab-36df42156b2e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[44]: [{'user_id': '886-16-2442',\n  'first_name': 'Louise',\n  'last_name': 'Simmmons',\n  'state': 'Idaho',\n  'zip': 94027,\n  'lat_long': {'latitude': '-45.1880', 'longitude': '66.2781'}},\n {'user_id': '988-64-9145',\n  'first_name': 'Marlene',\n  'last_name': 'Hart',\n  'state': 'Hawaii',\n  'zip': 71762,\n  'lat_long': {'latitude': '-12.9179', 'longitude': '97.0387'}},\n {'user_id': '623-91-2808',\n  'first_name': 'Marc',\n  'last_name': 'Webb',\n  'state': 'Idaho',\n  'zip': 32281,\n  'lat_long': {'latitude': '-25.5361', 'longitude': '80.7813'}},\n {'user_id': '663-77-3475',\n  'first_name': 'Jeanette',\n  'last_name': 'Powell',\n  'state': 'Hawaii',\n  'zip': 54744,\n  'lat_long': {'latitude': '-26.5964', 'longitude': '-58.0439'}},\n {'user_id': '743-39-5416',\n  'first_name': 'Toni',\n  'last_name': 'Jensen',\n  'state': 'Hawaii',\n  'zip': 74899,\n  'lat_long': {'latitude': '-60.4885', 'longitude': '61.2291'}}]"
     ]
    }
   ],
   "source": [
    "# collect the result means grab them from all the chunk nodes\n",
    "select_users_rdd.collect()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73e058ba-39fb-45ae-a27e-f359e82f4d52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[48]: [FileInfo(path='dbfs:/FileStore/pride_and_prejudice.txt', name='pride_and_prejudice.txt', size=798774, modificationTime=1695151386000)]"
     ]
    }
   ],
   "source": [
    "dbutils.fs.ls(\"/FileStore/pride_and_prejudice.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "630c8ced-716e-492b-86a3-85ab4786b5de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[45]: 4"
     ]
    }
   ],
   "source": [
    "# Building an RDD from a text file.\n",
    "text = sc.textFile('dbfs:/FileStore/pride_and_prejudice.txt', minPartitions=4)\n",
    "### Number of items in the RDD\n",
    "text.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caecd573-8bbe-4634-a2d7-286917452a35",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of objects in the RDD is 14579\n"
     ]
    }
   ],
   "source": [
    "text_rdd_size = text.count()\n",
    "print(f\"number of objects in the RDD is {text_rdd_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21364895-a949-46b5-a9bf-c7150ec21b5d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of subset_x is: 10\n\ntype of subset_x is: <class 'list'>\n\nsubset_x is:\n['The Project Gutenberg eBook of Pride and Prejudice, by Jane Austen', '', 'This eBook is for the use of anyone anywhere in the United States and', 'most other parts of the world at no cost and with almost no restrictions', 'whatsoever. You may copy it, give it away or re-use it under the terms', 'of the Project Gutenberg License included with this eBook or online at', 'www.gutenberg.org. If you are not located in the United States, you', 'will have to check the laws of the country where you are located before', 'using this eBook.', '']\n"
     ]
    }
   ],
   "source": [
    "subset_x = text.take(10)\n",
    "print(f\"len of subset_x is: {len(subset_x)}\\n\")\n",
    "print(f\"type of subset_x is: {type(subset_x)}\\n\")\n",
    "print(f\"subset_x is:\\n{subset_x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e5e91d7-50ef-4964-b453-cf77470b371b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[53]: [['THE',\n  'PROJECT',\n  'GUTENBERG',\n  'EBOOK',\n  'OF',\n  'PRIDE',\n  'AND',\n  'PREJUDICE',\n  'BY',\n  'JANE',\n  'AUSTEN'],\n [],\n ['THIS',\n  'EBOOK',\n  'IS',\n  'FOR',\n  'THE',\n  'USE',\n  'OF',\n  'ANYONE',\n  'ANYWHERE',\n  'IN',\n  'THE',\n  'UNITED',\n  'STATES',\n  'AND'],\n ['MOST',\n  'OTHER',\n  'PARTS',\n  'OF',\n  'THE',\n  'WORLD',\n  'AT',\n  'NO',\n  'COST',\n  'AND',\n  'WITH',\n  'ALMOST',\n  'NO',\n  'RESTRICTIONS'],\n ['WHATSOEVER',\n  'YOU',\n  'MAY',\n  'COPY',\n  'IT',\n  'GIVE',\n  'IT',\n  'AWAY',\n  'OR',\n  'RE',\n  'USE',\n  'IT',\n  'UNDER',\n  'THE',\n  'TERMS'],\n ['OF',\n  'THE',\n  'PROJECT',\n  'GUTENBERG',\n  'LICENSE',\n  'INCLUDED',\n  'WITH',\n  'THIS',\n  'EBOOK',\n  'OR',\n  'ONLINE',\n  'AT'],\n ['WWW',\n  'GUTENBERG',\n  'ORG',\n  'IF',\n  'YOU',\n  'ARE',\n  'NOT',\n  'LOCATED',\n  'IN',\n  'THE',\n  'UNITED',\n  'STATES',\n  'YOU'],\n ['WILL',\n  'HAVE',\n  'TO',\n  'CHECK',\n  'THE',\n  'LAWS',\n  'OF',\n  'THE',\n  'COUNTRY',\n  'WHERE',\n  'YOU',\n  'ARE',\n  'LOCATED',\n  'BEFORE'],\n ['USING', 'THIS', 'EBOOK'],\n [],\n ['TITLE', 'PRIDE', 'AND', 'PREJUDICE'],\n [],\n ['AUTHOR', 'JANE', 'AUSTEN'],\n [],\n ['RELEASE', 'DATE', 'JUNE', 'EBOOK'],\n ['MOST', 'RECENTLY', 'UPDATED', 'AUGUST'],\n [],\n ['LANGUAGE', 'ENGLISH'],\n [],\n ['CHARACTER', 'SET', 'ENCODING', 'UTF'],\n [],\n ['PRODUCED', 'BY', 'ANONYMOUS', 'VOLUNTEERS', 'AND', 'DAVID', 'WIDGER'],\n [],\n ['START',\n  'OF',\n  'THE',\n  'PROJECT',\n  'GUTENBERG',\n  'EBOOK',\n  'PRIDE',\n  'AND',\n  'PREJUDICE'],\n [],\n [],\n [],\n [],\n ['THERE',\n  'IS',\n  'AN',\n  'ILLUSTRATED',\n  'EDITION',\n  'OF',\n  'THIS',\n  'TITLE',\n  'WHICH',\n  'MAY',\n  'VIEWED',\n  'AT',\n  'EBOOK'],\n [],\n [],\n ['COVER'],\n [],\n [],\n [],\n [],\n ['PRIDE', 'AND', 'PREJUDICE'],\n [],\n ['BY', 'JANE', 'AUSTEN'],\n [],\n ['CONTENTS'],\n [],\n ['CHAPTER'],\n [],\n ['CHAPTER'],\n [],\n ['CHAPTER'],\n [],\n ['CHAPTER'],\n [],\n ['CHAPTER'],\n [],\n ['CHAPTER'],\n [],\n ['CHAPTER'],\n [],\n ['CHAPTER'],\n [],\n ['CHAPTER'],\n []]"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_split_line(line):\n",
    "    a = re.sub('\\d+', '', line)\n",
    "    b = re.sub('[\\W]+', ' ', a)\n",
    "    return b.upper().split()\n",
    "\n",
    "words = text.map(clean_split_line)\n",
    "words.take(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "378d00f7-542c-4a98-b78c-ad3fe02a5747",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[54]: ['THE',\n 'PROJECT',\n 'GUTENBERG',\n 'EBOOK',\n 'OF',\n 'PRIDE',\n 'AND',\n 'PREJUDICE',\n 'BY',\n 'JANE',\n 'AUSTEN',\n 'THIS',\n 'EBOOK',\n 'IS',\n 'FOR',\n 'THE',\n 'USE',\n 'OF',\n 'ANYONE',\n 'ANYWHERE',\n 'IN',\n 'THE',\n 'UNITED',\n 'STATES',\n 'AND',\n 'MOST',\n 'OTHER',\n 'PARTS',\n 'OF',\n 'THE',\n 'WORLD',\n 'AT',\n 'NO',\n 'COST',\n 'AND',\n 'WITH',\n 'ALMOST',\n 'NO',\n 'RESTRICTIONS',\n 'WHATSOEVER',\n 'YOU',\n 'MAY',\n 'COPY',\n 'IT',\n 'GIVE',\n 'IT',\n 'AWAY',\n 'OR',\n 'RE',\n 'USE',\n 'IT',\n 'UNDER',\n 'THE',\n 'TERMS',\n 'OF',\n 'THE',\n 'PROJECT',\n 'GUTENBERG',\n 'LICENSE',\n 'INCLUDED']"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_split_line(line):\n",
    "    a = re.sub('\\d+', '', line)\n",
    "    b = re.sub('[\\W]+', ' ', a)\n",
    "    return b.upper().split()\n",
    "\n",
    "words = text.flatMap(clean_split_line)\n",
    "words.take(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29c2185e-9ea0-48c4-98a9-8a4b303c52aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[55]: 126018"
     ]
    }
   ],
   "source": [
    "words.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0dc92a9-cfe2-498a-839a-5945925f6ea0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[56]: [('THE', 1),\n ('PROJECT', 1),\n ('GUTENBERG', 1),\n ('EBOOK', 1),\n ('OF', 1),\n ('PRIDE', 1),\n ('AND', 1),\n ('PREJUDICE', 1),\n ('BY', 1),\n ('JANE', 1)]"
     ]
    }
   ],
   "source": [
    "# We want to do something like the following\n",
    "# words_mapped = words.map(lambda x: (x,1))\n",
    "\n",
    "words_mapped = words.map(lambda x: (x,1))\n",
    "words_mapped.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b063e48d-b739-4678-8705-220eff5e2a54",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[57]: PythonRDD[36] at RDD at PythonRDD.scala:58"
     ]
    }
   ],
   "source": [
    "sorted_map = words_mapped.sortByKey()\n",
    "sorted_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2da4743b-d5bf-4c11-9073-b28c911ac012",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[58]: [('A', 1),\n ('A', 1),\n ('AFFABILITY', 1),\n ('ALL', 1),\n ('AMONG', 1),\n ('AN', 1),\n ('AND', 1),\n ('AND', 1),\n ('AND', 1),\n ('AND', 1),\n ('ASSEMBLY', 1),\n ('ATTEND', 1),\n ('BE', 1),\n ('BEG', 1),\n ('BETTER', 1),\n ('BRINGING', 1),\n ('BRINGING', 1),\n ('BY', 1),\n ('CAME', 1),\n ('CLOTHES', 1),\n ('DARCY', 1),\n ('DEAL', 1),\n ('DISAPPOINTMENT', 1),\n ('DO', 1),\n ('DO', 1),\n ('DOING', 1),\n ('EFFECT', 1),\n ('EITHER', 1),\n ('ESTABLISHMENT', 1),\n ('EVERYTHING', 1),\n ('EXPECTATIONS', 1),\n ('FIND', 1),\n ('FOR', 1),\n ('GLASS', 1),\n ('GOOD', 1),\n ('HAPPIEST', 1),\n ('HER', 1),\n ('HER', 1),\n ('HIS', 1),\n ('HIS', 1),\n ('HIS', 1),\n ('HOPES', 1),\n ('HORROR', 1),\n ('I', 1),\n ('I', 1),\n ('I', 1),\n ('I', 1),\n ('IMPARTIAL', 1),\n ('IN', 1),\n ('INSENSIBILITY', 1),\n ('IS', 1),\n ('IT', 1),\n ('JOY', 1),\n ('KNEW', 1),\n ('KNOW', 1),\n ('LEAST', 1),\n ('MANNER', 1),\n ('MENTIONED', 1),\n ('MINUTENESS', 1),\n ('MOMENT', 1),\n ('MONEY', 1),\n ('MORNING', 1),\n ('MUST', 1),\n ('NAME', 1),\n ('NEVER', 1),\n ('NEVER', 1),\n ('NEVER', 1),\n ('OF', 1),\n ('OF', 1),\n ('OF', 1),\n ('OFTEN', 1),\n ('OFTEN', 1),\n ('OH', 1),\n ('ON', 1),\n ('ONE', 1),\n ('ORDERED', 1),\n ('OWN', 1),\n ('PARTICULARS', 1),\n ('PERSONS', 1),\n ('PLACE', 1),\n ('PRAY', 1),\n ('PROMOTE', 1),\n ('RATIONAL', 1),\n ('RATIONAL', 1),\n ('READ', 1),\n ('SALLIED', 1),\n ('SATURDAY', 1),\n ('SEND', 1),\n ('SET', 1),\n ('SHE', 1),\n ('SHOCK', 1),\n ('SO', 1),\n ('SO', 1),\n ('SO', 1),\n ('SUCH', 1),\n ('SUPPLY', 1),\n ('TALKING', 1),\n ('TELL', 1),\n ('THE', 1),\n ('THE', 1),\n ('THE', 1),\n ('THE', 1),\n ('THE', 1),\n ('THE', 1),\n ('THE', 1),\n ('THE', 1),\n ('THEIR', 1),\n ('THEY', 1),\n ('THINK', 1),\n ('TO', 1),\n ('TO', 1),\n ('TONE', 1),\n ('TRIFLE', 1),\n ('UPON', 1),\n ('WAS', 1),\n ('WAS', 1),\n ('WAS', 1),\n ('WAS', 1),\n ('WE', 1),\n ('WE', 1),\n ('WILL', 1),\n ('WIN', 1),\n ('_EXPRESSION_', 1),\n ('_SUCH_', 1)]"
     ]
    }
   ],
   "source": [
    "sample = sorted_map.sample(withReplacement=False, fraction= 0.001)\n",
    "sample.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec9d2a96-e1f1-42f0-aae6-7b967f9a9c8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[59]: [('PRIDE', 52),\n ('UNITED', 22),\n ('OTHER', 227),\n ('WORLD', 68),\n ('NO', 501),\n ('GIVE', 127),\n ('LICENSE', 18),\n ('WWW', 9),\n ('ARE', 361),\n ('TO', 4245),\n ('DATE', 5),\n ('UPDATED', 2),\n ('ENGLISH', 1),\n ('CHARACTER', 65),\n ('PRODUCED', 13),\n ('ILLUSTRATED', 1),\n ('THAT', 1555),\n ('POSSESSION', 10),\n ('LITTLE', 187),\n ('KNOWN', 58),\n ('VIEWS', 11),\n ('CONSIDERED', 23),\n ('AS', 1193),\n ('ONE', 273),\n ('THEIR', 439),\n ('MR', 784),\n ('JUST', 72),\n ('TOLD', 69),\n ('ME', 427),\n ('ANSWER', 65),\n ('WHO', 288),\n ('TELL', 71),\n ('HEARING', 24),\n ('ENOUGH', 106),\n ('WHY', 53),\n ('YOUNG', 130),\n ('MONDAY', 8),\n ('FOUR', 35),\n ('MUCH', 327),\n ('AGREED', 13),\n ('MICHAELMAS', 2),\n ('SERVANTS', 13),\n ('WEEK', 29),\n ('NAME', 34),\n ('BINGLEY', 307),\n ('OH', 96),\n ('FIVE', 32),\n ('YEAR', 29),\n ('FINE', 31),\n ('CAN', 223)]"
     ]
    }
   ],
   "source": [
    "counts = words_mapped.reduceByKey(lambda x,y: x+y)\n",
    "counts.collect()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "061c3173-c09d-4670-ae2e-83fb9b45d7c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[62]: [('THE', 4521),\n ('TO', 4245),\n ('OF', 3734),\n ('AND', 3657),\n ('HER', 2202),\n ('I', 2048),\n ('A', 2006),\n ('IN', 1939),\n ('WAS', 1846),\n ('SHE', 1691),\n ('THAT', 1555),\n ('IT', 1550),\n ('NOT', 1449),\n ('YOU', 1401),\n ('HE', 1328),\n ('HIS', 1257),\n ('BE', 1256),\n ('AS', 1193),\n ('HAD', 1172),\n ('WITH', 1098),\n ('FOR', 1084),\n ('BUT', 1006),\n ('IS', 879),\n ('HAVE', 846),\n ('AT', 802),\n ('MR', 784),\n ('HIM', 752),\n ('ON', 729),\n ('MY', 702),\n ('S', 664),\n ('BY', 663),\n ('ALL', 640),\n ('ELIZABETH', 634),\n ('THEY', 599),\n ('SO', 593),\n ('WERE', 565),\n ('WHICH', 546),\n ('COULD', 526),\n ('BEEN', 513),\n ('FROM', 508),\n ('NO', 501),\n ('VERY', 490),\n ('THIS', 488),\n ('WHAT', 478),\n ('WOULD', 467),\n ('YOUR', 446),\n ('THEIR', 439),\n ('THEM', 429),\n ('ME', 427),\n ('DARCY', 417)]"
     ]
    }
   ],
   "source": [
    "counts = words_mapped.reduceByKey(lambda x,y: x+y).sortBy(lambda x: x[1], ascending=False)\n",
    "counts.collect()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2e54ff0-687c-4e5d-82cc-344771c6fb36",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 Âµs, sys: 0 ns, total: 3 Âµs\nWall time: 6.44 Âµs\nOut[64]: [('THE', 4521),\n ('TO', 4245),\n ('OF', 3734),\n ('AND', 3657),\n ('HER', 2202),\n ('I', 2048),\n ('A', 2006),\n ('IN', 1939),\n ('WAS', 1846),\n ('SHE', 1691),\n ('THAT', 1555),\n ('IT', 1550),\n ('NOT', 1449),\n ('YOU', 1401),\n ('HE', 1328),\n ('HIS', 1257),\n ('BE', 1256),\n ('AS', 1193),\n ('HAD', 1172),\n ('WITH', 1098),\n ('FOR', 1084),\n ('BUT', 1006),\n ('IS', 879),\n ('HAVE', 846),\n ('AT', 802),\n ('MR', 784),\n ('HIM', 752),\n ('ON', 729),\n ('MY', 702),\n ('S', 664),\n ('BY', 663),\n ('ALL', 640),\n ('ELIZABETH', 634),\n ('THEY', 599),\n ('SO', 593),\n ('WERE', 565),\n ('WHICH', 546),\n ('COULD', 526),\n ('BEEN', 513),\n ('FROM', 508),\n ('NO', 501),\n ('VERY', 490),\n ('THIS', 488),\n ('WHAT', 478),\n ('WOULD', 467),\n ('YOUR', 446),\n ('THEIR', 439),\n ('THEM', 429),\n ('ME', 427),\n ('DARCY', 417),\n ('WILL', 415),\n ('SAID', 402),\n ('SUCH', 391),\n ('OR', 376),\n ('WHEN', 372),\n ('IF', 372),\n ('AN', 367),\n ('ARE', 361),\n ('DO', 359),\n ('THERE', 353),\n ('MRS', 344),\n ('MUCH', 327),\n ('BENNET', 324),\n ('AM', 323),\n ('MORE', 321),\n ('MUST', 315),\n ('BINGLEY', 307),\n ('ANY', 307),\n ('JANE', 294),\n ('WHO', 288),\n ('THAN', 285),\n ('MISS', 283),\n ('ONE', 273),\n ('DID', 265),\n ('WE', 255),\n ('SHOULD', 247),\n ('KNOW', 237),\n ('HOW', 235),\n ('BEFORE', 231),\n ('OTHER', 227),\n ('HERSELF', 227),\n ('THOUGH', 226),\n ('HAS', 224),\n ('WELL', 224),\n ('CAN', 223),\n ('NEVER', 218),\n ('SISTER', 217),\n ('ONLY', 217),\n ('SOON', 215),\n ('THINK', 211),\n ('SOME', 208),\n ('MAY', 206),\n ('NOW', 203),\n ('TIME', 203),\n ('MIGHT', 201),\n ('AFTER', 200),\n ('GOOD', 200),\n ('EVERY', 198),\n ('WICKHAM', 194),\n ('MOST', 193)]"
     ]
    }
   ],
   "source": [
    "# As functional programming always returns new data instead of manipulating the data in-place, we can rewrite the above as:\n",
    "counts_test_2 = text.flatMap(clean_split_line).map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y).sortBy(lambda x: x[1], ascending=False)\n",
    "counts_test_2.take(100)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark_introduction_1",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
