{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d409b519-529a-4ef8-b774-2137612324f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af38d5d",
   "metadata": {},
   "source": [
    "### The case for distributed computing\n",
    "\n",
    "* What happens if the data to be analyzed is too large?\n",
    "  * e.g. cannot be stored on a single machine\n",
    "\n",
    "* What if the computation is too complex?\n",
    "  * e.g., in interactive mode, it is unacceptably slow\n",
    "\n",
    "* What if you have to deal with both situations?\n",
    "\n",
    "* Can you scale up? Can you scale out?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c935f96",
   "metadata": {},
   "source": [
    "### Scaling Up: Local Machine\n",
    "\n",
    "* Scaling up occurs within the same system hosting the data and running the computaiton\n",
    "  * Simple to carry out from a physical standpoint\n",
    "  * from a programmatic standpoint, it's managed by the operating system and programming libraries; does not require additional frameworks\n",
    "* Scabaility is typically limited by the OS physical resources on the system.\n",
    "  * RAM upper bound is determined by the OS  or the number of slots available on the motherboard\n",
    "* The cost of a single machine at the highest configuration may be prohibitive\n",
    "\n",
    "* May not meet the demands of the workload at hand\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7602d502",
   "metadata": {},
   "source": [
    "### Buying 256 GB of RAM \n",
    "\n",
    "* Single stick 256GB, DDR4 PC4-23400\n",
    "\n",
    "![](https://www.dropbox.com/s/b5v6sypv5mk8ber/Screen%20Shot%202022-09-05%20at%208.35.03%20PM.png?dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2f435f",
   "metadata": {},
   "source": [
    "### Maximum Number of Logical CPUs\n",
    "\n",
    "```Red Hat defines a logical CPU as any schedulable entity. So every core/thread in a multicore/thread processor is a logical CPU.``` [ref](https://access.redhat.com/articles/rhel-limits#maximum-logical-cpus-1)\n",
    "\n",
    "![](https://www.dropbox.com/s/1svoylj8jghz6n6/Screen%20Shot%202022-09-05%20at%208.19.22%20PM.png?dl=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5821b64",
   "metadata": {},
   "source": [
    "### Maximum Memory Supported\n",
    "\n",
    "![](https://www.dropbox.com/s/dlm7ofmqgko5wb4/Screen%20Shot%202022-09-05%20at%208.24.37%20PM.png?dl=1)\n",
    "From [ref](https://access.redhat.com/articles/rhel-limits#maximum-logical-cpus-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6772d058",
   "metadata": {},
   "source": [
    "### Storage limits\n",
    "\n",
    "![](https://www.dropbox.com/s/7hgqjm0gcrzcuej/Screen%20Shot%202022-09-05%20at%208.25.54%20PM.png?dl=1)\n",
    "From [ref](https://access.redhat.com/articles/rhel-limits#maximum-logical-cpus-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4553978",
   "metadata": {},
   "source": [
    "### Distributed Systems\n",
    "\n",
    "* Distributed systems divide the workload among multiple machines\n",
    "\n",
    "The typical process involves: \n",
    "  * A master node responsible for distributing the job and ensuring its completion\n",
    "  * Worker nodes that perform the actual work\n",
    " \n",
    "* Can be substantially more cost-effective compared to scaling out at maximum conifguration.\n",
    "\n",
    "* Distributed systems are typically complex due to the following requirements:\n",
    "  * Physically deploying and maintaining the platform; cabling, switches, machines, power management, etc.\n",
    "  * A new paradigm for implementing distributed tasks\n",
    "     * Think for instance, split-apply-combine paradigm\n",
    "  * Framework to manage the platform\n",
    "    * For instance, a master node needs to know how to split a file, way to assign workers, communicates the data, gathers and combine the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15ea1bc",
   "metadata": {},
   "source": [
    "### Distributed Systems\n",
    "\n",
    "* The hardware specs may not be the same across machines, adding another layer of complexity if it doesn't\n",
    "\n",
    "<img src=\"https://www.dropbox.com/s/8mncw4ffe8uajol/networking.jpg?dl=1\" width=\"700\" height=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd54e54",
   "metadata": {},
   "source": [
    "### Distributed Systems Requirements\n",
    "\n",
    "* Among other requirements, a distributed system should provide:\n",
    "\n",
    "  * Communication: nodes must be able to communicate with each other\n",
    "  * Fault tolerance: both data and computation should not be affected by system faults   \n",
    "  * Scalable processing: workload increases can be accommodated, to some extent, by increasing computational resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac85279",
   "metadata": {},
   "source": [
    "### Apache Spark\n",
    "\n",
    "* Apache Spark is an open-source, distributed processing system used for big data workloads.\n",
    "  * Runs on a cluster\n",
    "\n",
    "* It's an enhancement to Hadoop's MapReduce\n",
    "  * Processes and retains data in memory for subsequent steps\n",
    "  * For smaller workloads, Sparkâ€™s data processing speeds are up to 100x faster than Hadoop's MapReduce\n",
    "\n",
    "* Written in Scala and runs in the JVM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b4e7ee",
   "metadata": {},
   "source": [
    "### Apache Spark\n",
    "\n",
    "* Ideal for real-time processing as it utilizes in-memory caching and optimized query execution for fast queries against data of any size.  \n",
    "\n",
    "* Provides a richer ecosystem of functionality\n",
    "  * Over 80 high level operators beyond Map and Reduce\n",
    "    * Tools for pipeline construction and evaluation\n",
    "  * compared to Hadoop, Spark provides more operators other than map and reduce\n",
    "    * Includes libraries to support SQL queries, machine learning (MLlib), graph data analysis (GraphX) and streaming data analysis\n",
    "  * Plethora of functions for SQL-like operation, ML and working with graph data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241a217a-d6fe-42dd-96ab-1fe73797d988",
   "metadata": {},
   "source": [
    "### What is Apache Spark\n",
    "\n",
    "* [See Video](https://www.databricks.com/spark/about)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4bae26",
   "metadata": {},
   "source": [
    "### Spark and Functional Programming\n",
    "\n",
    "* To manipulate data, Spark uses functional programming\n",
    "  The functional programming paradigm is used in many popular languages including Common Lisp, Scheme, Clojure, OCaml, and Haskell\n",
    "  \n",
    "* Functional programming is a data oriented paradigm\n",
    "  * Decomposes a problem into a set of functions.\n",
    "  * Logic is implemented by applying and composing functions.\n",
    "\n",
    "* The idea is that functions should be able to manipulate data without maintaining any external state.\n",
    "  * No global variables\n",
    " \n",
    "* In functional programming, we need to always return new data instead of manipulating the data in-place."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31893de1",
   "metadata": {},
   "source": [
    "### Core Components of Spark\n",
    "<img src=\"https://www.dropbox.com/s/azebxe8nv5nsqne/spark_architecture.png?dl=1\" width=\"900\" height=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5c0d7d",
   "metadata": {},
   "source": [
    "### Core Components of Spark\n",
    "\n",
    "* **Spark Core**\n",
    "  * Basic functionality:\n",
    "    * APIs that define datasets (RDDs)\n",
    "    * operations and actions to process RDDs\n",
    "    \n",
    "* **Spark SQL**\n",
    "    * Option ti comply with the ANSI SQL standard\n",
    "    * APIs to interact with Apache Hive's variant of SQL called Hive Query Language (HiveQL).\n",
    "    * DB tables are RDDs and Spark SQL queries are transformed into Spark operations\n",
    "\n",
    "* **Spark Streaming**\n",
    "    * Enables the processing and manipulation of live streaming data.\n",
    "    \n",
    "* **MLlib**\n",
    "  * Implementation of machine learning algorithms using Spark on RDDs\n",
    "  * Basic algorithms for classifications, regressions,\n",
    "\n",
    "* **GraphX\n",
    "  * Functionality for manipulating graphs and performing parallel graph operations and computations\n",
    "  * A sort of large-scale Neo4J"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c32a78f",
   "metadata": {},
   "source": [
    "### Spark Operations and RDDs\n",
    "\n",
    "* Spark provides its own distributed data framework called resilient distributed datasets or RDDs\n",
    "  * RDD is an abstraction that represents a read-only collection of objects that are partitioned across machines\n",
    "  * Partitioning ensures fault tolerance and maximize job efficiency\n",
    "    * RDDs can be accessed via parallel operations\n",
    "\n",
    "* RDDs are cached in memory, making it efficient to iterate on the same data\n",
    "  * Ideal for operations such as optimization or some ML algorithms\n",
    "  * Fast operation speed makes it ideal for command-line-based queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718bd2bd",
   "metadata": {},
   "source": [
    "### Spark Paradigm\n",
    "\n",
    "* A Spark program typically follows a simple paradigm:\n",
    "\n",
    "\n",
    "1. The main program is the `application driver` \n",
    "2. The program has one or more workers, called executors,\n",
    "  * Those run code sent to them by the driver on their partitions of the data\n",
    "  * Execution is dispatched, not code\n",
    "3. Results are then sent back to the driver for aggregation or compilation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60f9070",
   "metadata": {},
   "source": [
    "### Spark Job Sequence\n",
    "\n",
    "<img src=\"https://www.dropbox.com/s/tv0vaxyuxzmxud4/Page3.jpg?dl=1\" width=\"900\" height=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b5f9f0",
   "metadata": {},
   "source": [
    "### Spark Application Manager\n",
    "\n",
    "<img src=\"https://www.dropbox.com/s/3fb4tw290rc3dy2/Page4.jpg?dl=1\" width=\"900\" height=\"600\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5659f28e",
   "metadata": {},
   "source": [
    "### Spark Execution using PySpark\n",
    "\n",
    "\n",
    "* Interact with the Scala Interface using the PySpark Python library\n",
    "\n",
    "  * Wrapper that uses almost exactly the same function and attribute names\n",
    "\n",
    "![](https://www.dropbox.com/s/do918x5bpeh8oh4/cluster_mode.png?dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5090af93",
   "metadata": {},
   "source": [
    "### Setting a Docker Cluster\n",
    "\n",
    "* Manually installing Spark and all its components can be a daunting task.\n",
    " * Manually deploying, configure and optimizing a Spark is complex and time consuming\n",
    " \n",
    "* Easy to deploy on the cloud. See for instance:\n",
    "  * [Amazon's EMR](https://aws.amazon.com/emr/features/spark/)\n",
    "  * DataBricks free [micro solution](https://www.databricks.com/product/faq/community-edition) and paid offerings\n",
    "  * Vairous other providers, incuding Google (dataproc), Microsoft (HDInsight), etc.\n",
    "  * A common approach to occasional jobs\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d9c330",
   "metadata": {},
   "source": [
    "### Installing Via Docker For ICS438\n",
    "\n",
    "* It is easy to use Docker to install locally. We will use the following Docker image\n",
    "  \n",
    "```  \n",
    "jupyter/all-spark-notebook\n",
    "```\n",
    "* There are other docker images, including (jupyter/pyspark-notebook), which does not include the jobs dashboard `http://localhost:4040`\n",
    "\n",
    "* We will run the infrastructure as follows:\n",
    "\n",
    "```\n",
    "docker run --rm -p 4040:4040 -p 8888:8888 -v $(pwd):/home/jovyan/work jupyter/all-spark-notebook\n",
    "```\n",
    "\n",
    "* This configuration created a master and compute nodes locally in a docker instance\n",
    " \n",
    "* While you're probably not going to need to, you can log into the running container using: `docker exec -it <CONTAINER_ID> bash`\n",
    "\n",
    " * where <CONTAINER_ID> of the container currently running the `jupyter/all-spark-notebook` image\n",
    "\n",
    "\n",
    "* The Docker instance has all the libraries installed and ready to go.\n",
    "\n",
    "* Make sure you run a Jupyer notebook on the Docker instnace\n",
    "  * If the code below fails, this means you're not running in the Docker instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8309122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pip install pyspark\n",
    "\n",
    "# from pyspark import SparkContext\n",
    "# sc = SparkContext()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47b7cb69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmaster\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mappName\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msparkHome\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpyFiles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0menvironment\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbatchSize\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mserializer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Serializer'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCloudPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mconf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkConf\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mgateway\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_gateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJavaGateway\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mjsc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_gateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJavaObject\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mprofiler_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mType\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBasicProfiler\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0;34m'pyspark.profiler.BasicProfiler'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mudf_profiler_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mType\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUDFBasicProfiler\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0;34m'pyspark.profiler.UDFBasicProfiler'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Main entry point for Spark functionality. A SparkContext represents the\n",
       "connection to a Spark cluster, and can be used to create :class:`RDD` and\n",
       "broadcast variables on that cluster.\n",
       "\n",
       "When you create a new SparkContext, at least the master and app name should\n",
       "be set, either through the named parameters here or through `conf`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "master : str, optional\n",
       "    Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]).\n",
       "appName : str, optional\n",
       "    A name for your job, to display on the cluster web UI.\n",
       "sparkHome : str, optional\n",
       "    Location where Spark is installed on cluster nodes.\n",
       "pyFiles : list, optional\n",
       "    Collection of .zip or .py files to send to the cluster\n",
       "    and add to PYTHONPATH.  These can be paths on the local file\n",
       "    system or HDFS, HTTP, HTTPS, or FTP URLs.\n",
       "environment : dict, optional\n",
       "    A dictionary of environment variables to set on\n",
       "    worker nodes.\n",
       "batchSize : int, optional\n",
       "    The number of Python objects represented as a single\n",
       "    Java object. Set 1 to disable batching, 0 to automatically choose\n",
       "    the batch size based on object sizes, or -1 to use an unlimited\n",
       "    batch size\n",
       "serializer : :class:`pyspark.serializers.Serializer`, optional\n",
       "    The serializer for RDDs.\n",
       "conf : :py:class:`pyspark.SparkConf`, optional\n",
       "    An object setting Spark properties.\n",
       "gateway : :py:class:`py4j.java_gateway.JavaGateway`,  optional\n",
       "    Use an existing gateway and JVM, otherwise a new JVM\n",
       "    will be instantiated. This is only used internally.\n",
       "jsc : :py:class:`py4j.java_gateway.JavaObject`, optional\n",
       "    The JavaSparkContext instance. This is only used internally.\n",
       "profiler_cls : type, optional\n",
       "    A class of custom Profiler used to do profiling\n",
       "    (default is :class:`pyspark.profiler.BasicProfiler`).\n",
       "udf_profiler_cls : type, optional\n",
       "    A class of custom Profiler used to do udf profiling\n",
       "    (default is :class:`pyspark.profiler.UDFBasicProfiler`).\n",
       "\n",
       "Notes\n",
       "-----\n",
       "Only one :class:`SparkContext` should be active per JVM. You must `stop()`\n",
       "the active :class:`SparkContext` before creating a new one.\n",
       "\n",
       ":class:`SparkContext` instance is not supported to share across multiple\n",
       "processes out of the box, and PySpark does not guarantee multi-processing execution.\n",
       "Use threads instead for concurrent processing purpose.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from pyspark.context import SparkContext\n",
       ">>> sc = SparkContext('local', 'test')\n",
       ">>> sc2 = SparkContext('local', 'test2') # doctest: +IGNORE_EXCEPTION_DETAIL\n",
       "Traceback (most recent call last):\n",
       "    ...\n",
       "ValueError: ...\n",
       "\u001b[0;31mFile:\u001b[0m           /usr/local/spark/python/pyspark/context.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SparkContext?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f872e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Spark version is {sc.version}\")\n",
    "\n",
    "print(f\"Python version is {sc.pythonVer}\")\n",
    "\n",
    "print(f\"The name of the master is {sc.master}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f0394f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.getConf().getAll()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab254578",
   "metadata": {},
   "source": [
    "### Creating a Text RDD\n",
    "\n",
    "* You can create RDDs in a number of ways:\n",
    "\n",
    "    * `parallelize()`: function to transform Python collections (list-like data structures) into `RDD`s\n",
    "        * Distributes the passed list and makes it fault-tolerant\n",
    "\n",
    "* Another easy way to create RDDs is to read in a file with `textFile()`\n",
    "\n",
    "* Creates an RDD where every object is a line of the input text file\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea7022a-665e-4789-919a-c5a91a8b3b98",
   "metadata": {},
   "source": [
    "### Fundamental Operations on RDDs\n",
    "\n",
    "* Once an RDD is created, we can access its `map`, `reduce` and `filter` methods\n",
    " * Those operations and others we will cover are called 'transformations'\n",
    " * A transformation on a `RDD` yields a new `RDD`\n",
    " * `flatMap` is also commonly used and is equivalent to `itertools.chain()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69604219",
   "metadata": {},
   "source": [
    "### Transformations Versus Actions\n",
    "\n",
    "* Transformation: transform the data. They include:\n",
    "    * Narrow-dependency transformations:\n",
    "      * data partitions can be processed independently, e.g. filter values and drop column\n",
    "    * Wide-dependency transformations: \n",
    "      * Dependency across partitions, e.g., groupBy, aggregate\n",
    "\n",
    "\n",
    "* Actions: trigger the work.\n",
    "  * Each action triggers a Spark jobs\n",
    "\n",
    "Ex. Why split those? \n",
    "  * E.g.: query optimization. For example, groupBy then filter is better optimized as filter then groupBy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b960fac3-ba6c-4085-9b63-ac63bec81c7a",
   "metadata": {},
   "source": [
    "### Example Query\n",
    "\n",
    "\n",
    "```python\n",
    "flights_df = spark.read.option(\"head\", \"true\").option(\"inferSchema\", \"true\").csv(\"flights_info.csv\")\n",
    "\n",
    "flights_data_partitioned_df = flights_data.repartition(minPartitions=4)\n",
    "counts_df = flights_data_partitioned_df.where(\"duration > 120\")\n",
    "                                       .select(\"dep\", \"dest\", \"carrier\", \"durations\")\n",
    "                                       .groupBy(\"carrier\")\n",
    "                                       .count()\n",
    "counts_df.collect()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040cdadb",
   "metadata": {},
   "source": [
    "### PySpark: Job, Stages and Tasks\n",
    "\n",
    "<img src=\"https://www.dropbox.com/s/5qa1fb7p867i787/Page5.jpg?dl=1\" width=\"900\" height=\"600\">\n",
    "\n",
    "\n",
    "* Stages are run in succession, potentially across separate nodes\n",
    "* Tasks are run in parallel\n",
    "* Computation is dispatched to the data.\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9826f923",
   "metadata": {},
   "source": [
    "<img src=\"https://www.dropbox.com/s/4n6cyf2n60zs46v/Page6.jpg?dl=1\" width=\"900\" height=\"600\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c44926f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting randomuser\n",
      "  Downloading randomuser-1.6.tar.gz (5.0 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: randomuser\n",
      "  Building wheel for randomuser (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for randomuser: filename=randomuser-1.6-py3-none-any.whl size=5067 sha256=7a0cf14e8630ad5f5a2fd977269a710c9b3d3be26a06c2cb0bb3d08a88aff43c\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/b8/f3/19/6a938647065b4bb2471a9d063647d14d4fcc3236731f4e2b53\n",
      "Successfully built randomuser\n",
      "Installing collected packages: randomuser\n",
      "Successfully installed randomuser-1.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install randomuser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78163e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user object  is <randomuser.RandomUser object at 0x4066ef5870>\n",
      "user json representation  is\n",
      " {'user_id': '960-45-6374', 'first_name': 'Julian', 'last_name': 'Bowman', 'state': 'Maryland', 'zip': 11844, 'lat_long': {'latitude': '84.1334', 'longitude': '-110.5892'}}\n"
     ]
    }
   ],
   "source": [
    "# Insstall using the following if not already installed \n",
    "\n",
    "from randomuser import RandomUser\n",
    "\n",
    "# # Generate a single user\n",
    "user = RandomUser({\"nat\": \"us\"})\n",
    "print(f\"user object  is {user}\")\n",
    "def get_user_info(u):\n",
    "\n",
    "    user_dict = {\n",
    "        \"user_id\": u.get_id()[\"number\"], \n",
    "        \"first_name\": u.get_first_name(), \n",
    "        \"last_name\": u.get_last_name(), \n",
    "        \"state\": u.get_state(),\n",
    "        \"zip\": u.get_zipcode(),\n",
    "        \"lat_long\": u.get_coordinates()\n",
    "    }\n",
    "    return user_dict\n",
    "\n",
    "user_json = get_user_info(user)\n",
    "print(f\"user json representation  is\\n {user_json}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9cd1a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<randomuser.RandomUser at 0x407921f5e0>,\n",
       " <randomuser.RandomUser at 0x407921e260>,\n",
       " <randomuser.RandomUser at 0x407921e590>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_users = RandomUser.generate_users(5000, {\"nat\": \"us\"})\n",
    "print(len(my_users))\n",
    "my_users[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fc4ca61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'user_id': '988-44-7974',\n",
       "  'first_name': 'Julio',\n",
       "  'last_name': 'White',\n",
       "  'state': 'Michigan',\n",
       "  'zip': 64953,\n",
       "  'lat_long': {'latitude': '-85.8992', 'longitude': '-35.8376'}},\n",
       " {'user_id': '848-14-4659',\n",
       "  'first_name': 'Isaac',\n",
       "  'last_name': 'Phillips',\n",
       "  'state': 'Indiana',\n",
       "  'zip': 70288,\n",
       "  'lat_long': {'latitude': '-34.5206', 'longitude': '1.8089'}},\n",
       " {'user_id': '419-81-7907',\n",
       "  'first_name': 'Gary',\n",
       "  'last_name': 'Wells',\n",
       "  'state': 'Connecticut',\n",
       "  'zip': 37970,\n",
       "  'lat_long': {'latitude': '-49.4423', 'longitude': '11.3928'}}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a list of 10 random users\n",
    "\n",
    "user_dicts = list(map(get_user_info, my_users))\n",
    "\n",
    "user_dicts[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96121a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of objects in my RDD is: 5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'user_id': '675-04-0332',\n",
       "  'first_name': 'Oscar',\n",
       "  'last_name': 'Chambers',\n",
       "  'state': 'Oregon',\n",
       "  'zip': 17604,\n",
       "  'lat_long': {'latitude': '60.8608', 'longitude': '-82.8117'}},\n",
       " {'user_id': '302-13-3578',\n",
       "  'first_name': 'Julie',\n",
       "  'last_name': 'Watson',\n",
       "  'state': 'Georgia',\n",
       "  'zip': 86710,\n",
       "  'lat_long': {'latitude': '87.7597', 'longitude': '-27.8451'}},\n",
       " {'user_id': '707-49-4477',\n",
       "  'first_name': 'Manuel',\n",
       "  'last_name': 'Burns',\n",
       "  'state': 'Florida',\n",
       "  'zip': 83279,\n",
       "  'lat_long': {'latitude': '64.0851', 'longitude': '99.6517'}}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_rdd = sc.parallelize(user_dicts)\n",
    "users_rdd_size  = users_rdd.count()\n",
    "print(f\"The number of objects in my RDD is: {users_rdd_size}\")\n",
    "users_rdd.takeSample(False, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "628f1734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[8] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_users_rdd = users_rdd.filter(lambda x: x['state'] in [\"Nebraska\", \"Hawaii\", \"Idaho\"])\n",
    "select_users_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2dbd10d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'user_id': '048-73-1753',\n",
       "  'first_name': 'Albert',\n",
       "  'last_name': 'Scott',\n",
       "  'state': 'Nebraska',\n",
       "  'zip': 94069,\n",
       "  'lat_long': {'latitude': '73.8760', 'longitude': '-13.5365'}},\n",
       " {'user_id': '018-56-7439',\n",
       "  'first_name': 'Veronica',\n",
       "  'last_name': 'Woods',\n",
       "  'state': 'Idaho',\n",
       "  'zip': 85540,\n",
       "  'lat_long': {'latitude': '35.9805', 'longitude': '-0.2080'}},\n",
       " {'user_id': '808-14-7364',\n",
       "  'first_name': 'June',\n",
       "  'last_name': 'Baker',\n",
       "  'state': 'Nebraska',\n",
       "  'zip': 89660,\n",
       "  'lat_long': {'latitude': '-76.5984', 'longitude': '-51.9512'}},\n",
       " {'user_id': '753-04-9746',\n",
       "  'first_name': 'Marvin',\n",
       "  'last_name': 'Beck',\n",
       "  'state': 'Idaho',\n",
       "  'zip': 62505,\n",
       "  'lat_long': {'latitude': '37.4552', 'longitude': '-169.8550'}},\n",
       " {'user_id': '553-71-2902',\n",
       "  'first_name': 'Anita',\n",
       "  'last_name': 'Hernandez',\n",
       "  'state': 'Nebraska',\n",
       "  'zip': 37841,\n",
       "  'lat_long': {'latitude': '-0.7319', 'longitude': '-57.2282'}},\n",
       " {'user_id': '770-54-5308',\n",
       "  'first_name': 'Adam',\n",
       "  'last_name': 'Powell',\n",
       "  'state': 'Idaho',\n",
       "  'zip': 23455,\n",
       "  'lat_long': {'latitude': '-32.0396', 'longitude': '-84.6901'}},\n",
       " {'user_id': '549-79-3805',\n",
       "  'first_name': 'Harper',\n",
       "  'last_name': 'Wilson',\n",
       "  'state': 'Hawaii',\n",
       "  'zip': 39060,\n",
       "  'lat_long': {'latitude': '52.3857', 'longitude': '83.3369'}},\n",
       " {'user_id': '548-33-4893',\n",
       "  'first_name': 'Alexis',\n",
       "  'last_name': 'Gray',\n",
       "  'state': 'Idaho',\n",
       "  'zip': 94175,\n",
       "  'lat_long': {'latitude': '-13.8357', 'longitude': '80.4992'}},\n",
       " {'user_id': '239-81-2371',\n",
       "  'first_name': 'Kenzi',\n",
       "  'last_name': 'Barrett',\n",
       "  'state': 'Hawaii',\n",
       "  'zip': 46168,\n",
       "  'lat_long': {'latitude': '86.0542', 'longitude': '37.6299'}},\n",
       " {'user_id': '278-25-5983',\n",
       "  'first_name': 'Tony',\n",
       "  'last_name': 'Rose',\n",
       "  'state': 'Idaho',\n",
       "  'zip': 83281,\n",
       "  'lat_long': {'latitude': '89.1543', 'longitude': '41.5824'}}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect the result means grab them from all the chunk nodes\n",
    "select_users_rdd.collect()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef4a3e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building an RDD from a text file.\n",
    "text = sc.textFile('data/pride_and_prejudice.txt', minPartitions=4)\n",
    "### Number of items in the RDD\n",
    "text.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f176316c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numbe of objects in the RDD is 14579\n",
      "numbe of lines in the text file is 14579\n"
     ]
    }
   ],
   "source": [
    "text_rdd_size = text.count()\n",
    "print(f\"numbe of objects in the RDD is {text_rdd_size}\")\n",
    "\n",
    "nb_lines = len(open(\"data/pride_and_prejudice.txt\").readlines())\n",
    "print(f\"numbe of lines in the text file is {nb_lines}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81655fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of subset_x is: 10\n",
      "\n",
      "type of subset_x is: <class 'list'>\n",
      "\n",
      "subset_x is:\n",
      "['The Project Gutenberg eBook of Pride and Prejudice, by Jane Austen', '', 'This eBook is for the use of anyone anywhere in the United States and', 'most other parts of the world at no cost and with almost no restrictions', 'whatsoever. You may copy it, give it away or re-use it under the terms', 'of the Project Gutenberg License included with this eBook or online at', 'www.gutenberg.org. If you are not located in the United States, you', 'will have to check the laws of the country where you are located before', 'using this eBook.', '']\n"
     ]
    }
   ],
   "source": [
    "subset_x = text.take(10)\n",
    "print(f\"len of subset_x is: {len(subset_x)}\\n\")\n",
    "print(f\"type of subset_x is: {type(subset_x)}\\n\")\n",
    "print(f\"subset_x is:\\n{subset_x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "883d4b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['THE',\n",
       "  'PROJECT',\n",
       "  'GUTENBERG',\n",
       "  'EBOOK',\n",
       "  'OF',\n",
       "  'PRIDE',\n",
       "  'AND',\n",
       "  'PREJUDICE',\n",
       "  'BY',\n",
       "  'JANE',\n",
       "  'AUSTEN'],\n",
       " [],\n",
       " ['THIS',\n",
       "  'EBOOK',\n",
       "  'IS',\n",
       "  'FOR',\n",
       "  'THE',\n",
       "  'USE',\n",
       "  'OF',\n",
       "  'ANYONE',\n",
       "  'ANYWHERE',\n",
       "  'IN',\n",
       "  'THE',\n",
       "  'UNITED',\n",
       "  'STATES',\n",
       "  'AND'],\n",
       " ['MOST',\n",
       "  'OTHER',\n",
       "  'PARTS',\n",
       "  'OF',\n",
       "  'THE',\n",
       "  'WORLD',\n",
       "  'AT',\n",
       "  'NO',\n",
       "  'COST',\n",
       "  'AND',\n",
       "  'WITH',\n",
       "  'ALMOST',\n",
       "  'NO',\n",
       "  'RESTRICTIONS'],\n",
       " ['WHATSOEVER',\n",
       "  'YOU',\n",
       "  'MAY',\n",
       "  'COPY',\n",
       "  'IT',\n",
       "  'GIVE',\n",
       "  'IT',\n",
       "  'AWAY',\n",
       "  'OR',\n",
       "  'RE',\n",
       "  'USE',\n",
       "  'IT',\n",
       "  'UNDER',\n",
       "  'THE',\n",
       "  'TERMS'],\n",
       " ['OF',\n",
       "  'THE',\n",
       "  'PROJECT',\n",
       "  'GUTENBERG',\n",
       "  'LICENSE',\n",
       "  'INCLUDED',\n",
       "  'WITH',\n",
       "  'THIS',\n",
       "  'EBOOK',\n",
       "  'OR',\n",
       "  'ONLINE',\n",
       "  'AT'],\n",
       " ['WWW',\n",
       "  'GUTENBERG',\n",
       "  'ORG',\n",
       "  'IF',\n",
       "  'YOU',\n",
       "  'ARE',\n",
       "  'NOT',\n",
       "  'LOCATED',\n",
       "  'IN',\n",
       "  'THE',\n",
       "  'UNITED',\n",
       "  'STATES',\n",
       "  'YOU'],\n",
       " ['WILL',\n",
       "  'HAVE',\n",
       "  'TO',\n",
       "  'CHECK',\n",
       "  'THE',\n",
       "  'LAWS',\n",
       "  'OF',\n",
       "  'THE',\n",
       "  'COUNTRY',\n",
       "  'WHERE',\n",
       "  'YOU',\n",
       "  'ARE',\n",
       "  'LOCATED',\n",
       "  'BEFORE'],\n",
       " ['USING', 'THIS', 'EBOOK'],\n",
       " [],\n",
       " ['TITLE', 'PRIDE', 'AND', 'PREJUDICE'],\n",
       " [],\n",
       " ['AUTHOR', 'JANE', 'AUSTEN'],\n",
       " [],\n",
       " ['RELEASE', 'DATE', 'JUNE', 'EBOOK'],\n",
       " ['MOST', 'RECENTLY', 'UPDATED', 'AUGUST'],\n",
       " [],\n",
       " ['LANGUAGE', 'ENGLISH'],\n",
       " [],\n",
       " ['CHARACTER', 'SET', 'ENCODING', 'UTF'],\n",
       " [],\n",
       " ['PRODUCED', 'BY', 'ANONYMOUS', 'VOLUNTEERS', 'AND', 'DAVID', 'WIDGER'],\n",
       " [],\n",
       " ['START',\n",
       "  'OF',\n",
       "  'THE',\n",
       "  'PROJECT',\n",
       "  'GUTENBERG',\n",
       "  'EBOOK',\n",
       "  'PRIDE',\n",
       "  'AND',\n",
       "  'PREJUDICE'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['THERE',\n",
       "  'IS',\n",
       "  'AN',\n",
       "  'ILLUSTRATED',\n",
       "  'EDITION',\n",
       "  'OF',\n",
       "  'THIS',\n",
       "  'TITLE',\n",
       "  'WHICH',\n",
       "  'MAY',\n",
       "  'VIEWED',\n",
       "  'AT',\n",
       "  'EBOOK'],\n",
       " [],\n",
       " [],\n",
       " ['COVER'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['PRIDE', 'AND', 'PREJUDICE'],\n",
       " [],\n",
       " ['BY', 'JANE', 'AUSTEN'],\n",
       " [],\n",
       " ['CONTENTS'],\n",
       " [],\n",
       " ['CHAPTER'],\n",
       " [],\n",
       " ['CHAPTER'],\n",
       " [],\n",
       " ['CHAPTER'],\n",
       " [],\n",
       " ['CHAPTER'],\n",
       " [],\n",
       " ['CHAPTER'],\n",
       " [],\n",
       " ['CHAPTER'],\n",
       " [],\n",
       " ['CHAPTER'],\n",
       " [],\n",
       " ['CHAPTER'],\n",
       " [],\n",
       " ['CHAPTER'],\n",
       " []]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_split_line(line):\n",
    "    a = re.sub('\\d+', '', line)\n",
    "    b = re.sub('[\\W]+', ' ', a)\n",
    "    return b.upper().split()\n",
    "\n",
    "words = text.map(clean_split_line)\n",
    "words.take(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7b2eba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['THE',\n",
       " 'PROJECT',\n",
       " 'GUTENBERG',\n",
       " 'EBOOK',\n",
       " 'OF',\n",
       " 'PRIDE',\n",
       " 'AND',\n",
       " 'PREJUDICE',\n",
       " 'BY',\n",
       " 'JANE',\n",
       " 'AUSTEN',\n",
       " 'THIS',\n",
       " 'EBOOK',\n",
       " 'IS',\n",
       " 'FOR',\n",
       " 'THE',\n",
       " 'USE',\n",
       " 'OF',\n",
       " 'ANYONE',\n",
       " 'ANYWHERE',\n",
       " 'IN',\n",
       " 'THE',\n",
       " 'UNITED',\n",
       " 'STATES',\n",
       " 'AND',\n",
       " 'MOST',\n",
       " 'OTHER',\n",
       " 'PARTS',\n",
       " 'OF',\n",
       " 'THE',\n",
       " 'WORLD',\n",
       " 'AT',\n",
       " 'NO',\n",
       " 'COST',\n",
       " 'AND',\n",
       " 'WITH',\n",
       " 'ALMOST',\n",
       " 'NO',\n",
       " 'RESTRICTIONS',\n",
       " 'WHATSOEVER',\n",
       " 'YOU',\n",
       " 'MAY',\n",
       " 'COPY',\n",
       " 'IT',\n",
       " 'GIVE',\n",
       " 'IT',\n",
       " 'AWAY',\n",
       " 'OR',\n",
       " 'RE',\n",
       " 'USE',\n",
       " 'IT',\n",
       " 'UNDER',\n",
       " 'THE',\n",
       " 'TERMS',\n",
       " 'OF',\n",
       " 'THE',\n",
       " 'PROJECT',\n",
       " 'GUTENBERG',\n",
       " 'LICENSE',\n",
       " 'INCLUDED']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_split_line(line):\n",
    "    a = re.sub('\\d+', '', line)\n",
    "    b = re.sub('[\\W]+', ' ', a)\n",
    "    return b.upper().split()\n",
    "\n",
    "words = text.flatMap(clean_split_line)\n",
    "words.take(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6601d40a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126018"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19ce8420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['THE',\n",
       "   'PROJECT',\n",
       "   'GUTENBERG',\n",
       "   'EBOOK',\n",
       "   'OF',\n",
       "   'PRIDE',\n",
       "   'AND',\n",
       "   'PREJUDICE',\n",
       "   'BY',\n",
       "   'JANE',\n",
       "   'AUSTEN'],\n",
       "  1),\n",
       " ([], 1),\n",
       " (['THIS',\n",
       "   'EBOOK',\n",
       "   'IS',\n",
       "   'FOR',\n",
       "   'THE',\n",
       "   'USE',\n",
       "   'OF',\n",
       "   'ANYONE',\n",
       "   'ANYWHERE',\n",
       "   'IN',\n",
       "   'THE',\n",
       "   'UNITED',\n",
       "   'STATES',\n",
       "   'AND'],\n",
       "  1),\n",
       " (['MOST',\n",
       "   'OTHER',\n",
       "   'PARTS',\n",
       "   'OF',\n",
       "   'THE',\n",
       "   'WORLD',\n",
       "   'AT',\n",
       "   'NO',\n",
       "   'COST',\n",
       "   'AND',\n",
       "   'WITH',\n",
       "   'ALMOST',\n",
       "   'NO',\n",
       "   'RESTRICTIONS'],\n",
       "  1),\n",
       " (['WHATSOEVER',\n",
       "   'YOU',\n",
       "   'MAY',\n",
       "   'COPY',\n",
       "   'IT',\n",
       "   'GIVE',\n",
       "   'IT',\n",
       "   'AWAY',\n",
       "   'OR',\n",
       "   'RE',\n",
       "   'USE',\n",
       "   'IT',\n",
       "   'UNDER',\n",
       "   'THE',\n",
       "   'TERMS'],\n",
       "  1),\n",
       " (['OF',\n",
       "   'THE',\n",
       "   'PROJECT',\n",
       "   'GUTENBERG',\n",
       "   'LICENSE',\n",
       "   'INCLUDED',\n",
       "   'WITH',\n",
       "   'THIS',\n",
       "   'EBOOK',\n",
       "   'OR',\n",
       "   'ONLINE',\n",
       "   'AT'],\n",
       "  1),\n",
       " (['WWW',\n",
       "   'GUTENBERG',\n",
       "   'ORG',\n",
       "   'IF',\n",
       "   'YOU',\n",
       "   'ARE',\n",
       "   'NOT',\n",
       "   'LOCATED',\n",
       "   'IN',\n",
       "   'THE',\n",
       "   'UNITED',\n",
       "   'STATES',\n",
       "   'YOU'],\n",
       "  1),\n",
       " (['WILL',\n",
       "   'HAVE',\n",
       "   'TO',\n",
       "   'CHECK',\n",
       "   'THE',\n",
       "   'LAWS',\n",
       "   'OF',\n",
       "   'THE',\n",
       "   'COUNTRY',\n",
       "   'WHERE',\n",
       "   'YOU',\n",
       "   'ARE',\n",
       "   'LOCATED',\n",
       "   'BEFORE'],\n",
       "  1),\n",
       " (['USING', 'THIS', 'EBOOK'], 1),\n",
       " ([], 1)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We want to do something like the following\n",
    "# words_mapped = words.map(lambda x: (x,1))\n",
    "\n",
    "words_mapped = words.map(lambda x: (x,1))\n",
    "words_mapped.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d160702f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[21] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_map = words_mapped.sortByKey()\n",
    "sorted_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e02e627f-5dec-4a19-92f2-3953b7b8d4f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 1),\n",
       " ('A', 1),\n",
       " ('A', 1),\n",
       " ('ALL', 1),\n",
       " ('AND', 1),\n",
       " ('AND', 1),\n",
       " ('AND', 1),\n",
       " ('ASSEMBLED', 1),\n",
       " ('AT', 1),\n",
       " ('BE', 1),\n",
       " ('BE', 1),\n",
       " ('BEFORE', 1),\n",
       " ('BEFORE', 1),\n",
       " ('BENNET', 1),\n",
       " ('BUT', 1),\n",
       " ('BUT', 1),\n",
       " ('BY', 1),\n",
       " ('CANDOUR', 1),\n",
       " ('COACH', 1),\n",
       " ('COLONEL', 1),\n",
       " ('COLONEL', 1),\n",
       " ('CONFIRMATION', 1),\n",
       " ('CONTRIVED', 1),\n",
       " ('CRIED', 1),\n",
       " ('DARCY', 1),\n",
       " ('DAY', 1),\n",
       " ('DESIRED', 1),\n",
       " ('DIFFIDENCE', 1),\n",
       " ('DON', 1),\n",
       " ('EBOOK', 1),\n",
       " ('EDWARD', 1),\n",
       " ('ELIZABETH', 1),\n",
       " ('EMBARRASSMENT', 1),\n",
       " ('EVERYTHING', 1),\n",
       " ('EXAGGERATION', 1),\n",
       " ('HAD', 1),\n",
       " ('HAD', 1),\n",
       " ('HAD', 1),\n",
       " ('HE', 1),\n",
       " ('HER', 1),\n",
       " ('HER', 1),\n",
       " ('HERTFORDSHIRE', 1),\n",
       " ('HIM', 1),\n",
       " ('HIS', 1),\n",
       " ('HIS', 1),\n",
       " ('HOPE', 1),\n",
       " ('IN', 1),\n",
       " ('IN', 1),\n",
       " ('INDEED', 1),\n",
       " ('INDUCEMENT', 1),\n",
       " ('IS', 1),\n",
       " ('IT', 1),\n",
       " ('IT', 1),\n",
       " ('ITS', 1),\n",
       " ('KNOW', 1),\n",
       " ('LAUGHED', 1),\n",
       " ('LINES', 1),\n",
       " ('LOOSE', 1),\n",
       " ('MANNERS', 1),\n",
       " ('ME', 1),\n",
       " ('MORE', 1),\n",
       " ('MUST', 1),\n",
       " ('MYSELF', 1),\n",
       " ('NO', 1),\n",
       " ('NOT', 1),\n",
       " ('OCCASION', 1),\n",
       " ('OF', 1),\n",
       " ('OF', 1),\n",
       " ('OF', 1),\n",
       " ('PUT', 1),\n",
       " ('QUICKNESS', 1),\n",
       " ('REPEATED', 1),\n",
       " ('RETURN', 1),\n",
       " ('S', 1),\n",
       " ('S', 1),\n",
       " ('SAID', 1),\n",
       " ('SAID', 1),\n",
       " ('SATURDAY', 1),\n",
       " ('SEE', 1),\n",
       " ('SHE', 1),\n",
       " ('SHE', 1),\n",
       " ('SOMETIMES', 1),\n",
       " ('SPREAD', 1),\n",
       " ('STRUCK', 1),\n",
       " ('STUDYING', 1),\n",
       " ('TELL', 1),\n",
       " ('TEN', 1),\n",
       " ('THAN', 1),\n",
       " ('THAN', 1),\n",
       " ('THAT', 1),\n",
       " ('THAT', 1),\n",
       " ('THE', 1),\n",
       " ('THE', 1),\n",
       " ('THE', 1),\n",
       " ('THE', 1),\n",
       " ('THE', 1),\n",
       " ('THEM', 1),\n",
       " ('THEM', 1),\n",
       " ('THERE', 1),\n",
       " ('THERE', 1),\n",
       " ('THOUGHT', 1),\n",
       " ('THROUGH', 1),\n",
       " ('TO', 1),\n",
       " ('TO', 1),\n",
       " ('TO', 1),\n",
       " ('VERY', 1),\n",
       " ('VOICE', 1),\n",
       " ('WALKED', 1),\n",
       " ('WERE', 1),\n",
       " ('WERE', 1),\n",
       " ('WHAT', 1),\n",
       " ('WILL', 1),\n",
       " ('YES', 1),\n",
       " ('YES', 1),\n",
       " ('YOU', 1),\n",
       " ('_WAS_', 1)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = sorted_map.sample(withReplacement=False, fraction= 0.001)\n",
    "sample.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "62131881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PRIDE', 52),\n",
       " ('UNITED', 22),\n",
       " ('OTHER', 227),\n",
       " ('WORLD', 68),\n",
       " ('NO', 501),\n",
       " ('GIVE', 127),\n",
       " ('LICENSE', 18),\n",
       " ('WWW', 9),\n",
       " ('ARE', 361),\n",
       " ('TO', 4245),\n",
       " ('DATE', 5),\n",
       " ('UPDATED', 2),\n",
       " ('ENGLISH', 1),\n",
       " ('CHARACTER', 65),\n",
       " ('PRODUCED', 13),\n",
       " ('ILLUSTRATED', 1),\n",
       " ('THAT', 1555),\n",
       " ('POSSESSION', 10),\n",
       " ('LITTLE', 187),\n",
       " ('KNOWN', 58),\n",
       " ('VIEWS', 11),\n",
       " ('CONSIDERED', 23),\n",
       " ('AS', 1193),\n",
       " ('ONE', 273),\n",
       " ('THEIR', 439),\n",
       " ('MR', 784),\n",
       " ('JUST', 72),\n",
       " ('TOLD', 69),\n",
       " ('ME', 427),\n",
       " ('ANSWER', 65),\n",
       " ('WHO', 288),\n",
       " ('TELL', 71),\n",
       " ('HEARING', 24),\n",
       " ('ENOUGH', 106),\n",
       " ('WHY', 53),\n",
       " ('YOUNG', 130),\n",
       " ('MONDAY', 8),\n",
       " ('FOUR', 35),\n",
       " ('MUCH', 327),\n",
       " ('AGREED', 13),\n",
       " ('MICHAELMAS', 2),\n",
       " ('SERVANTS', 13),\n",
       " ('WEEK', 29),\n",
       " ('NAME', 34),\n",
       " ('BINGLEY', 307),\n",
       " ('OH', 96),\n",
       " ('FIVE', 32),\n",
       " ('YEAR', 29),\n",
       " ('FINE', 31),\n",
       " ('CAN', 223)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = words_mapped.reduceByKey(lambda x,y: x+y)\n",
    "counts.collect()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cd67ef09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40.9 ms, sys: 7.41 ms, total: 48.4 ms\n",
      "Wall time: 2.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('PRIDE', 52),\n",
       " ('UNITED', 22),\n",
       " ('OTHER', 227),\n",
       " ('WORLD', 68),\n",
       " ('NO', 501),\n",
       " ('GIVE', 127),\n",
       " ('LICENSE', 18),\n",
       " ('WWW', 9),\n",
       " ('ARE', 361),\n",
       " ('TO', 4245),\n",
       " ('DATE', 5),\n",
       " ('UPDATED', 2),\n",
       " ('ENGLISH', 1),\n",
       " ('CHARACTER', 65),\n",
       " ('PRODUCED', 13),\n",
       " ('ILLUSTRATED', 1),\n",
       " ('THAT', 1555),\n",
       " ('POSSESSION', 10),\n",
       " ('LITTLE', 187),\n",
       " ('KNOWN', 58),\n",
       " ('VIEWS', 11),\n",
       " ('CONSIDERED', 23),\n",
       " ('AS', 1193),\n",
       " ('ONE', 273),\n",
       " ('THEIR', 439),\n",
       " ('MR', 784),\n",
       " ('JUST', 72),\n",
       " ('TOLD', 69),\n",
       " ('ME', 427),\n",
       " ('ANSWER', 65),\n",
       " ('WHO', 288),\n",
       " ('TELL', 71),\n",
       " ('HEARING', 24),\n",
       " ('ENOUGH', 106),\n",
       " ('WHY', 53),\n",
       " ('YOUNG', 130),\n",
       " ('MONDAY', 8),\n",
       " ('FOUR', 35),\n",
       " ('MUCH', 327),\n",
       " ('AGREED', 13),\n",
       " ('MICHAELMAS', 2),\n",
       " ('SERVANTS', 13),\n",
       " ('WEEK', 29),\n",
       " ('NAME', 34),\n",
       " ('BINGLEY', 307),\n",
       " ('OH', 96),\n",
       " ('FIVE', 32),\n",
       " ('YEAR', 29),\n",
       " ('FINE', 31),\n",
       " ('CAN', 223),\n",
       " ('NONSENSE', 8),\n",
       " ('THEREFORE', 75),\n",
       " ('VISIT', 53),\n",
       " ('PERHAPS', 76),\n",
       " ('PARTY', 58),\n",
       " ('THAN', 285),\n",
       " ('CONSIDER', 33),\n",
       " ('YOUR', 446),\n",
       " ('ESTABLISHMENT', 6),\n",
       " ('WILLIAM', 46),\n",
       " ('LUCAS', 70),\n",
       " ('DETERMINED', 32),\n",
       " ('THEY', 599),\n",
       " ('FEW', 72),\n",
       " ('HEARTY', 2),\n",
       " ('CONSENT', 13),\n",
       " ('DESIRE', 23),\n",
       " ('BIT', 3),\n",
       " ('OTHERS', 55),\n",
       " ('ALWAYS', 119),\n",
       " ('RECOMMEND', 12),\n",
       " ('SISTERS', 76),\n",
       " ('CHILDREN', 25),\n",
       " ('VEXING', 1),\n",
       " ('POOR', 38),\n",
       " ('NERVES', 4),\n",
       " ('HIGH', 17),\n",
       " ('OLD', 15),\n",
       " ('YEARS', 36),\n",
       " ('SINCE', 59),\n",
       " ('CAPRICE', 4),\n",
       " ('INSUFFICIENT', 10),\n",
       " ('UNDERSTANDING', 21),\n",
       " ('FANCIED', 7),\n",
       " ('NERVOUS', 4),\n",
       " ('VISITING', 5),\n",
       " ('WAITED', 7),\n",
       " ('ASSURING', 4),\n",
       " ('TILL', 93),\n",
       " ('EVENING', 74),\n",
       " ('AFTER', 200),\n",
       " ('FOLLOWING', 27),\n",
       " ('TRIMMING', 1),\n",
       " ('SUDDENLY', 16),\n",
       " ('ADDRESSED', 17),\n",
       " ('WE', 255),\n",
       " ('FORGET', 17),\n",
       " ('SHALL', 164),\n",
       " ('PROMISED', 21),\n",
       " ('TWO', 131)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As functional programming always returns new data instead of manipulating the data in-place, we can rewrite the above as:\n",
    "\n",
    "%%time\n",
    "counts_test_2 = text.flatMap(clean_split_line).map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y)\n",
    "counts_test_2.take(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b4a9b177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Project Gutenberg eBook of Pride and Prejudice, by Jane Austen']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeac4d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following won't return an error until an action is performed\n",
    "\n",
    "data_s1 = text.map(lambda x: len(x)/0)\n",
    "data_s2.filter(lambda x: x>0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a17040c",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 12.0 failed 1 times, most recent failure: Lost task 1.0 in stage 12.0 (TID 46) (bd518172607a executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_113/403181734.py\", line 3, in <lambda>\nZeroDivisionError: division by zero\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_113/403181734.py\", line 3, in <lambda>\nZeroDivisionError: division by zero\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# The following will generate an error since the transformation dividing by 0 \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# is executed\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# the `ZeroDivisionError: division by zero` is burried in many Scala error messages.\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mdata_s2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:1197\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[1;32m   1196\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1197\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 12.0 failed 1 times, most recent failure: Lost task 1.0 in stage 12.0 (TID 46) (bd518172607a executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_113/403181734.py\", line 3, in <lambda>\nZeroDivisionError: division by zero\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_113/403181734.py\", line 3, in <lambda>\nZeroDivisionError: division by zero\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# The following will generate an error since the transformation dividing by 0 \n",
    "# is executed\n",
    "# the `ZeroDivisionError: division by zero` is burried in many Scala error messages.\n",
    "\n",
    "data.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c483f56",
   "metadata": {},
   "source": [
    "### The Spark Computation DAG\n",
    "\n",
    "* Lazy evaluation is possible because Spark maintains a graph (DAG) of the transformations\n",
    "* The transformating are optimized and executed in the graph once an action is triggered\n",
    "\n",
    "* A simple exampe of an execution is:\n",
    "\n",
    "```python\n",
    "data_2 =  data_1.map(lambda x: x+2)\n",
    "# do some work here\n",
    "data_3 =  data_2.map(lambda x: x-2)\n",
    "```\n",
    "\n",
    "* The above transformations are not run because it does not change the value of `x`.\n",
    "  * `data_3` is equal to `data_1`\n",
    "\n",
    "* See the the following blog post about the catalyst optimizer.\n",
    "\n",
    "https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html\n",
    "\n",
    "* More on this when we cover Adaptive Query Execution \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
