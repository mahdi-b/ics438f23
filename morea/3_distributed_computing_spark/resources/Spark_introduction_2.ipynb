{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84f67fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/09/27 22:23:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# We start by importing SparkContext and creating a context\n",
    "# which we will use in class\n",
    "\n",
    "from pyspark import SparkContext\n",
    "# sc.stop()\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214d5671",
   "metadata": {},
   "source": [
    "### Fundamendal RDD Functions\n",
    "\n",
    "* In addition to `map`, `flatmap`, `filter` and `reduce`, Spark provides other useful RDD methods (transformations and action)\n",
    "\n",
    "* The objective is to provide the set of functions that facilitate programming in the map-reduce paradigm.\n",
    "\n",
    "* Some examples transformation are:\n",
    "  * `distinct`: return an RDD comprised of the unique elements of the calling `RDD`\n",
    "  * `union`: computes the union of two `RDD`s\n",
    "  * `intersection`: computes the intersection of two `RDD`s\n",
    "  * `foreach`: Takes a lambda function and applies to each element of the `RDD`\n",
    "    * Like `map` but does not return values\n",
    "  * `cartesian` returns the cartesian product between the calling `RDD` \n",
    "  * etc. \n",
    "\n",
    "* There are also various actions that can be applied to an `RDD`\n",
    "\n",
    "  * `count`: returns the number of elements in the `RDD`\n",
    "  * `sum`: returns the sum of of elements in the `RDD` \n",
    "    * assumes the values in the RDD are numric\n",
    "  * `mean`: returns the sum of of elements in the `RDD` \n",
    "    * assumes the values in the RDD are numric\n",
    "  * `stats`: computes and returns a tuple with the `count`, the `mean`, `stdev`, `max`, `min`\n",
    "  * etc.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5072fd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['A', 'B', 'C', 'D']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distinct example \n",
    "dataset_1 = sc.parallelize([\"A\", \"B\", \"C\", \"A\", \"C\", \"D\"])\n",
    "dataset_1.distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7792adb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'B', 'C', 'D', 'E', 'F']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Union example\n",
    "\n",
    "dataset_1 = sc.parallelize([\"A\", \"B\", \"C\"])\n",
    "dataset_2 = sc.parallelize([\"D\", \"E\", \"F\"])\n",
    "\n",
    "both_datasets = dataset_1.union(dataset_2)\n",
    "\n",
    "both_datasets.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dd3e369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B', 'C']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# intersection example\n",
    "dataset_1 = sc.parallelize([\"A\", \"B\", \"C\"])\n",
    "dataset_2 = sc.parallelize([\"N\", \"B\", \"C\", \"M\"])\n",
    "dataset_1.intersection(dataset_2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9007bb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A\n",
      "B\n",
      "C\n"
     ]
    }
   ],
   "source": [
    "# foreach example\n",
    "dataset_1.foreach(lambda x: print(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2470f8c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'D'), ('A', 'E'), ('A', 'F'), ('B', 'D'), ('B', 'E'), ('B', 'F')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cartesian example\n",
    "\n",
    "dataset_1 = sc.parallelize([\"A\", \"B\"])\n",
    "\n",
    "dataset_2 = sc.parallelize([\"D\", \"E\", \"F\"])\n",
    "\n",
    "dataset_1.cartesian(dataset_2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38786449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 3.5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sum, mean axamples\n",
    "\n",
    "dataset_1 = sc.parallelize([1, 2, 3, 4, 5, 6])\n",
    "(dataset_1.sum(), dataset_1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36424c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(count: 6, mean: 3.5, stdev: 1.707825127659933, max: 6.0, min: 1.0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stats example\n",
    "\n",
    "dataset_1.stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aa298e",
   "metadata": {},
   "source": [
    "### Fundamendal RDD Functions on Tuples\n",
    "\n",
    "* Recall that tuples of key, values are fundamental concept in the Map Reduce framework.\n",
    "  * ex. `[(\"THE\", 12), (\"HI\", \"2\"), (\"COURSE\", 2), (\"STUDENTS\", 3), ... ]`\n",
    "* `Spark` provides a set of methods (transformations and actions) that can be applied to data tuples \n",
    "\n",
    "* Transformations:\n",
    "  * `sortByKey`: returns a new RDD sorted by key   \n",
    "  * `reduceByKey`: takes a function `f` as input and returns a new RDD where the values were reduced using `f`   \n",
    "  * `groupByKey`: returns a new RDD where values were grouped by key\n",
    "  * `join`: returns a new RDD by grouping values between keys in two datasets\n",
    "    * Includes leftOuter, rightOuter, fullOuter\n",
    "\n",
    "* Actions: \n",
    "  * Type agnostic actions such as `count` work on any type of data\n",
    "  * Tuple RDD can be transformed using `map` before invoking an arithmetic action. E.g.:\n",
    "  \n",
    "```\n",
    "data_1  = sc.parallelize([(\"A\", 1), (\"A\", 3), (\"B\", 4), (\"C\", 6), ])\n",
    "data_1.map(lambda x: x[1]).sum()  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcb7a453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 2), ('B', 3), ('C', 12), ('D', '2')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sortByKey example\n",
    "\n",
    "data_1  = sc.parallelize([(\"C\", 12), (\"D\", \"2\"), (\"A\", 2), (\"B\", 3)])\n",
    "data_1.sortByKey().collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfb5a9fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 14), ('B', 3), ('C', 5)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduceByKey example\n",
    "\n",
    "data_1  = sc.parallelize([(\"A\", 12), (\"A\", 2), (\"C\", 2), (\"B\", 3), (\"C\", 3)])\n",
    "data_1.reduceByKey(lambda x,y: x+y).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af2003ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\t[12, 2, 5]\n",
      "B\t[3]\n",
      "C\t[2, 3]\n"
     ]
    }
   ],
   "source": [
    "# groupByKey example\n",
    "\n",
    "data_1  = sc.parallelize([(\"A\", 12), (\"A\", 2), (\"C\", 2), (\"B\", 3), (\"C\", 3), (\"A\", 5)])\n",
    "grouped_data = data_1.groupByKey().collect()\n",
    "\n",
    "for key, val in grouped_data:\n",
    "    print(f\"{key}\\t{list(val)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a4d6819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B', (4, 5)), ('C', (6, 7)), ('A', (1, 2)), ('A', (3, 2))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join exmaple\n",
    "\n",
    "data_1  = sc.parallelize([(\"A\", 1), (\"A\", 3), (\"B\", 4), (\"C\", 6), ])\n",
    "data_2  = sc.parallelize([(\"A\", 2),           (\"B\", 5), (\"C\", 7), ])\n",
    "\n",
    "data_1.join(data_2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffbe39af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B', (4, 5)), ('C', (6, 7)), ('A', (1, 2)), ('D', (3, None))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# leftOuterJoin exmaple\n",
    "\n",
    "data_1  = sc.parallelize([(\"A\", 1), (\"D\", 3), (\"B\", 4), (\"C\", 6), ])\n",
    "data_2  = sc.parallelize([(\"A\", 2),           (\"B\", 5), (\"C\", 7), ])\n",
    "\n",
    "data_1.leftOuterJoin(data_2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47b1942f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B', (4, 5)), ('C', (6, 7)), ('A', (1, 2))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rightOuterJoin exmaple\n",
    "\n",
    "data_1  = sc.parallelize([(\"A\", 1), (\"D\", 3), (\"B\", 4), (\"C\", 6), ])\n",
    "data_2  = sc.parallelize([(\"A\", 2),           (\"B\", 5), (\"C\", 7), ])\n",
    "\n",
    "data_1.rightOuterJoin(data_2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4248db03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply non tuple actions  by first using map\n",
    "# extract the values\n",
    "\n",
    "data_1  = sc.parallelize([(\"A\", 1), (\"A\", 3), (\"B\", 4), (\"C\", 6), ])\n",
    "data_1.map(lambda x: x[1]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a4d8c3",
   "metadata": {},
   "source": [
    "### Conslusion\n",
    "\n",
    "* RDD is an immutable distributed collection of objects of your data\n",
    "  * Lines, tuples, JSON objects, etc.\n",
    "    \n",
    "* RDD are partitioned across nodes and are operated in parallel with a low-level API\n",
    "  * transformations on RDD generate new RDD\n",
    "\n",
    "    \n",
    "* There are many RDD methods; use `dir` to see which methods are available\n",
    "\n",
    "\n",
    "* Note that RDD does not enforce structure\n",
    "  * There is nothing that prevents something like \n",
    "    \n",
    "    ```sc.parallelize([(\"A\", 1), {\"First\": \"John\", \"Salary\": 125_000}, (\"B\", 4), (\"C\", 6), ])```\n",
    "    \n",
    "* Spark provides an data structure that imposes structure and validation on data\n",
    "  * Allows for a better optimization of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c1fc4b",
   "metadata": {},
   "source": [
    "### Spark DataFrame\n",
    "\n",
    "* pySpark DataFrames are immutable distributed collection of data organized into named columns\n",
    "\n",
    "  * Conceptually equivalent to a table in a relational database or Python DataFrame \n",
    "  * Supports SQL queries or Python like selects \n",
    "\n",
    "```\n",
    "session.sql(\"SELECT * from users where age < 21\")\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "```\n",
    "users.filter(users.age < 21)\n",
    "```\n",
    "\n",
    "* The fact that the data is structured allows for richer optimizations under the hood\n",
    "\n",
    "* DataFrames can be constructed from a wide array of sources such as: \n",
    "  * structured data files\n",
    "  * tables in Hive \n",
    "  * external databases\n",
    "  * or existing RDDs\n",
    "\n",
    "* Just like RDDs were collections of objects, DataFrames are collections of instances (rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da572c4",
   "metadata": {},
   "source": [
    "\n",
    "### Spark Functions on DataFrames\n",
    "\n",
    "* Spark provides dozens of other high level functions that apply to its DataFrames\n",
    "  * These functions use the map-reduce paradigm to implement useful and commonly-used functions\n",
    "  * Important to remember that these are just conveniences that are efficiently implemented using the same core set of function: map, flatmap, filter and reduce\n",
    "  \n",
    "*  `SparkContext` was required to work with `RDD`s. Working DataFrames requires `SparkSession`\n",
    "  * Create, register and execute SQL queries require `SparkSession`\n",
    "  \n",
    "* We can create a DataFrame (DF) using different methods\n",
    "  * Reading a `csv`: every row is an object\n",
    "  * Reading a `json`: every record is an object\n",
    "    * Each line must contain a separate, self-contained valid JSON object.\n",
    "      * This is a special format called newline-delimited JSON.\n",
    "  * Reading a text file: every row is an object\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7a94a8",
   "metadata": {},
   "source": [
    "### On Schemas\n",
    "\n",
    "* Schemas describe the data types of your fields\n",
    "* Schemas are at the heart of optimizations in Spark\n",
    "* Schemas are critical for deriving the correct in-memory compression\n",
    "  * After compression, data in RAM can be smaller than the raw data on disk\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88cba53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext()\n",
    "from pyspark.sql import SparkSession\n",
    "session = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a8616e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14579\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(value='The Project Gutenberg eBook of Pride and Prejudice, by Jane Austen')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df  = session.read.text('./data/pride_and_prejudice.txt')\n",
    "print(text_df.count())\n",
    "text_df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44316f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "csv_df = session.read.csv(\"data/nyc-flights.csv\", header=True)\n",
    "print(csv_df.count())\n",
    "csv_df.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c207e5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(year,StringType,true),StructField(month,StringType,true),StructField(day,StringType,true),StructField(dep_time,StringType,true),StructField(dep_delay,StringType,true),StructField(arr_time,StringType,true),StructField(arr_delay,StringType,true),StructField(carrier,StringType,true),StructField(tailnum,StringType,true),StructField(flight,StringType,true),StructField(origin,StringType,true),StructField(dest,StringType,true),StructField(air_time,StringType,true),StructField(distance,StringType,true),StructField(hour,StringType,true),StructField(minute,StringType,true)))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b244a33",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StructType' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_34/668570451.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m StructType(\n\u001b[0m\u001b[1;32m      2\u001b[0m \tList(StructField(year,StringType,true),\n\u001b[1;32m      3\u001b[0m                 \u001b[0mStructField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mStringType\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                 \u001b[0mStructField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mday\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mStringType\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0mStructField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdep_time\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mStringType\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'StructType' is not defined"
     ]
    }
   ],
   "source": [
    "StructType(\n",
    "\tList(StructField(year,StringType,true),\n",
    "\t\tStructField(month,StringType,true),\n",
    "\t\tStructField(day,StringType,true),\n",
    "\t\tStructField(dep_time,StringType,true),\n",
    "\t\tStructField(dep_delay,StringType,true),\n",
    "\t\tStructField(arr_time,StringType,true),\n",
    "\t\tStructField(arr_delay,StringType,true),\n",
    "\t\tStructField(carrier,StringType,true),\n",
    "\t\tStructField(tailnum,StringType,true),\n",
    "\t\tStructField(flight,StringType,true),\n",
    "\t\tStructField(origin,StringType,true),\n",
    "\t\tStructField(dest,StringType,true),\n",
    "\t\tStructField(air_time,StringType,true),\n",
    "\t\tStructField(distance,StringType,true),\n",
    "\t\tStructField(hour,StringType,true),\n",
    "\t\tStructField(minute,StringType,true)\n",
    "\t)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "86b18edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32735\n",
      "CPU times: user 13.9 ms, sys: 8.83 ms, total: 22.7 ms\n",
      "Wall time: 2.26 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(year=2013, month=6, day=30, dep_time=940, dep_delay=15, arr_time=1216, arr_delay=-4, carrier='VX', tailnum='N626VA', flight=407, origin='JFK', dest='LAX', air_time=313, distance=2475, hour=9, minute=40),\n",
       " Row(year=2013, month=5, day=7, dep_time=1657, dep_delay=-3, arr_time=2104, arr_delay=10, carrier='DL', tailnum='N3760C', flight=329, origin='JFK', dest='SJU', air_time=216, distance=1598, hour=16, minute=57)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "csv_df = session.read.options(inferSchema = True).csv(\"data/nyc-flights.csv\", header=True)\n",
    "\n",
    "print(csv_df.count())\n",
    "\n",
    "csv_df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "45c3aa6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(year,IntegerType,true),StructField(month,IntegerType,true),StructField(day,IntegerType,true),StructField(dep_time,IntegerType,true),StructField(dep_delay,IntegerType,true),StructField(arr_time,IntegerType,true),StructField(arr_delay,IntegerType,true),StructField(carrier,StringType,true),StructField(tailnum,StringType,true),StructField(flight,IntegerType,true),StructField(origin,StringType,true),StructField(dest,StringType,true),StructField(air_time,IntegerType,true),StructField(distance,IntegerType,true),StructField(hour,IntegerType,true),StructField(minute,IntegerType,true)))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea8833a",
   "metadata": {},
   "source": [
    "```\n",
    "StructType(\n",
    "\tList(\n",
    "\t\tStructField(year,IntegerType,true),\n",
    "\t\tStructField(month,IntegerType,true),\n",
    "\t\tStructField(day,IntegerType,true),\n",
    "\t\tStructField(dep_time,IntegerType,true),\n",
    "\t\tStructField(dep_delay,IntegerType,true),\n",
    "\t\tStructField(arr_time,IntegerType,true),\n",
    "\t\tStructField(arr_delay,IntegerType,true),\n",
    "\t\tStructField(carrier,StringType,true),\n",
    "\t\tStructField(tailnum,StringType,true),\n",
    "\t\tStructField(flight,IntegerType,true),\n",
    "\t\tStructField(origin,StringType,true),\n",
    "\t\tStructField(dest,StringType,true),\n",
    "\t\tStructField(air_time,IntegerType,true),\n",
    "\t\tStructField(distance,IntegerType,true),\n",
    "\t\tStructField(hour,IntegerType,true),\n",
    "\t\tStructField(minute,IntegerType,true)\n",
    "\t)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e4501a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "root\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- lat_long: struct (nullable = true)\n",
      " |    |-- latitude: double (nullable = true)\n",
      " |    |-- longitude: double (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- zip: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df  = session.read.json('./data/random_user_dicts.json')\n",
    "print(json_df.count())\n",
    "json_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e955e133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+--------------------+------------+-----------+-----+\n",
      "| first_name|last_name|            lat_long|       state|    user_id|  zip|\n",
      "+-----------+---------+--------------------+------------+-----------+-----+\n",
      "|Christopher|   Morgan|  {6.4442, -78.5063}|    Nebraska|895-76-0473|73093|\n",
      "|      Riley| Franklin|  {86.3108, 81.9157}|     Indiana|218-53-2453|43053|\n",
      "|      Tammy|Gutierrez|{10.2281, -141.5519}|    Delaware|290-41-4495|11862|\n",
      "|       Toni|  Steward| {84.6309, -97.4512}|     Alabama|937-19-1777|89473|\n",
      "|    Vanessa|   Vargas|{-46.5554, -22.8648}|    New York|036-93-1373|64349|\n",
      "|   Brooklyn|   Parker| {15.5293, 158.2673}|    Arkansas|946-76-3712|83694|\n",
      "|       Andy|  Carroll| {-42.0753, 91.2244}|     Wyoming|350-48-6550|74170|\n",
      "|     Austin|    Gomez|  {0.0292, -15.2938}|  Washington|491-00-8790|35510|\n",
      "|     Ernest|   Barnes|{-21.7537, -175.5...|    New York|990-89-5629|40333|\n",
      "|    Tiffany|  Bennett|{-42.3378, -68.4757}|    Colorado|624-71-6721|91276|\n",
      "|      Grace| Richards|{-6.0477, -162.1852}|      Kansas|571-00-9462|12056|\n",
      "|     Nevaeh|     Lowe|{-36.5445, -167.1...|    Maryland|930-96-1036|61852|\n",
      "|  Katherine|     Soto|   {-18.3, -26.1633}|        Ohio|138-27-9208|92254|\n",
      "|       Zack|   Murray|{77.5836, -132.4111}|  New Jersey|517-08-7893|14970|\n",
      "|     Joshua|    Hicks|{87.4694, -144.4074}|       Idaho|991-70-3007|98160|\n",
      "|     Sophie|     Ward| {55.3441, -90.8462}|   Minnesota|034-87-2163|58807|\n",
      "|  Francisco|    Payne| {42.7129, -10.4481}|    Arkansas|240-02-6185|92801|\n",
      "|       Jill|   Fisher| {38.5528, -38.2191}|Pennsylvania|130-07-1577|26650|\n",
      "|     Hector|  Griffin|{-64.5517, -172.6...|    Nebraska|643-40-7005|79261|\n",
      "|     Jessie|   Kelley|  {79.6399, 90.8092}|      Oregon|995-79-0823|10023|\n",
      "+-----------+---------+--------------------+------------+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c93db3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "\n",
    "json_struct = StructType([\n",
    "    StructField(\"first_name\", StringType(), nullable=False, metadata=None),\n",
    "    StructField(\"last_name\", StringType(),  nullable=False, metadata=None),\n",
    "    StructField(\"lat_long\", \n",
    "                StructType([\n",
    "                    StructField(\"latitude\", FloatType(), metadata=None, nullable=True),\n",
    "                    StructField(\"longitude\", FloatType(), metadata=None, nullable=True)\n",
    "                ]), nullable=True, metadata=None),\n",
    "    StructField(\"state\", StringType(),  nullable=True, metadata=None),\n",
    "    StructField(\"user_id\", StringType(),  nullable=True, metadata=None),\n",
    "    StructField(\"zip\", StringType(),  nullable=True, metadata=None),    \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eac5ec49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ArrayType',\n",
       " 'AtomicType',\n",
       " 'BinaryType',\n",
       " 'BooleanType',\n",
       " 'ByteType',\n",
       " 'CloudPickleSerializer',\n",
       " 'DataType',\n",
       " 'DataTypeSingleton',\n",
       " 'DateConverter',\n",
       " 'DateType',\n",
       " 'DatetimeConverter',\n",
       " 'DecimalType',\n",
       " 'DoubleType',\n",
       " 'FloatType',\n",
       " 'FractionalType',\n",
       " 'IntegerType',\n",
       " 'IntegralType',\n",
       " 'JavaClass',\n",
       " 'LongType',\n",
       " 'MapType',\n",
       " 'NullType',\n",
       " 'NumericType',\n",
       " 'Row',\n",
       " 'ShortType',\n",
       " 'SparkContext',\n",
       " 'StringType',\n",
       " 'StructField',\n",
       " 'StructType',\n",
       " 'TimestampType',\n",
       " 'UserDefinedType',\n",
       " '_FIXED_DECIMAL',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_acceptable_types',\n",
       " '_all_atomic_types',\n",
       " '_all_complex_types',\n",
       " '_array_signed_int_typecode_ctype_mappings',\n",
       " '_array_type_mappings',\n",
       " '_array_unsigned_int_typecode_ctype_mappings',\n",
       " '_atomic_types',\n",
       " '_create_converter',\n",
       " '_create_row',\n",
       " '_create_row_inbound_converter',\n",
       " '_has_nulltype',\n",
       " '_infer_schema',\n",
       " '_infer_type',\n",
       " '_int_size_to_type',\n",
       " '_make_type_verifier',\n",
       " '_merge_type',\n",
       " '_need_converter',\n",
       " '_parse_datatype_json_string',\n",
       " '_parse_datatype_json_value',\n",
       " '_parse_datatype_string',\n",
       " '_test',\n",
       " '_type_mappings',\n",
       " '_typecode',\n",
       " 'array',\n",
       " 'base64',\n",
       " 'calendar',\n",
       " 'ctypes',\n",
       " 'datetime',\n",
       " 'decimal',\n",
       " 'dt',\n",
       " 'json',\n",
       " 're',\n",
       " 'register_input_converter',\n",
       " 'size',\n",
       " 'sys',\n",
       " 'time']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "dir(pyspark.sql.types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e9c7683b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "{'fields': [{'metadata': {},\n",
      "             'name': 'first_name',\n",
      "             'nullable': True,\n",
      "             'type': 'string'},\n",
      "            {'metadata': {},\n",
      "             'name': 'last_name',\n",
      "             'nullable': True,\n",
      "             'type': 'string'},\n",
      "            {'metadata': {},\n",
      "             'name': 'lat_long',\n",
      "             'nullable': True,\n",
      "             'type': {'fields': [{'metadata': {},\n",
      "                                  'name': 'latitude',\n",
      "                                  'nullable': True,\n",
      "                                  'type': 'float'},\n",
      "                                 {'metadata': {},\n",
      "                                  'name': 'longitude',\n",
      "                                  'nullable': True,\n",
      "                                  'type': 'float'}],\n",
      "                      'type': 'struct'}},\n",
      "            {'metadata': {},\n",
      "             'name': 'state',\n",
      "             'nullable': True,\n",
      "             'type': 'string'},\n",
      "            {'metadata': {},\n",
      "             'name': 'user_id',\n",
      "             'nullable': True,\n",
      "             'type': 'string'},\n",
      "            {'metadata': {},\n",
      "             'name': 'zip',\n",
      "             'nullable': True,\n",
      "             'type': 'string'}],\n",
      " 'type': 'struct'}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "import json\n",
    "\n",
    "json_df  = session.read.schema(json_struct).json('./data/random_user_dicts.json')\n",
    "print(json_df.count())\n",
    "new_json = json_df.schema.json()\n",
    "pprint.pprint(json.loads(new_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2a53bfae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(first_name='Christopher', last_name='Morgan', lat_long=Row(latitude=6.444200038909912, longitude=-78.50630187988281), state='Nebraska', user_id='895-76-0473', zip='73093')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note the lat_long field. It has its own format\n",
    "\n",
    "json_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985b8eb7",
   "metadata": {},
   "source": [
    "### On Data Formats\n",
    "\n",
    "* Structured data (ex. tab or comma delimited tables) are ideal when data does not changes\n",
    "    * This is not the case in real life, data changes    \n",
    "\n",
    "* In Semi-structured data, fields are not necessarily shared by all observations\n",
    "\n",
    "```\n",
    "[ {\"user_id\": \"Jane1234\", employed: True, \"salary\": \"95000\"}, \n",
    "  {\"user_id\": \"Johhn777\", employed: False}, \n",
    "  ...\n",
    "]\n",
    "```\n",
    "\n",
    "\n",
    "* Also, tables do not allow for nested structures\n",
    "  * Cars is a list\n",
    "  * Each child has its own nested structure\n",
    "[..., {\n",
    "        \"user_id\": \"Jane1234\", \n",
    "        \"cars\": [\"Sedan\", \"Truck\"]\n",
    "        \"children\": {\n",
    "        \"Jonah\": {\"age\": 8\n",
    "                  \"School\": \"Noelani Elementary\"},\n",
    "        \"Mary\": {\"age\": 12\n",
    "                  \"School\": \"Sacred Hearts\"}\n",
    "                  \n",
    "        } \n",
    "      }...]\n",
    "\n",
    "\n",
    "* Disadvantages of JSON here:\n",
    "  * Space on disk (duplicated fields)\n",
    "    * Fields are not stored in the objects (in RAM) \n",
    "  * JSON, don't work with tranditional SQL\n",
    "    * We can do programmatic queries\n",
    "    * We can creare tempViews that can be queried \n",
    "      * Spark knows how to convert a colleciton of JSON objects to a table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5f4218",
   "metadata": {},
   "source": [
    "### On DataFrames and SQL\n",
    "\n",
    "* DataFrames support SQL \n",
    "  * SQL Only supported on DataFrames\n",
    "* The same SQL queries work out of the box on DataFrames\n",
    "* Use the `.sql()` method to pass it a SQL query\n",
    "  * Need to create a view\n",
    "* Queries are evaluated for optimization prior to execution \n",
    "  * Standard SQL-type query optimizations        \n",
    "* Optimization is done using Catalyst SQL optimizer \n",
    "    https://databricks.com/glossary/catalyst-optimizer\n",
    "* Helps to remember that the data is immutable\n",
    "    * adding new colums needs to be done on a new DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4023c9",
   "metadata": {},
   "source": [
    "# Example of query optimization\n",
    "![](https://www.dropbox.com/s/e756fxrsi36yvj4/unoptimized_optimized.png?dl=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "916dc8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "json_df  = session.read.schema(json_struct).json('./data/random_user_dicts.json')\n",
    "print(json_df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "498257d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|first_name|count(1)|\n",
      "+----------+--------+\n",
      "|     Tyler|       6|\n",
      "|  Samantha|      10|\n",
      "|    Aubrey|       4|\n",
      "|   Carolyn|      11|\n",
      "|      Chad|       9|\n",
      "|   Shannon|       8|\n",
      "|     Shawn|       5|\n",
      "|       Sue|      11|\n",
      "|     Scott|       6|\n",
      "|     Ruben|       9|\n",
      "|     Flenn|       5|\n",
      "|  Rosemary|       3|\n",
      "|     Grace|       7|\n",
      "|     Lucas|       8|\n",
      "|     Keith|      12|\n",
      "|    Gerald|      10|\n",
      "|       Jar|       6|\n",
      "|     Edwin|       6|\n",
      "|     Soham|       5|\n",
      "|  Savannah|       7|\n",
      "+----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df.createTempView(\"users\")\n",
    "\n",
    "session.sql(\"\"\"\n",
    "SELECT first_name, COUNT(*)\n",
    "FROM users\n",
    "GROUP BY first_name; \n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "b75799d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------------+-------------+-----------+-----+\n",
      "|first_name|last_name|            lat_long|        state|    user_id|  zip|\n",
      "+----------+---------+--------------------+-------------+-----------+-----+\n",
      "|      Evan|     Beck|  {-17.7362, -3.022}|New Hampshire|505-92-9095|87501|\n",
      "|      Evan|   Snyder| {56.7583, -64.3217}|    Minnesota|075-11-1233|74201|\n",
      "|     Sarah|     Webb|   {2.5834, -89.618}| South Dakota|247-48-1845|86063|\n",
      "|     Sarah|  Stanley|{-18.5369, -81.8778}|     Missouri|997-11-7309|68852|\n",
      "|      John|  Nichols|{-10.576, -148.6093}|      Alabama|575-68-2404|17965|\n",
      "|      John|     Reid|   {0.659, -70.5511}|  Mississippi|370-33-2662|57788|\n",
      "|      John|  Freeman|{-69.3141, -12.2351}|        Maine|621-84-6581|13221|\n",
      "|      John|    Price| {-7.5446, 109.4057}| North Dakota|323-62-2196|18403|\n",
      "|      John|    Davis|{-39.9862, -72.4857}|        Maine|211-00-2584|56206|\n",
      "|      Evan|   Torres|{-81.3332, 118.9237}|      Vermont|547-50-2905|64904|\n",
      "|      Evan|    Watts| {16.6248, -78.8383}|     Colorado|576-73-6525|41332|\n",
      "|      Evan|    Ortiz|{-76.9669, 150.0783}|  Mississippi|023-52-7382|61349|\n",
      "|      John|     Rose|  {63.1506, 82.6388}| Pennsylvania|857-84-5597|36311|\n",
      "|      John|    Mason| {89.3237, -14.4558}| Pennsylvania|639-53-5867|85588|\n",
      "|      Evan|    Reyes| {35.6993, 126.5463}|      Georgia|676-54-0295|29735|\n",
      "|      John|  Fleming|  {-88.764, -6.8801}|      Alabama|027-70-6298|36102|\n",
      "|      John|Mitchelle|{-18.6557, -68.5627}|     Michigan|177-04-6747|96502|\n",
      "|     Sarah|     King| {15.7888, 159.0693}|      Indiana|322-68-4414|16774|\n",
      "|      Evan|     Dean|{-48.4929, -93.6729}|       Hawaii|692-88-6252|81605|\n",
      "|     Sarah|   Little|  {-4.8479, 112.987}|     New York|421-82-5048|56231|\n",
      "+----------+---------+--------------------+-------------+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM users\n",
    "WHERE first_name IN (\"Evan\", \"Sarah\", \"John\"); \n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78bfbb7",
   "metadata": {},
   "source": [
    "### DataFrame and Select Queries\n",
    "\n",
    "* The same functionality is available using Python\n",
    "* Many additional functions, inlcuding analytics-specific ones are available through specific library\n",
    "\n",
    "    ```from pyspark.sql import functions as F```\n",
    "    * The functions as used with a select\n",
    "  * Can use `agg` to do specific operations and rename columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "2c1ae459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------------+-------------+-----------+-----+\n",
      "|first_name|last_name|            lat_long|        state|    user_id|  zip|\n",
      "+----------+---------+--------------------+-------------+-----------+-----+\n",
      "|       Joe|  Jackson|{-62.895, -143.5974}|      Florida|664-45-7303|23155|\n",
      "|       Mia|   Chavez|  {13.112, 109.6125}|     Arkansas|309-47-7003|42409|\n",
      "|       Jon|     Cole| {71.6651, 101.2318}| Rhode Island|217-81-4486|92912|\n",
      "|       Mia|  Barrett| {-57.829, -63.2612}|New Hampshire|236-16-5120|31229|\n",
      "|       Max|   Willis|   {31.288, 52.5112}|         Utah|858-00-9946|78049|\n",
      "|       Kim|  Spencer|  {50.676, -70.7382}|        Idaho|715-58-2909|67867|\n",
      "|       Roy| Mitchell|   {49.4706, 1.6422}|   Washington|421-56-4226|62647|\n",
      "|       Eli|   Turner|{-42.0611, 144.1344}|     Delaware|744-31-7784|96253|\n",
      "|       Ida|  Gilbert|   {1.7455, 81.5158}|    Louisiana|777-95-8206|65696|\n",
      "|       Joe|      Fox| {-57.2874, 25.6431}|      Montana|754-89-1224|18754|\n",
      "|       Lee|  Stevens|{-15.2751, -167.8...|     Oklahoma|029-32-9894|45832|\n",
      "|       Sue|  Hawkins| {33.9588, -75.0505}|     Michigan|661-10-1080|77189|\n",
      "|       Sue|     Hill| {23.1992, -69.0122}| Pennsylvania|383-85-2222|69322|\n",
      "|       Joe|  Johnson|  {-10.9211, 36.068}|       Nevada|782-93-1718|34881|\n",
      "|       Sue|    Ortiz|{39.5834, -143.9266}|       Nevada|138-02-5642|48403|\n",
      "|       Max|  Alvarez| {46.1686, -127.487}|Massachusetts|669-92-4046|11812|\n",
      "|       Eva| Crawford|{-31.2772, 119.7725}|      Wyoming|952-02-3738|31019|\n",
      "|       Mia|   Morgan| {12.5617, -32.9626}|      Georgia|293-98-2078|64384|\n",
      "|       Jon|  Coleman|  {12.244, 111.4276}|West Virginia|206-38-4424|48542|\n",
      "|       Guy|    James| {-34.1869, 87.7498}| North Dakota|386-76-3027|90178|\n",
      "+----------+---------+--------------------+-------------+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df.filter(F.length(json_df.first_name) < 4).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "28be4d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|last_name|count|\n",
      "+---------+-----+\n",
      "| Harrison|   14|\n",
      "|   Porter|   17|\n",
      "|    Scott|   23|\n",
      "|Robertson|   20|\n",
      "|   Wilson|   17|\n",
      "|  Griffin|   15|\n",
      "|    Lucas|   22|\n",
      "|   Castro|   13|\n",
      "|     Pena|   13|\n",
      "|     Boyd|   25|\n",
      "|    Jones|   18|\n",
      "|   Graham|   18|\n",
      "|  Herrera|   17|\n",
      "| Crawford|   24|\n",
      "|     Lowe|   11|\n",
      "|  Sanchez|   13|\n",
      "|Gutierrez|   15|\n",
      "|    Garza|   18|\n",
      "|    James|   15|\n",
      "|     Soto|   13|\n",
      "+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df.groupby(\"first_name\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "9a747c02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Column',\n",
       " 'DataFrame',\n",
       " 'DataType',\n",
       " 'PandasUDFType',\n",
       " 'PythonEvalType',\n",
       " 'SparkContext',\n",
       " 'StringType',\n",
       " 'UserDefinedFunction',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_create_column_from_literal',\n",
       " '_create_lambda',\n",
       " '_create_udf',\n",
       " '_get_get_jvm_function',\n",
       " '_get_lambda_parameters',\n",
       " '_invoke_binary_math_function',\n",
       " '_invoke_function',\n",
       " '_invoke_function_over_column',\n",
       " '_invoke_higher_order_function',\n",
       " '_options_to_str',\n",
       " '_test',\n",
       " '_to_java_column',\n",
       " '_to_seq',\n",
       " '_unresolved_named_lambda_variable',\n",
       " 'abs',\n",
       " 'acos',\n",
       " 'acosh',\n",
       " 'add_months',\n",
       " 'aggregate',\n",
       " 'approxCountDistinct',\n",
       " 'approx_count_distinct',\n",
       " 'array',\n",
       " 'array_contains',\n",
       " 'array_distinct',\n",
       " 'array_except',\n",
       " 'array_intersect',\n",
       " 'array_join',\n",
       " 'array_max',\n",
       " 'array_min',\n",
       " 'array_position',\n",
       " 'array_remove',\n",
       " 'array_repeat',\n",
       " 'array_sort',\n",
       " 'array_union',\n",
       " 'arrays_overlap',\n",
       " 'arrays_zip',\n",
       " 'asc',\n",
       " 'asc_nulls_first',\n",
       " 'asc_nulls_last',\n",
       " 'ascii',\n",
       " 'asin',\n",
       " 'asinh',\n",
       " 'assert_true',\n",
       " 'atan',\n",
       " 'atan2',\n",
       " 'atanh',\n",
       " 'avg',\n",
       " 'base64',\n",
       " 'bin',\n",
       " 'bitwiseNOT',\n",
       " 'broadcast',\n",
       " 'bround',\n",
       " 'bucket',\n",
       " 'cbrt',\n",
       " 'ceil',\n",
       " 'coalesce',\n",
       " 'col',\n",
       " 'collect_list',\n",
       " 'collect_set',\n",
       " 'column',\n",
       " 'concat',\n",
       " 'concat_ws',\n",
       " 'conv',\n",
       " 'corr',\n",
       " 'cos',\n",
       " 'cosh',\n",
       " 'count',\n",
       " 'countDistinct',\n",
       " 'covar_pop',\n",
       " 'covar_samp',\n",
       " 'crc32',\n",
       " 'create_map',\n",
       " 'cume_dist',\n",
       " 'current_date',\n",
       " 'current_timestamp',\n",
       " 'date_add',\n",
       " 'date_format',\n",
       " 'date_sub',\n",
       " 'date_trunc',\n",
       " 'datediff',\n",
       " 'dayofmonth',\n",
       " 'dayofweek',\n",
       " 'dayofyear',\n",
       " 'days',\n",
       " 'decode',\n",
       " 'degrees',\n",
       " 'dense_rank',\n",
       " 'desc',\n",
       " 'desc_nulls_first',\n",
       " 'desc_nulls_last',\n",
       " 'element_at',\n",
       " 'encode',\n",
       " 'exists',\n",
       " 'exp',\n",
       " 'explode',\n",
       " 'explode_outer',\n",
       " 'expm1',\n",
       " 'expr',\n",
       " 'factorial',\n",
       " 'filter',\n",
       " 'first',\n",
       " 'flatten',\n",
       " 'floor',\n",
       " 'forall',\n",
       " 'format_number',\n",
       " 'format_string',\n",
       " 'from_csv',\n",
       " 'from_json',\n",
       " 'from_unixtime',\n",
       " 'from_utc_timestamp',\n",
       " 'functools',\n",
       " 'get_json_object',\n",
       " 'greatest',\n",
       " 'grouping',\n",
       " 'grouping_id',\n",
       " 'hash',\n",
       " 'hex',\n",
       " 'hour',\n",
       " 'hours',\n",
       " 'hypot',\n",
       " 'initcap',\n",
       " 'input_file_name',\n",
       " 'instr',\n",
       " 'isnan',\n",
       " 'isnull',\n",
       " 'json_tuple',\n",
       " 'kurtosis',\n",
       " 'lag',\n",
       " 'last',\n",
       " 'last_day',\n",
       " 'lead',\n",
       " 'least',\n",
       " 'length',\n",
       " 'levenshtein',\n",
       " 'lit',\n",
       " 'locate',\n",
       " 'log',\n",
       " 'log10',\n",
       " 'log1p',\n",
       " 'log2',\n",
       " 'lower',\n",
       " 'lpad',\n",
       " 'ltrim',\n",
       " 'map_concat',\n",
       " 'map_entries',\n",
       " 'map_filter',\n",
       " 'map_from_arrays',\n",
       " 'map_from_entries',\n",
       " 'map_keys',\n",
       " 'map_values',\n",
       " 'map_zip_with',\n",
       " 'max',\n",
       " 'md5',\n",
       " 'mean',\n",
       " 'min',\n",
       " 'minute',\n",
       " 'monotonically_increasing_id',\n",
       " 'month',\n",
       " 'months',\n",
       " 'months_between',\n",
       " 'nanvl',\n",
       " 'next_day',\n",
       " 'nth_value',\n",
       " 'ntile',\n",
       " 'overlay',\n",
       " 'pandas_udf',\n",
       " 'percent_rank',\n",
       " 'percentile_approx',\n",
       " 'posexplode',\n",
       " 'posexplode_outer',\n",
       " 'pow',\n",
       " 'quarter',\n",
       " 'radians',\n",
       " 'raise_error',\n",
       " 'rand',\n",
       " 'randn',\n",
       " 'rank',\n",
       " 'regexp_extract',\n",
       " 'regexp_replace',\n",
       " 'repeat',\n",
       " 'reverse',\n",
       " 'rint',\n",
       " 'round',\n",
       " 'row_number',\n",
       " 'rpad',\n",
       " 'rtrim',\n",
       " 'schema_of_csv',\n",
       " 'schema_of_json',\n",
       " 'second',\n",
       " 'sequence',\n",
       " 'sha1',\n",
       " 'sha2',\n",
       " 'shiftLeft',\n",
       " 'shiftRight',\n",
       " 'shiftRightUnsigned',\n",
       " 'shuffle',\n",
       " 'signum',\n",
       " 'sin',\n",
       " 'since',\n",
       " 'sinh',\n",
       " 'size',\n",
       " 'skewness',\n",
       " 'slice',\n",
       " 'sort_array',\n",
       " 'soundex',\n",
       " 'spark_partition_id',\n",
       " 'split',\n",
       " 'sqrt',\n",
       " 'stddev',\n",
       " 'stddev_pop',\n",
       " 'stddev_samp',\n",
       " 'struct',\n",
       " 'substring',\n",
       " 'substring_index',\n",
       " 'sum',\n",
       " 'sumDistinct',\n",
       " 'sys',\n",
       " 'tan',\n",
       " 'tanh',\n",
       " 'timestamp_seconds',\n",
       " 'toDegrees',\n",
       " 'toRadians',\n",
       " 'to_csv',\n",
       " 'to_date',\n",
       " 'to_json',\n",
       " 'to_str',\n",
       " 'to_timestamp',\n",
       " 'to_utc_timestamp',\n",
       " 'transform',\n",
       " 'transform_keys',\n",
       " 'transform_values',\n",
       " 'translate',\n",
       " 'trim',\n",
       " 'trunc',\n",
       " 'udf',\n",
       " 'unbase64',\n",
       " 'unhex',\n",
       " 'unix_timestamp',\n",
       " 'upper',\n",
       " 'var_pop',\n",
       " 'var_samp',\n",
       " 'variance',\n",
       " 'warnings',\n",
       " 'weekofyear',\n",
       " 'when',\n",
       " 'window',\n",
       " 'xxhash64',\n",
       " 'year',\n",
       " 'years',\n",
       " 'zip_with']"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pysparak.sql import functions as F\n",
    "\n",
    "dir(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "7edee9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|first_name|First Name Counts|\n",
      "+----------+-----------------+\n",
      "|     Tyler|                6|\n",
      "|  Samantha|               10|\n",
      "|    Aubrey|                4|\n",
      "|   Carolyn|               11|\n",
      "|      Chad|                9|\n",
      "|   Shannon|                8|\n",
      "|     Shawn|                5|\n",
      "|       Sue|               11|\n",
      "|     Scott|                6|\n",
      "|     Ruben|                9|\n",
      "|     Flenn|                5|\n",
      "|  Rosemary|                3|\n",
      "|     Grace|                7|\n",
      "|     Lucas|                8|\n",
      "|     Keith|               12|\n",
      "|    Gerald|               10|\n",
      "|       Jar|                6|\n",
      "|     Edwin|                6|\n",
      "|     Soham|                5|\n",
      "|  Savannah|                7|\n",
      "+----------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df.groupby(\"first_name\").agg(F.count(\"first_name\").alias('First Name Counts')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0b6967",
   "metadata": {},
   "source": [
    "### More on Optimization\n",
    "\n",
    "1. Caching the data\n",
    "  * Caches contents of a table or output of a query\n",
    "  * Cached tables can occupy substantially less RAM that the version on disk\n",
    "    * Tungsten compression\n",
    "  * Can implement Lazy Caching to cache as needed\n",
    "    * UNCACHE to leverage space for caching other Data Frames\n",
    "2. Changing the number of data frame partition\n",
    "  * Some operations are self-contained to a partition: Narrow operation\n",
    "    * example, `COUNT` can operate on a single partition\n",
    "  * Some operations require data from other partitions: Wide operation\n",
    "    * example, `GROUPBY ` requires transferring a partition to another node so that data two partitions are grouped\n",
    "  * If the number of \"resulting\" partitions is large, the bulk of the operation time will be spent on data transfer \n",
    "\n",
    "* Only scratches the surface of performance tuning:\n",
    "  * See here for more: https://spark.apache.org/docs/latest/sql-performance-tuning.html     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1f57693e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 jovyan users 798M Sep 22 20:54 ./data/random_user_dicts_larger.json\n"
     ]
    }
   ],
   "source": [
    "!ls -l --block-size=M ./data/random_user_dicts_larger.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ad228b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500000\n"
     ]
    }
   ],
   "source": [
    "json_df_large  = session.read.option(\"inferSchema\", \"true\").json('./data/random_user_dicts_large.json')\n",
    "print(json_df_large.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0701a49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.catalog.dropTempView(\"users_large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7f91cada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "* HashAggregate (4)\n",
      "+- Exchange (3)\n",
      "   +- * HashAggregate (2)\n",
      "      +- Scan json  (1)\n",
      "\n",
      "\n",
      "(1) Scan json \n",
      "Output [1]: [first_name#1325]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/home/jovyan/work/data/random_user_dicts_large.json]\n",
      "ReadSchema: struct<first_name:string>\n",
      "\n",
      "(2) HashAggregate [codegen id : 1]\n",
      "Input [1]: [first_name#1325]\n",
      "Keys [1]: [first_name#1325]\n",
      "Functions [1]: [partial_count(1)]\n",
      "Aggregate Attributes [1]: [count#1352L]\n",
      "Results [2]: [first_name#1325, count#1353L]\n",
      "\n",
      "(3) Exchange\n",
      "Input [2]: [first_name#1325, count#1353L]\n",
      "Arguments: hashpartitioning(first_name#1325, 200), ENSURE_REQUIREMENTS, [id=#813]\n",
      "\n",
      "(4) HashAggregate [codegen id : 2]\n",
      "Input [2]: [first_name#1325, count#1353L]\n",
      "Keys [1]: [first_name#1325]\n",
      "Functions [1]: [count(1)]\n",
      "Aggregate Attributes [1]: [count(1)#1348L]\n",
      "Results [2]: [first_name#1325, count(1)#1348L AS count(1)#1349L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df_large.createTempView(\"users_large\")\n",
    "#session.catalog.dropTempView(\"users_large\")\n",
    "\n",
    "temp_df = session.sql(\"\"\"\n",
    "SELECT first_name, COUNT(*)\n",
    "FROM users_large\n",
    "GROUP BY first_name; \n",
    "\"\"\")\n",
    "\n",
    "temp_df.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8862a253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_df_large.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "80e060b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9167c67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.59 s ± 183 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "temp_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5dccb00d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df_2 = temp_df.coalesce(3)\n",
    "temp_df_2.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b53db5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "778 ms ± 111 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "temp_df_2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f571bdb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 175:========================================>                (5 + 2) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000000\n",
      "CPU times: user 9.63 ms, sys: 7.98 ms, total: 17.6 ms\n",
      "Wall time: 7.19 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "json_df_larger  = session.read.option(\"inferSchema\", \"true\").json('./data/random_user_dicts_larger.json')\n",
    "print(json_df_larger.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "67f657f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 177:========================================>                (5 + 2) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.82 ms, sys: 7.32 ms, total: 11.1 ms\n",
      "Wall time: 4.93 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# inferring schema can be a computationally expensive operation on large data\n",
    "json_df_larger  = session.read.json('./data/random_user_dicts_larger.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7ada925d",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.catalog.dropTempView(\"users_larger\")\n",
    "json_df_larger.createTempView(\"users_larger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "05caf464",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 192:========================================>                (5 + 2) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.48 s ± 68.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "session.sql(\"\"\"\n",
    "SELECT COUNT(*) FROM users_larger\n",
    "\"\"\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "46278a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 196:========================================>                (5 + 2) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.3 ms, sys: 1.12 ms, total: 11.4 ms\n",
      "Wall time: 2.64 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(count(1)=5000000)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "session.sql(\"\"\"\n",
    "SELECT COUNT(*) FROM users_larger\n",
    "\"\"\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "53834ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 198:========================================>                (5 + 2) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.3 ms, sys: 5.3 ms, total: 15.6 ms\n",
      "Wall time: 17.2 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "session.sql(\"\"\"\n",
    "CACHE TABLE users_larger\n",
    "\"\"\").collect()\n",
    "\n",
    "### Should be viewable on SparkUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b8949684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73.5 ms ± 3.86 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "session.sql(\"\"\"\n",
    "SELECT COUNT(*) FROM users_larger\n",
    "\"\"\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7041992d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sql(\"\"\"\n",
    "UNCACHE TABLE users_larger\n",
    "\"\"\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ccc61d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 362:========================================>                (5 + 2) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.73 ms, sys: 7.77 ms, total: 10.5 ms\n",
      "Wall time: 2.64 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(count(1)=5000000)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "session.sql(\"\"\"\n",
    "SELECT COUNT(*) FROM users_larger\n",
    "\"\"\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed24466",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
