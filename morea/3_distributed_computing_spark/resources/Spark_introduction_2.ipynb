{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26d28d55-2c38-4bd3-a1e8-f832ca45bbf4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We start by importing SparkContext and creating a context\n",
    "# which we will use in class\n",
    "\n",
    "# from pyspark import SparkContext\n",
    "# sc = SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9b0e668-69d8-4f64-809c-92c2b1c106bc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Core RDD Functions in Spark\n",
    "* Beyond basic functions like `map`, `flatmap`, `filter`, and `reduce`, Spark offers a variety of other essential RDD methods (both transformations and actions).\n",
    "* These functions enable easier and more efficient programming in the map-reduce paradigm.\n",
    "* Some transformative examples include:\n",
    "    * `distinct`: Yields an RDD with the unique elements from the source RDD.\n",
    "    * `union`: Combines two RDDs.\n",
    "    * `intersection`: Identifies common elements between two RDDs.\n",
    "    * `foreach`: Applies a lambda function to each RDD element without returning any value.\n",
    "      * Similar to map but without returning results.\n",
    "    * `cartesian`: Generates the cartesian product of the given RDD.\n",
    "    * And many more...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93475416-fdbb-4e9c-9d70-9ec0378f584d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Core RDD Functions in Spark - Cont'd\n",
    "\n",
    "* Spark also provides a range of actions for RDD, such as:\n",
    "    * `count`: Gives the total number of RDD elements.\n",
    "    * `sum`: Calculates the total sum of RDD elements (requires numeric data).\n",
    "    * `mean`: Determines the average of RDD elements (requires numeric data).\n",
    "    * `stats`: Returns comprehensive statistics, including count, mean, stdev, max, and min.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2c3396f-f551-4793-bdcb-e7ced437a5ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[3]: ['B', 'C', 'A', 'D']"
     ]
    }
   ],
   "source": [
    "# Distinct example \n",
    "dataset_1 = sc.parallelize([\"A\", \"B\", \"C\", \"A\", \"C\", \"D\"])\n",
    "dataset_1.distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04c14459-4546-454c-9f1a-0214ada41af2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[4]: ['__add__',\n '__class__',\n '__class_getitem__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getnewargs__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__orig_bases__',\n '__parameters__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__slots__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n '_computeFractionForSampleSize',\n '_defaultReducePartitions',\n '_id',\n '_is_barrier',\n '_is_protocol',\n '_jrdd',\n '_jrdd_deserializer',\n '_memory_limit',\n '_pickled',\n '_reserialize',\n '_to_java_object_rdd',\n 'aggregate',\n 'aggregateByKey',\n 'barrier',\n 'cache',\n 'cartesian',\n 'checkpoint',\n 'cleanShuffleDependencies',\n 'coalesce',\n 'cogroup',\n 'collect',\n 'collectAsMap',\n 'collectWithJobGroup',\n 'combineByKey',\n 'context',\n 'count',\n 'countApprox',\n 'countApproxDistinct',\n 'countByKey',\n 'countByValue',\n 'ctx',\n 'distinct',\n 'filter',\n 'first',\n 'flatMap',\n 'flatMapValues',\n 'fold',\n 'foldByKey',\n 'foreach',\n 'foreachPartition',\n 'fullOuterJoin',\n 'getCheckpointFile',\n 'getNumPartitions',\n 'getResourceProfile',\n 'getStorageLevel',\n 'glom',\n 'groupBy',\n 'groupByKey',\n 'groupWith',\n 'has_resource_profile',\n 'histogram',\n 'id',\n 'intersection',\n 'isCheckpointed',\n 'isEmpty',\n 'isLocallyCheckpointed',\n 'is_cached',\n 'is_checkpointed',\n 'join',\n 'keyBy',\n 'keys',\n 'leftOuterJoin',\n 'localCheckpoint',\n 'lookup',\n 'map',\n 'mapPartitions',\n 'mapPartitionsWithIndex',\n 'mapPartitionsWithSplit',\n 'mapValues',\n 'max',\n 'mean',\n 'meanApprox',\n 'min',\n 'name',\n 'partitionBy',\n 'partitioner',\n 'persist',\n 'pipe',\n 'randomSplit',\n 'reduce',\n 'reduceByKey',\n 'reduceByKeyLocally',\n 'repartition',\n 'repartitionAndSortWithinPartitions',\n 'rightOuterJoin',\n 'sample',\n 'sampleByKey',\n 'sampleStdev',\n 'sampleVariance',\n 'saveAsHadoopDataset',\n 'saveAsHadoopFile',\n 'saveAsNewAPIHadoopDataset',\n 'saveAsNewAPIHadoopFile',\n 'saveAsPickleFile',\n 'saveAsSequenceFile',\n 'saveAsTextFile',\n 'setName',\n 'sortBy',\n 'sortByKey',\n 'stats',\n 'stdev',\n 'subtract',\n 'subtractByKey',\n 'sum',\n 'sumApprox',\n 'take',\n 'takeOrdered',\n 'takeSample',\n 'toDF',\n 'toDebugString',\n 'toLocalIterator',\n 'top',\n 'treeAggregate',\n 'treeReduce',\n 'union',\n 'unpersist',\n 'values',\n 'variance',\n 'withResources',\n 'zip',\n 'zipWithIndex',\n 'zipWithUniqueId']"
     ]
    }
   ],
   "source": [
    "dir(dataset_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b12f712e-91fc-4fd4-b849-0a5e32506ea5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[5]: 8"
     ]
    }
   ],
   "source": [
    "dataset_1.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3885f8ac-4762-4081-a6e6-2f330fae15bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[6]: ['A', 'B', 'C', 'D', 'E', 'F']"
     ]
    }
   ],
   "source": [
    "# Union example\n",
    "\n",
    "dataset_1 = sc.parallelize([\"A\", \"B\", \"C\"])\n",
    "dataset_2 = sc.parallelize([\"D\", \"E\", \"F\"])\n",
    "\n",
    "both_datasets = dataset_1.union(dataset_2)\n",
    "\n",
    "both_datasets.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e948466-9b45-420f-84c3-586505add26b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[7]: ['B', 'C']"
     ]
    }
   ],
   "source": [
    "# intersection example\n",
    "dataset_1 = sc.parallelize([\"A\", \"B\", \"C\"])\n",
    "dataset_2 = sc.parallelize([\"N\", \"B\", \"C\", \"M\"])\n",
    "dataset_1.intersection(dataset_2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b323fa7b-1795-4b67-bda6-afc2bf5fea68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[8]: ['A', 'B', 'C']"
     ]
    }
   ],
   "source": [
    "dataset_1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5da70402-83ff-47e9-a777-2a9c7ce4ae1d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# foreach example: do you think this should work?\n",
    "out = dataset_1.foreach(lambda x: print(x))\n",
    "out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d228f25f-665c-4f57-b187-7943a6a6e52b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[10]: NoneType"
     ]
    }
   ],
   "source": [
    "type(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc4d6363-c1f7-4b72-9f1b-7bcb6df5ad60",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# foreach example.\n",
    "# Do you thin the following will work?\n",
    "dataset_1.foreach(lambda x: print(x + \"-suffix\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22db1795-ab8d-44ce-bd51-ba8ff61e1372",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[13]: ['A-suffix', 'B-suffix', 'C-suffix']"
     ]
    }
   ],
   "source": [
    "modified_rdd = dataset_1.map(lambda x: x + \"-suffix\")    \n",
    "modified_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6877b470-ce71-4e8c-8512-c085817d138d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[14]: [('A', 'D'), ('A', 'E'), ('A', 'F'), ('B', 'D'), ('B', 'E'), ('B', 'F')]"
     ]
    }
   ],
   "source": [
    "# cartesian example\n",
    "dataset_1 = sc.parallelize([\"A\", \"B\"])\n",
    "\n",
    "dataset_2 = sc.parallelize([\"D\", \"E\", \"F\"])\n",
    "\n",
    "dataset_1.cartesian(dataset_2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4a68269-0074-4a31-9815-6ab0a793648b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[15]: (21, 3.5)"
     ]
    }
   ],
   "source": [
    "# sum, mean axamples\n",
    "\n",
    "dataset_1 = sc.parallelize([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "(dataset_1.sum(), dataset_1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f748a67-6a35-4c2f-b84a-8a4644991861",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[16]: (count: 6, mean: 3.5, stdev: 1.707825127659933, max: 6.0, min: 1.0)"
     ]
    }
   ],
   "source": [
    "# stats example\n",
    "\n",
    "dataset_1.stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f6105e5-5bad-475e-be34-cb4bc32a72bc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Essential RDD Functions for Tuples\n",
    "\n",
    "* Remember, key-value pairs (tuples) are a core component in the Map Reduce paradigm.\n",
    "\n",
    "  * e.g. `[(\"THE\", 12), (\"HI\", 2), (\"COURSE\", 2), (\"STUDENTS\", 3), ... ]`\n",
    "  * Spark offers a collection of methods tailored for these tuples, both as transformations and actions.\n",
    "\n",
    "* Transformations on Tuples:\n",
    "    * `sortByKey`: yields a new RDD ordered by its keys.\n",
    "    * `reduceByKey`: applies a function `f` to combine values by key, producing a new RDD.\n",
    "    * `groupByKey`: Creates a new RDD where values are assembled under their respective keys.\n",
    "    * `join`: Generates a new RDD by pairing values with matching keys from two datasets.\n",
    "        * Variants include: `leftOuter`, `rightOuter`, `fullOuter` joins.\n",
    "\n",
    "* Actions for Tuples:\n",
    "  * General actions like count are adaptable to any data type.\n",
    "  * To perform arithmetic actions on a tuple RDD, you can first transform it using map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "854ab657-79ae-4060-9542-4771ba8890ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[17]: [('A', 2), ('B', 3), ('C', 12), ('D', '2')]"
     ]
    }
   ],
   "source": [
    "# sortByKey example\n",
    "\n",
    "data_1  = sc.parallelize([(\"C\", 12), (\"D\", 2), (\"A\", 2), (\"B\", 3)])\n",
    "data_1.sortByKey().collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33760e11-66b5-40c2-a6ae-afc8a0ee2c28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[18]: [('B', 3), ('C', 5), ('A', 14)]"
     ]
    }
   ],
   "source": [
    "# reduceByKey example\n",
    "\n",
    "data_1  = sc.parallelize([(\"A\", 12), (\"A\", 2), (\"C\", 2), (\"B\", 3), (\"C\", 3)])\n",
    "data_1.reduceByKey(lambda x,y: x+y).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e73a08f-6584-43e5-aa82-7652e48cd7dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[19]: [('B', <pyspark.resultiterable.ResultIterable at 0x7f8a74b93160>),\n ('C', <pyspark.resultiterable.ResultIterable at 0x7f8a74b93ee0>),\n ('A', <pyspark.resultiterable.ResultIterable at 0x7f8a7498f2e0>)]"
     ]
    }
   ],
   "source": [
    "# groupByKey example\n",
    "\n",
    "data_1  = sc.parallelize([(\"A\", 12), (\"A\", 2), (\"C\", 2), (\"B\", 3), (\"C\", 3), (\"A\", 5)])\n",
    "grouped_data = data_1.groupByKey().collect()\n",
    "grouped_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c343e454-a954-4863-bab3-84502a720b2f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\t[3]\nC\t[2, 3]\nA\t[12, 2, 5]\n"
     ]
    }
   ],
   "source": [
    "for key, val in grouped_data:\n",
    "    print(f\"{key}\\t{list(val)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c72a93ec-61c1-434f-8ee2-9bf7f6c36bbe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[24]: [('B', (4, 5)), ('C', (6, 7)), ('A', (1, 2)), ('A', (3, 2))]"
     ]
    }
   ],
   "source": [
    "# join exmaple\n",
    "\n",
    "data_1  = sc.parallelize([(\"A\", 1), (\"A\", 3), (\"B\", 4), (\"C\", 6), (\"D\", 11)          ])\n",
    "data_2  = sc.parallelize([(\"A\", 2),           (\"B\", 5), (\"C\", 7),           (\"E\", 11)])\n",
    "\n",
    "data_1.join(data_2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4f76ad0-ab91-43aa-b41d-a1fb2ac31851",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[30]: [('B', (4, 5)), ('D', (3, None)), ('C', (6, 7)), ('A', (1, 2))]"
     ]
    }
   ],
   "source": [
    "# leftOuterJoin exmaple\n",
    "\n",
    "data_1  = sc.parallelize([(\"A\", 1), (\"D\", 3), (\"B\", 4), (\"C\", 6), ])\n",
    "data_2  = sc.parallelize([(\"A\", 2),           (\"B\", 5), (\"C\", 7), ])\n",
    "\n",
    "data_1.leftOuterJoin(data_2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c228b00f-9670-486b-a532-525f9905744d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('B', (4, 5)), ('C', (6, 7)), ('A', (1, 2))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rightOuterJoin exmaple\n",
    "\n",
    "data_1  = sc.parallelize([(\"A\", 1), (\"D\", 3), (\"B\", 4), (\"C\", 6), ])\n",
    "data_2  = sc.parallelize([(\"A\", 2),           (\"B\", 5), (\"C\", 7), ])\n",
    "\n",
    "data_1.rightOuterJoin(data_2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7d28264-a87a-4fdb-b225-899cdd0131f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[36]: 14"
     ]
    }
   ],
   "source": [
    "# apply non tuple actions  by first using map\n",
    "# extract the values\n",
    "\n",
    "data_1  = sc.parallelize([(\"A\", 1), (\"A\", 3), (\"B\", 4), (\"C\", 6)])\n",
    "data_1.map(lambda x: x[1]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a177eca1-5752-4539-9d85-27e509a2de45",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Conclusion\n",
    "* An RDD (Resilient Distributed Dataset) is an immutable distributed collection of various data objects.\n",
    "  * This can include lines, tuples, JSON objects, and more.\n",
    "\n",
    "* RDDs are distributed across multiple nodes, enabling parallel operations through a low-level API.\n",
    "  * Any transformation on an RDD yields a new RDD.\n",
    "  * To explore the myriad of methods available for RDDs, use the dir function.\n",
    "\n",
    "* Importantly, RDDs don't enforce a specific data structure.\n",
    "  * For instance, you can have:\n",
    "```python\n",
    "sc.parallelize([(\"A\", 1), {\"First\": \"John\", \"Salary\": 125_000}, (\"B\", 4), (\"C\", 6), ])\n",
    "```\n",
    "  * This is not a desired situation when working with structured data.\n",
    "    * Cannot run queries on all instances (e.g., `select Salary from my_rdd`)\n",
    "\n",
    "* Spark also offers a data structure that enforces data validation and structure.\n",
    "   * This aids in optimizing data operations more effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a913ea7b-1967-44f7-96c2-47e59be1d444",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Dive Into Spark DataFrame\n",
    "* Spark DataFrames in pySpark represent immutable, distributed collections of data neatly organized into named columns.\n",
    "  * They can be visualized as tables in relational databases or analogous to Pandas's DataFrame.\n",
    "  * Facilitates querying either via SQL or Python-style syntax.\n",
    "\n",
    "* Example using SQL:\n",
    "\n",
    "```SQL\n",
    "session.sql(\"SELECT * from users WHERE age < 21\")\n",
    "```\n",
    "\n",
    "* Example using Python-style:\n",
    "\n",
    "```\n",
    "users.filter(users.age < 21)\n",
    "```\n",
    "\n",
    "* Given their structured nature, DataFrames enable enhanced optimizations internally.\n",
    "  * They can be created from various sources, including:\n",
    "    * Structured data files (json, csv, etc.)\n",
    "    * Parquet Files\n",
    "    * External databases\n",
    "    * Pre-existing RDDs\n",
    "* While RDDs are seen as collections of objects, DataFrames can be understood as collections of rows (instances).\n",
    "  * Similar to Pandas DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e5d05d2-3700-4d05-b3d6-7249ba315933",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### DataFrame Operations in Spark\n",
    "\n",
    "* Spark offers a variety of high-level functions tailored for DataFrames.\n",
    "  * Rooted in the map-reduce model, these functions address common tasks efficiently.\n",
    "  * While they serve as shortcuts, remember they're underpinned by core functions: map, flatmap, filter, and reduce.\n",
    "* While SparkContext is essential for RDDs, DataFrames rely on SparkSession.\n",
    "  * Tasks such as creating, registering, and executing SQL queries necessitate SparkSession.\n",
    "* Multiple methods are available to instantiate a DataFrame:\n",
    "  * From a csv: Each row is an object.\n",
    "  * From a json: Every record is as an object.\n",
    "    * Ensure every line contains a distinct, valid JSON object.\n",
    "      * This format used in assignment 1.\n",
    "    * From a text file: Here too, each row becomes an object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "523aa088-460b-40e5-a097-138f6650e88a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Understanding Schemas\n",
    "* Schemas outline the data type structure of your fields.\n",
    "* They play an important role in Spark's optimization processes.\n",
    "* Schemas are essential in achieving the right in-memory compression.\n",
    "  * Post-compression, the data size in memory might be more compact than its uncompressed counterpart on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d25c6f68-3b3b-41bb-9eb9-16172ef1b452",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark import SparkContext\n",
    "# sc = SparkContext()\n",
    "from pyspark.sql import SparkSession\n",
    "session = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed4311ea-5f45-42c2-96a1-5668ee79ce95",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14579\nOut[60]: Row(value='The Project Gutenberg eBook of Pride and Prejudice, by Jane Austen')"
     ]
    }
   ],
   "source": [
    "text_df  = session.read.text('dbfs:/FileStore/pride_and_prejudice.txt')\n",
    "print(text_df.count())\n",
    "text_df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "112e5725-2262-4e5c-8715-52a81adeda3e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[61]: DataFrame[value: string]"
     ]
    }
   ],
   "source": [
    "text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09c88eea-8808-464e-9185-9bc5ad94a5e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aeb76197-b3cc-4d69-ba4f-ae69ce4ba43f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450017\nCPU times: user 15.1 ms, sys: 3.34 ms, total: 18.4 ms\nWall time: 10.2 s\nOut[24]: [Row(_c0='0', DayOfWeek='2', UniqueCarrier='AA', FlightNum='494', Origin='CLT', Dest='PHX', CRSDepTime='1619', DepTime='1616.0', TaxiOut='17.0', WheelsOff='1633.0', WheelsOn='1837.0', TaxiIn='5.0', CRSArrTime='1856', ArrTime='1842.0', Cancelled='0.0', CancellationCode=None, Distance='1773.0', CarrierDelay=None, WeatherDelay=None, NASDelay=None, SecurityDelay=None, LateAircraftDelay=None)]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "csv_df = session.read.csv(\"dbfs:/FileStore/flight_info.csv\", header=True)\n",
    "print(csv_df.count())\n",
    "csv_df.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a95d4cf3-3ff5-494b-bb54-cdde66f1df43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[25]: StructType([StructField('_c0', StringType(), True), StructField('DayOfWeek', StringType(), True), StructField('UniqueCarrier', StringType(), True), StructField('FlightNum', StringType(), True), StructField('Origin', StringType(), True), StructField('Dest', StringType(), True), StructField('CRSDepTime', StringType(), True), StructField('DepTime', StringType(), True), StructField('TaxiOut', StringType(), True), StructField('WheelsOff', StringType(), True), StructField('WheelsOn', StringType(), True), StructField('TaxiIn', StringType(), True), StructField('CRSArrTime', StringType(), True), StructField('ArrTime', StringType(), True), StructField('Cancelled', StringType(), True), StructField('CancellationCode', StringType(), True), StructField('Distance', StringType(), True), StructField('CarrierDelay', StringType(), True), StructField('WeatherDelay', StringType(), True), StructField('NASDelay', StringType(), True), StructField('SecurityDelay', StringType(), True), StructField('LateAircraftDelay', StringType(), True)])"
     ]
    }
   ],
   "source": [
    "csv_df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f26b705f-c4b5-4e91-88c0-5dbd0b341f53",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "* In Spark's StructField, the third parameter indicates whether the field is nullable or not.\n",
    "  * I.e., whether the field can be null or not.\n",
    "\n",
    "```python\n",
    "StructType(\n",
    "\tList(StructField(year,StringType,true),\n",
    "\t\tStructField(month,StringType,true),\n",
    "\t\tStructField(day,StringType,true),\n",
    "\t\tStructField(dep_time,StringType,true),\n",
    "\t\tStructField(dep_delay,StringType,true),\n",
    "\t\tStructField(arr_time,StringType,true),\n",
    "\t\tStructField(arr_delay,StringType,true),\n",
    "\t\tStructField(carrier,StringType,true),\n",
    "\t\tStructField(tailnum,StringType,true),\n",
    "\t\tStructField(flight,StringType,true),\n",
    "\t\tStructField(origin,StringType,true),\n",
    "\t\tStructField(dest,StringType,true),\n",
    "\t\tStructField(air_time,StringType,true),\n",
    "\t\tStructField(distance,StringType,true),\n",
    "\t\tStructField(hour,StringType,true),\n",
    "\t\tStructField(minute,StringType,true)\n",
    "\t)\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4248de0-2b2b-4ac0-9b42-196915882650",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450017\nCPU times: user 14.6 ms, sys: 8.35 ms, total: 23 ms\nWall time: 14.2 s\nOut[26]: [Row(_c0=0, DayOfWeek=2, UniqueCarrier='AA', FlightNum=494, Origin='CLT', Dest='PHX', CRSDepTime=1619, DepTime=1616.0, TaxiOut=17.0, WheelsOff=1633.0, WheelsOn=1837.0, TaxiIn=5.0, CRSArrTime=1856, ArrTime=1842.0, Cancelled=0.0, CancellationCode=None, Distance=1773.0, CarrierDelay=None, WeatherDelay=None, NASDelay=None, SecurityDelay=None, LateAircraftDelay=None),\n Row(_c0=1, DayOfWeek=3, UniqueCarrier='AA', FlightNum=494, Origin='CLT', Dest='PHX', CRSDepTime=1619, DepTime=1614.0, TaxiOut=13.0, WheelsOff=1627.0, WheelsOn=1815.0, TaxiIn=6.0, CRSArrTime=1856, ArrTime=1821.0, Cancelled=0.0, CancellationCode=None, Distance=1773.0, CarrierDelay=None, WeatherDelay=None, NASDelay=None, SecurityDelay=None, LateAircraftDelay=None)]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "csv_df = session.read.options(inferSchema = True).csv(\"dbfs:/FileStore/flight_info.csv\", header=True)\n",
    "\n",
    "print(csv_df.count())\n",
    "\n",
    "csv_df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "915f522c-115a-4fd7-b940-3e802ea10168",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[27]: StructType([StructField('_c0', IntegerType(), True), StructField('DayOfWeek', IntegerType(), True), StructField('UniqueCarrier', StringType(), True), StructField('FlightNum', IntegerType(), True), StructField('Origin', StringType(), True), StructField('Dest', StringType(), True), StructField('CRSDepTime', IntegerType(), True), StructField('DepTime', DoubleType(), True), StructField('TaxiOut', DoubleType(), True), StructField('WheelsOff', DoubleType(), True), StructField('WheelsOn', DoubleType(), True), StructField('TaxiIn', DoubleType(), True), StructField('CRSArrTime', IntegerType(), True), StructField('ArrTime', DoubleType(), True), StructField('Cancelled', DoubleType(), True), StructField('CancellationCode', StringType(), True), StructField('Distance', DoubleType(), True), StructField('CarrierDelay', DoubleType(), True), StructField('WeatherDelay', DoubleType(), True), StructField('NASDelay', DoubleType(), True), StructField('SecurityDelay', DoubleType(), True), StructField('LateAircraftDelay', DoubleType(), True)])"
     ]
    }
   ],
   "source": [
    "csv_df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7c48dd0-ff4f-46b0-a426-eb0831d59fca",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "```\n",
    "StructType(\n",
    "\tList(\n",
    "\t\tStructField(year,IntegerType,true),\n",
    "\t\tStructField(month,IntegerType,true),\n",
    "\t\tStructField(day,IntegerType,true),\n",
    "\t\tStructField(dep_time,IntegerType,true),\n",
    "\t\tStructField(dep_delay,IntegerType,true),\n",
    "\t\tStructField(arr_time,IntegerType,true),\n",
    "\t\tStructField(arr_delay,IntegerType,true),\n",
    "\t\tStructField(carrier,StringType,true),\n",
    "\t\tStructField(tailnum,StringType,true),\n",
    "\t\tStructField(flight,IntegerType,true),\n",
    "\t\tStructField(origin,StringType,true),\n",
    "\t\tStructField(dest,StringType,true),\n",
    "\t\tStructField(air_time,IntegerType,true),\n",
    "\t\tStructField(distance,IntegerType,true),\n",
    "\t\tStructField(hour,IntegerType,true),\n",
    "\t\tStructField(minute,IntegerType,true)\n",
    "\t)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35a807f1-3f50-468c-8496-f92b86085704",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "json_df  = session.read.json('dbfs:/FileStore/random_user_dicts.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35eaaf0c-93d2-4b7a-b948-1c3249258301",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------------+--------------+-----------+-----+\n|first_name| last_name|            lat_long|         state|    user_id|  zip|\n+----------+----------+--------------------+--------------+-----------+-----+\n|  Benjamin|   Hawkins|{-88.1663, -70.6146}|       Vermont|028-73-9282|10447|\n|     Flenn|   Chapman|   {24.893, 60.9322}|       Montana|852-16-2595|96244|\n|     Stacy|     Owens| {-86.3024, 18.6558}|      Virginia|439-88-6137|46616|\n|  Michelle|     Meyer|  {64.4687, 12.1853}|      Missouri|771-99-0447|19031|\n|    Tamara|     Young|{-16.5984, -79.6928}| West Virginia|824-38-7655|46919|\n| Nathaniel|      Wood|  {35.9258, 30.4532}|      Arkansas|146-06-1162|71250|\n|      Dale|Cunningham|  {67.421, -24.9799}|         Maine|654-80-5980|67678|\n|     Pedro|    Weaver| {37.2385, 155.5874}|North Carolina|163-01-4459|98164|\n|   Stanley|     Olson|  {56.325, -74.2028}|       Indiana|939-30-2636|39194|\n|   Willard|   Coleman|{-86.8209, 132.7801}|       Florida|572-03-6425|77132|\n|      Dora|      Hale| {-56.8422, 66.8386}|    California|727-04-1323|47140|\n|      Alan|    Rhodes|  {-4.706, 179.2765}|     Wisconsin|911-06-8944|80512|\n|      Irma|   Stanley|{-59.2304, 153.4512}|        Kansas|558-18-7781|30501|\n|      Leta|     Evans|{-82.6907, -124.7...|  Rhode Island|734-06-6644|61949|\n|     Leona|   Jenkins| {-63.3151, 72.2236}|       Florida|611-02-0993|24210|\n|    Arnold|    Tucker|{-23.3092, -117.6...| New Hampshire|120-72-4808|38387|\n| Sebastian|  Marshall|{43.1142, -128.3444}|    New Jersey|883-89-0882|87799|\n| Francisco|      Ross| {-33.5087, -0.2318}|      Oklahoma|125-36-8198|58113|\n|    Melvin|      Pena|{-67.8474, -50.1362}|       Arizona|029-44-6407|54624|\n|   Francis|      Carr| {-4.074, -155.6821}|        Nevada|915-65-8542|41626|\n+----------+----------+--------------------+--------------+-----------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "json_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3eb1207-1760-413b-b0af-3eaf1214350a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "print(json_df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21a10884-c12a-4ecd-a8ad-4095658d250c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- lat_long: struct (nullable = true)\n |    |-- latitude: double (nullable = true)\n |    |-- longitude: double (nullable = true)\n |-- state: string (nullable = true)\n |-- user_id: string (nullable = true)\n |-- zip: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "json_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ef31580-3ff9-418f-9c54-f68b4df166af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[48]: [Row(first_name='Benjamin'),\n Row(first_name='Flenn'),\n Row(first_name='Stacy'),\n Row(first_name='Michelle'),\n Row(first_name='Tamara'),\n Row(first_name='Nathaniel'),\n Row(first_name='Dale'),\n Row(first_name='Pedro'),\n Row(first_name='Stanley'),\n Row(first_name='Willard')]"
     ]
    }
   ],
   "source": [
    "json_df.select(\"first_name\").collect()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9accd0cc-8b4a-44df-99c1-4d17d8754c90",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "\n",
    "json_struct = StructType([\n",
    "    StructField(\"first_name\", StringType(), nullable=False, metadata=None),\n",
    "    StructField(\"last_name\", StringType(),  nullable=False, metadata=None),\n",
    "    StructField(\"lat_long\", \n",
    "                StructType([\n",
    "                    StructField(\"latitude\", FloatType(), metadata=None, nullable=True),\n",
    "                    StructField(\"longitude\", FloatType(), metadata=None, nullable=True)\n",
    "                ]), nullable=True, metadata=None),\n",
    "    StructField(\"state\", StringType(),  nullable=True, metadata=None),\n",
    "    StructField(\"user_id\", StringType(),  nullable=True, metadata=None),\n",
    "    StructField(\"zip\", StringType(),  nullable=True, metadata=None),    \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30e11c43-50ad-416f-8791-ad6c5f6626d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[50]: ['Any',\n 'ArrayType',\n 'AtomicType',\n 'BinaryType',\n 'BooleanType',\n 'ByteType',\n 'Callable',\n 'CharType',\n 'ClassVar',\n 'CloudPickleSerializer',\n 'DataType',\n 'DataTypeSingleton',\n 'DateConverter',\n 'DateType',\n 'DatetimeConverter',\n 'DatetimeNTZConverter',\n 'DayTimeIntervalType',\n 'DayTimeIntervalTypeConverter',\n 'DecimalType',\n 'Dict',\n 'DoubleType',\n 'FloatType',\n 'FractionalType',\n 'GatewayClient',\n 'IntegerType',\n 'IntegralType',\n 'Iterable',\n 'Iterator',\n 'JavaClass',\n 'JavaObject',\n 'List',\n 'LongType',\n 'MapType',\n 'NullType',\n 'NumericType',\n 'Optional',\n 'Row',\n 'ShortType',\n 'StringType',\n 'StructField',\n 'StructType',\n 'T',\n 'TYPE_CHECKING',\n 'TimestampNTZType',\n 'TimestampType',\n 'Tuple',\n 'Type',\n 'TypeVar',\n 'U',\n 'Union',\n 'UserDefinedType',\n 'VarcharType',\n '_FIXED_DECIMAL',\n '_INTERVAL_DAYTIME',\n '_LENGTH_CHAR',\n '_LENGTH_VARCHAR',\n '__all__',\n '__annotations__',\n '__builtins__',\n '__cached__',\n '__doc__',\n '__file__',\n '__loader__',\n '__name__',\n '__package__',\n '__spec__',\n '_acceptable_types',\n '_all_atomic_types',\n '_all_complex_types',\n '_array_signed_int_typecode_ctype_mappings',\n '_array_type_mappings',\n '_array_unsigned_int_typecode_ctype_mappings',\n '_atomic_types',\n '_complex_types',\n '_create_converter',\n '_create_row',\n '_create_row_inbound_converter',\n '_from_numpy_type',\n '_has_nulltype',\n '_infer_schema',\n '_infer_type',\n '_int_size_to_type',\n '_make_type_verifier',\n '_merge_type',\n '_need_converter',\n '_parse_datatype_json_string',\n '_parse_datatype_json_value',\n '_parse_datatype_string',\n '_test',\n '_type_mappings',\n '_typecode',\n 'array',\n 'base64',\n 'calendar',\n 'cast',\n 'ctypes',\n 'datetime',\n 'decimal',\n 'dt',\n 'json',\n 'math',\n 'overload',\n 're',\n 'register_input_converter',\n 'size',\n 'sys',\n 'time']"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "dir(pyspark.sql.types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2680f006-72b4-4228-808d-82aceafc1406",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n{'fields': [{'metadata': {},\n             'name': 'first_name',\n             'nullable': True,\n             'type': 'string'},\n            {'metadata': {},\n             'name': 'last_name',\n             'nullable': True,\n             'type': 'string'},\n            {'metadata': {},\n             'name': 'lat_long',\n             'nullable': True,\n             'type': {'fields': [{'metadata': {},\n                                  'name': 'latitude',\n                                  'nullable': True,\n                                  'type': 'float'},\n                                 {'metadata': {},\n                                  'name': 'longitude',\n                                  'nullable': True,\n                                  'type': 'float'}],\n                      'type': 'struct'}},\n            {'metadata': {},\n             'name': 'state',\n             'nullable': True,\n             'type': 'string'},\n            {'metadata': {},\n             'name': 'user_id',\n             'nullable': True,\n             'type': 'string'},\n            {'metadata': {},\n             'name': 'zip',\n             'nullable': True,\n             'type': 'string'}],\n 'type': 'struct'}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "import json\n",
    "\n",
    "json_df  = session.read.schema(json_struct).json('./data/random_user_dicts.json')\n",
    "print(json_df.count())\n",
    "new_json = json_df.schema.json()\n",
    "pprint.pprint(json.loads(new_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fe0beac-9ec6-481e-a29b-f3e11a643148",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Row(first_name='Christopher', last_name='Morgan', lat_long=Row(latitude=6.444200038909912, longitude=-78.50630187988281), state='Nebraska', user_id='895-76-0473', zip='73093')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note the lat_long field. It has its own format\n",
    "\n",
    "json_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5339d2f-1093-4b81-9bd0-c258c98d10da",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Insights into Data Formats\n",
    "\n",
    "* For stable datasets, structured data formats such as tab or comma-delimited tables (CSV/TSV) are recommended.\n",
    "  * However, data often evolves over time which may affect the structure of datasets.\n",
    "  * Semi-structured data formats like JSON are flexible to accommodate varying fields across records.\n",
    "```python\n",
    "[ \n",
    "  {\"user_id\": \"Jane1234\", \"employed\": True, \"salary\": 95000}, \n",
    "  {\"user_id\": \"John777\", \"employed\": False}, \n",
    "  ...\n",
    "]\n",
    "```\n",
    "* Traditional tabular data formats struggle with handling nested or hierarchical data structures.\n",
    "  * Complex data types like lists or nested objects are often better represented in semi-structured formats:\n",
    "```python\n",
    "[{\n",
    "    \"user_id\": \"Jane1234\", \n",
    "    \"cars\": [\"Sedan\", \"Truck\"],\n",
    "    \"children\": {\n",
    "        \"Jonah\": {\"age\": 8, \"school\": \"Noelani Elementary\"},\n",
    "        \"Mary\": {\"age\": 12, \"school\": \"Sacred Hearts\"}\n",
    "    } \n",
    "}, ...]\n",
    "```\n",
    "\n",
    "* Some considerations when using JSON:\n",
    "  * Increased disk storage might be a concern due to field name duplication across records.\n",
    "  * However, when loaded into memory, field names are typically not duplicated, saving space.\n",
    "\n",
    "* JSON format may not be natively compatible with traditional SQL querying.\n",
    "  * Temporary views or tables can be generated to facilitate SQL-like querying.\n",
    "* Apache Spark excels at converting collections of JSON objects into tabular formats for querying.\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a641f75-208e-4daa-814e-a59f124ecfe0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### SQL with Spark DataFrames\n",
    "\n",
    "* SQL capabilities are integral to Spark DataFrames.\n",
    "    * Direct SQL support is exclusive to DataFrames.\n",
    "* Familiar SQL queries are seamlessly compatible with DataFrames.\n",
    "* To execute SQL on DataFrames:\n",
    "    * First, register a temporary view.\n",
    "    * Use the `.sql()` method for querying.\n",
    "* Every query undergoes an optimization phase before execution.\n",
    "    * This employs the Catalyst SQL optimizer.\n",
    "        [Catalyst Optimizer Details](https://databricks.com/glossary/catalyst-optimizer)\n",
    "* Bear in mind, DataFrame data is immutable.\n",
    "    * To add new columns, generate a new DataFrame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "642156da-3c50-40fe-844a-d9e826cf643d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Example of query optimization\n",
    "![](https://www.dropbox.com/s/e756fxrsi36yvj4/unoptimized_optimized.png?dl=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea224c61-f847-4c18-993f-0527464199b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "json_df  = session.read.schema(json_struct).json('./data/random_user_dicts.json')\n",
    "print(json_df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38a89b2f-3a1c-4441-942a-e3aa33a091bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n|first_name|count(1)|\n+----------+--------+\n|     Tyler|       6|\n|  Samantha|      10|\n|    Aubrey|       4|\n|   Carolyn|      11|\n|      Chad|       9|\n|   Shannon|       8|\n|     Shawn|       5|\n|       Sue|      11|\n|     Scott|       6|\n|     Ruben|       9|\n|     Flenn|       5|\n|  Rosemary|       3|\n|     Grace|       7|\n|     Lucas|       8|\n|     Keith|      12|\n|    Gerald|      10|\n|       Jar|       6|\n|     Edwin|       6|\n|     Soham|       5|\n|  Savannah|       7|\n+----------+--------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "json_df.createTempView(\"users\")\n",
    "\n",
    "session.sql(\"\"\"\n",
    "SELECT first_name, COUNT(*)\n",
    "FROM users\n",
    "GROUP BY first_name; \n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9032c62c-cfe4-40e2-a461-28e03bad0988",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------------+-------------+-----------+-----+\n|first_name|last_name|            lat_long|        state|    user_id|  zip|\n+----------+---------+--------------------+-------------+-----------+-----+\n|      Evan|     Beck|  {-17.7362, -3.022}|New Hampshire|505-92-9095|87501|\n|      Evan|   Snyder| {56.7583, -64.3217}|    Minnesota|075-11-1233|74201|\n|     Sarah|     Webb|   {2.5834, -89.618}| South Dakota|247-48-1845|86063|\n|     Sarah|  Stanley|{-18.5369, -81.8778}|     Missouri|997-11-7309|68852|\n|      John|  Nichols|{-10.576, -148.6093}|      Alabama|575-68-2404|17965|\n|      John|     Reid|   {0.659, -70.5511}|  Mississippi|370-33-2662|57788|\n|      John|  Freeman|{-69.3141, -12.2351}|        Maine|621-84-6581|13221|\n|      John|    Price| {-7.5446, 109.4057}| North Dakota|323-62-2196|18403|\n|      John|    Davis|{-39.9862, -72.4857}|        Maine|211-00-2584|56206|\n|      Evan|   Torres|{-81.3332, 118.9237}|      Vermont|547-50-2905|64904|\n|      Evan|    Watts| {16.6248, -78.8383}|     Colorado|576-73-6525|41332|\n|      Evan|    Ortiz|{-76.9669, 150.0783}|  Mississippi|023-52-7382|61349|\n|      John|     Rose|  {63.1506, 82.6388}| Pennsylvania|857-84-5597|36311|\n|      John|    Mason| {89.3237, -14.4558}| Pennsylvania|639-53-5867|85588|\n|      Evan|    Reyes| {35.6993, 126.5463}|      Georgia|676-54-0295|29735|\n|      John|  Fleming|  {-88.764, -6.8801}|      Alabama|027-70-6298|36102|\n|      John|Mitchelle|{-18.6557, -68.5627}|     Michigan|177-04-6747|96502|\n|     Sarah|     King| {15.7888, 159.0693}|      Indiana|322-68-4414|16774|\n|      Evan|     Dean|{-48.4929, -93.6729}|       Hawaii|692-88-6252|81605|\n|     Sarah|   Little|  {-4.8479, 112.987}|     New York|421-82-5048|56231|\n+----------+---------+--------------------+-------------+-----------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "session.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM users\n",
    "WHERE first_name IN (\"Evan\", \"Sarah\", \"John\"); \n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60b5559e-0114-417c-9795-79cbf3061e59",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### DataFrame and Select Queries\n",
    "\n",
    "* The same functionality is available using Python\n",
    "* Many additional functions, inlcuding analytics-specific ones are available through specific library\n",
    "\n",
    "    ```from pyspark.sql import functions as F```\n",
    "    * The functions as used with a select\n",
    "  * Can use `agg` to do specific operations and rename columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8344ba02-6ebc-4ca2-89ef-7791e425a6a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------------+-------------+-----------+-----+\n|first_name|last_name|            lat_long|        state|    user_id|  zip|\n+----------+---------+--------------------+-------------+-----------+-----+\n|       Joe|  Jackson|{-62.895, -143.5974}|      Florida|664-45-7303|23155|\n|       Mia|   Chavez|  {13.112, 109.6125}|     Arkansas|309-47-7003|42409|\n|       Jon|     Cole| {71.6651, 101.2318}| Rhode Island|217-81-4486|92912|\n|       Mia|  Barrett| {-57.829, -63.2612}|New Hampshire|236-16-5120|31229|\n|       Max|   Willis|   {31.288, 52.5112}|         Utah|858-00-9946|78049|\n|       Kim|  Spencer|  {50.676, -70.7382}|        Idaho|715-58-2909|67867|\n|       Roy| Mitchell|   {49.4706, 1.6422}|   Washington|421-56-4226|62647|\n|       Eli|   Turner|{-42.0611, 144.1344}|     Delaware|744-31-7784|96253|\n|       Ida|  Gilbert|   {1.7455, 81.5158}|    Louisiana|777-95-8206|65696|\n|       Joe|      Fox| {-57.2874, 25.6431}|      Montana|754-89-1224|18754|\n|       Lee|  Stevens|{-15.2751, -167.8...|     Oklahoma|029-32-9894|45832|\n|       Sue|  Hawkins| {33.9588, -75.0505}|     Michigan|661-10-1080|77189|\n|       Sue|     Hill| {23.1992, -69.0122}| Pennsylvania|383-85-2222|69322|\n|       Joe|  Johnson|  {-10.9211, 36.068}|       Nevada|782-93-1718|34881|\n|       Sue|    Ortiz|{39.5834, -143.9266}|       Nevada|138-02-5642|48403|\n|       Max|  Alvarez| {46.1686, -127.487}|Massachusetts|669-92-4046|11812|\n|       Eva| Crawford|{-31.2772, 119.7725}|      Wyoming|952-02-3738|31019|\n|       Mia|   Morgan| {12.5617, -32.9626}|      Georgia|293-98-2078|64384|\n|       Jon|  Coleman|  {12.244, 111.4276}|West Virginia|206-38-4424|48542|\n|       Guy|    James| {-34.1869, 87.7498}| North Dakota|386-76-3027|90178|\n+----------+---------+--------------------+-------------+-----------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "json_df.filter(F.length(json_df.first_name) < 4).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3edc63b4-ece1-4b8b-834a-9750522df7df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n|last_name|count|\n+---------+-----+\n| Harrison|   14|\n|   Porter|   17|\n|    Scott|   23|\n|Robertson|   20|\n|   Wilson|   17|\n|  Griffin|   15|\n|    Lucas|   22|\n|   Castro|   13|\n|     Pena|   13|\n|     Boyd|   25|\n|    Jones|   18|\n|   Graham|   18|\n|  Herrera|   17|\n| Crawford|   24|\n|     Lowe|   11|\n|  Sanchez|   13|\n|Gutierrez|   15|\n|    Garza|   18|\n|    James|   15|\n|     Soto|   13|\n+---------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "json_df.groupby(\"first_name\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d628fcc5-3a10-4f53-86a9-7814e0ebcec4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Column',\n",
       " 'DataFrame',\n",
       " 'DataType',\n",
       " 'PandasUDFType',\n",
       " 'PythonEvalType',\n",
       " 'SparkContext',\n",
       " 'StringType',\n",
       " 'UserDefinedFunction',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_create_column_from_literal',\n",
       " '_create_lambda',\n",
       " '_create_udf',\n",
       " '_get_get_jvm_function',\n",
       " '_get_lambda_parameters',\n",
       " '_invoke_binary_math_function',\n",
       " '_invoke_function',\n",
       " '_invoke_function_over_column',\n",
       " '_invoke_higher_order_function',\n",
       " '_options_to_str',\n",
       " '_test',\n",
       " '_to_java_column',\n",
       " '_to_seq',\n",
       " '_unresolved_named_lambda_variable',\n",
       " 'abs',\n",
       " 'acos',\n",
       " 'acosh',\n",
       " 'add_months',\n",
       " 'aggregate',\n",
       " 'approxCountDistinct',\n",
       " 'approx_count_distinct',\n",
       " 'array',\n",
       " 'array_contains',\n",
       " 'array_distinct',\n",
       " 'array_except',\n",
       " 'array_intersect',\n",
       " 'array_join',\n",
       " 'array_max',\n",
       " 'array_min',\n",
       " 'array_position',\n",
       " 'array_remove',\n",
       " 'array_repeat',\n",
       " 'array_sort',\n",
       " 'array_union',\n",
       " 'arrays_overlap',\n",
       " 'arrays_zip',\n",
       " 'asc',\n",
       " 'asc_nulls_first',\n",
       " 'asc_nulls_last',\n",
       " 'ascii',\n",
       " 'asin',\n",
       " 'asinh',\n",
       " 'assert_true',\n",
       " 'atan',\n",
       " 'atan2',\n",
       " 'atanh',\n",
       " 'avg',\n",
       " 'base64',\n",
       " 'bin',\n",
       " 'bitwiseNOT',\n",
       " 'broadcast',\n",
       " 'bround',\n",
       " 'bucket',\n",
       " 'cbrt',\n",
       " 'ceil',\n",
       " 'coalesce',\n",
       " 'col',\n",
       " 'collect_list',\n",
       " 'collect_set',\n",
       " 'column',\n",
       " 'concat',\n",
       " 'concat_ws',\n",
       " 'conv',\n",
       " 'corr',\n",
       " 'cos',\n",
       " 'cosh',\n",
       " 'count',\n",
       " 'countDistinct',\n",
       " 'covar_pop',\n",
       " 'covar_samp',\n",
       " 'crc32',\n",
       " 'create_map',\n",
       " 'cume_dist',\n",
       " 'current_date',\n",
       " 'current_timestamp',\n",
       " 'date_add',\n",
       " 'date_format',\n",
       " 'date_sub',\n",
       " 'date_trunc',\n",
       " 'datediff',\n",
       " 'dayofmonth',\n",
       " 'dayofweek',\n",
       " 'dayofyear',\n",
       " 'days',\n",
       " 'decode',\n",
       " 'degrees',\n",
       " 'dense_rank',\n",
       " 'desc',\n",
       " 'desc_nulls_first',\n",
       " 'desc_nulls_last',\n",
       " 'element_at',\n",
       " 'encode',\n",
       " 'exists',\n",
       " 'exp',\n",
       " 'explode',\n",
       " 'explode_outer',\n",
       " 'expm1',\n",
       " 'expr',\n",
       " 'factorial',\n",
       " 'filter',\n",
       " 'first',\n",
       " 'flatten',\n",
       " 'floor',\n",
       " 'forall',\n",
       " 'format_number',\n",
       " 'format_string',\n",
       " 'from_csv',\n",
       " 'from_json',\n",
       " 'from_unixtime',\n",
       " 'from_utc_timestamp',\n",
       " 'functools',\n",
       " 'get_json_object',\n",
       " 'greatest',\n",
       " 'grouping',\n",
       " 'grouping_id',\n",
       " 'hash',\n",
       " 'hex',\n",
       " 'hour',\n",
       " 'hours',\n",
       " 'hypot',\n",
       " 'initcap',\n",
       " 'input_file_name',\n",
       " 'instr',\n",
       " 'isnan',\n",
       " 'isnull',\n",
       " 'json_tuple',\n",
       " 'kurtosis',\n",
       " 'lag',\n",
       " 'last',\n",
       " 'last_day',\n",
       " 'lead',\n",
       " 'least',\n",
       " 'length',\n",
       " 'levenshtein',\n",
       " 'lit',\n",
       " 'locate',\n",
       " 'log',\n",
       " 'log10',\n",
       " 'log1p',\n",
       " 'log2',\n",
       " 'lower',\n",
       " 'lpad',\n",
       " 'ltrim',\n",
       " 'map_concat',\n",
       " 'map_entries',\n",
       " 'map_filter',\n",
       " 'map_from_arrays',\n",
       " 'map_from_entries',\n",
       " 'map_keys',\n",
       " 'map_values',\n",
       " 'map_zip_with',\n",
       " 'max',\n",
       " 'md5',\n",
       " 'mean',\n",
       " 'min',\n",
       " 'minute',\n",
       " 'monotonically_increasing_id',\n",
       " 'month',\n",
       " 'months',\n",
       " 'months_between',\n",
       " 'nanvl',\n",
       " 'next_day',\n",
       " 'nth_value',\n",
       " 'ntile',\n",
       " 'overlay',\n",
       " 'pandas_udf',\n",
       " 'percent_rank',\n",
       " 'percentile_approx',\n",
       " 'posexplode',\n",
       " 'posexplode_outer',\n",
       " 'pow',\n",
       " 'quarter',\n",
       " 'radians',\n",
       " 'raise_error',\n",
       " 'rand',\n",
       " 'randn',\n",
       " 'rank',\n",
       " 'regexp_extract',\n",
       " 'regexp_replace',\n",
       " 'repeat',\n",
       " 'reverse',\n",
       " 'rint',\n",
       " 'round',\n",
       " 'row_number',\n",
       " 'rpad',\n",
       " 'rtrim',\n",
       " 'schema_of_csv',\n",
       " 'schema_of_json',\n",
       " 'second',\n",
       " 'sequence',\n",
       " 'sha1',\n",
       " 'sha2',\n",
       " 'shiftLeft',\n",
       " 'shiftRight',\n",
       " 'shiftRightUnsigned',\n",
       " 'shuffle',\n",
       " 'signum',\n",
       " 'sin',\n",
       " 'since',\n",
       " 'sinh',\n",
       " 'size',\n",
       " 'skewness',\n",
       " 'slice',\n",
       " 'sort_array',\n",
       " 'soundex',\n",
       " 'spark_partition_id',\n",
       " 'split',\n",
       " 'sqrt',\n",
       " 'stddev',\n",
       " 'stddev_pop',\n",
       " 'stddev_samp',\n",
       " 'struct',\n",
       " 'substring',\n",
       " 'substring_index',\n",
       " 'sum',\n",
       " 'sumDistinct',\n",
       " 'sys',\n",
       " 'tan',\n",
       " 'tanh',\n",
       " 'timestamp_seconds',\n",
       " 'toDegrees',\n",
       " 'toRadians',\n",
       " 'to_csv',\n",
       " 'to_date',\n",
       " 'to_json',\n",
       " 'to_str',\n",
       " 'to_timestamp',\n",
       " 'to_utc_timestamp',\n",
       " 'transform',\n",
       " 'transform_keys',\n",
       " 'transform_values',\n",
       " 'translate',\n",
       " 'trim',\n",
       " 'trunc',\n",
       " 'udf',\n",
       " 'unbase64',\n",
       " 'unhex',\n",
       " 'unix_timestamp',\n",
       " 'upper',\n",
       " 'var_pop',\n",
       " 'var_samp',\n",
       " 'variance',\n",
       " 'warnings',\n",
       " 'weekofyear',\n",
       " 'when',\n",
       " 'window',\n",
       " 'xxhash64',\n",
       " 'year',\n",
       " 'years',\n",
       " 'zip_with']"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pysparak.sql import functions as F\n",
    "\n",
    "dir(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71111de0-d6fa-4952-8b75-70fc2b11fd41",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n|first_name|First Name Counts|\n+----------+-----------------+\n|     Tyler|                6|\n|  Samantha|               10|\n|    Aubrey|                4|\n|   Carolyn|               11|\n|      Chad|                9|\n|   Shannon|                8|\n|     Shawn|                5|\n|       Sue|               11|\n|     Scott|                6|\n|     Ruben|                9|\n|     Flenn|                5|\n|  Rosemary|                3|\n|     Grace|                7|\n|     Lucas|                8|\n|     Keith|               12|\n|    Gerald|               10|\n|       Jar|                6|\n|     Edwin|                6|\n|     Soham|                5|\n|  Savannah|                7|\n+----------+-----------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "json_df.groupby(\"first_name\").agg(F.count(\"first_name\").alias('First Name Counts')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5556e816-74b0-41dc-b3fe-5af3ab85f049",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### More on Optimization\n",
    "\n",
    "1. Caching Data:\n",
    "   * Cache table contents or query outputs to expedite repeated data access.\n",
    "   * Cached data, especially with Tungsten compression, can occupy less RAM compared to disk storage.\n",
    "   * Employ Lazy Caching to cache data as needed and utilize `UNCACHE` to free up space for caching other DataFrames.\n",
    "\n",
    "2. **Adjusting DataFrame Partitioning**:\n",
    "   * Narrow Operations (e.g., `COUNT`): Operate independently on each partition, no data shuffle required.\n",
    "   * Wide Operations (e.g., `GROUPBY`): Require data shuffling as they need data from multiple partitions.\n",
    "   * Minimize data shuffling by tuning the number of partitions; excessive shuffling can lead to performance bottlenecks.\n",
    "\n",
    "3. Additional Performance Tuning:\n",
    "   * Explore further optimizations in Spark's [Performance Tuning documentation](https://spark.apache.org/docs/latest/sql-performance-tuning.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d66b9221-cc6e-439c-8fc8-ebb124702f21",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark_introduction_2",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
