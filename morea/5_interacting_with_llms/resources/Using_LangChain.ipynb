{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8f3f7bc",
   "metadata": {},
   "source": [
    "### Understanding LangChain: A Modular Framework for LLMs\n",
    "\n",
    "* LangChain is fundamentally a framework designed for Large Language Models (LLMs).\n",
    "\n",
    "* It enables the development of various applications such as chatbots, Generative Question-Answering (GQA), content summarization, and beyond.\n",
    "\n",
    "* The essence of the framework lies in its ability to \"chain\" diverse components, facilitating the creation of sophisticated functionalities utilizing LLMs.\n",
    "  * Chains are composed of various elements across different modules, including:\n",
    "\n",
    "* These are pre-designed templates tailored for specific interactions, ranging from chatbot dialogues to Explain Like I'm Five (ELI5) question-responding formats.\n",
    "\n",
    "* This encompasses a range of Large Language Models such as ChatGPT, Bard, Claude, etc.\n",
    "* Agents leverage LLMs to determine necessary actions. They can employ tools like web search or calculators, integrated into a cohesive operational loop.\n",
    "* Incorporating both short-term and long-term memory functionalities.\n",
    "\n",
    "* Our primary aim here is to delve into the functionality that enables the transformation of unstructured text into structured data, extracting valuable insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcb8e12",
   "metadata": {},
   "source": [
    "### Core Components of LangChain\n",
    "\n",
    "* Chains are composed of various modules that can be combined to enhance the capabilities of LLMs.\n",
    "\n",
    "Key Modules Include:\n",
    "\n",
    "  * Prompt Templates: Customizable templates suited for different interaction styles, including chatbot  conversations.\n",
    "  * LLMs: Incorporation of various Large Language Models such as ChatGPT, Bard, Claude, etc.\n",
    "  *  Agents: Agents utilize LLMs to determine the necessary actions, employing tools like web searches or calculators within a logical operational loop.\n",
    "  * Memory Modules: These include both short-term and long-term memory functionalities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dac5307c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://onboard.openai.com for details, or email support@openai.com if you have any questions.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# openai.api_key = \"ADD API KEY HERE\"\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m  \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "File \u001b[0;32m~/miniconda3/envs/temp/lib/python3.9/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/miniconda3/envs/temp/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py:149\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[1;32m    141\u001b[0m         timeout,\n\u001b[1;32m    142\u001b[0m         stream,\n\u001b[1;32m    143\u001b[0m         headers,\n\u001b[1;32m    144\u001b[0m         request_timeout,\n\u001b[1;32m    145\u001b[0m         typed_api_type,\n\u001b[1;32m    146\u001b[0m         requestor,\n\u001b[1;32m    147\u001b[0m         url,\n\u001b[1;32m    148\u001b[0m         params,\n\u001b[0;32m--> 149\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__prepare_create_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_version\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morganization\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m requestor\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/temp/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py:106\u001b[0m, in \u001b[0;36mEngineAPIResource.__prepare_create_request\u001b[0;34m(cls, api_key, api_base, api_type, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    104\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m MAX_TIMEOUT\n\u001b[0;32m--> 106\u001b[0m requestor \u001b[38;5;241m=\u001b[39m \u001b[43mapi_requestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAPIRequestor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43morganization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morganization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mclass_url(engine, api_type, api_version)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    115\u001b[0m     deployment_id,\n\u001b[1;32m    116\u001b[0m     engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    124\u001b[0m     params,\n\u001b[1;32m    125\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/temp/lib/python3.9/site-packages/openai/api_requestor.py:130\u001b[0m, in \u001b[0;36mAPIRequestor.__init__\u001b[0;34m(self, key, api_base, api_type, api_version, organization)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    123\u001b[0m     key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m     organization\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    128\u001b[0m ):\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_base \u001b[38;5;241m=\u001b[39m api_base \u001b[38;5;129;01mor\u001b[39;00m openai\u001b[38;5;241m.\u001b[39mapi_base\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m key \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_api_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_type \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    132\u001b[0m         ApiType\u001b[38;5;241m.\u001b[39mfrom_str(api_type)\n\u001b[1;32m    133\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m api_type\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m ApiType\u001b[38;5;241m.\u001b[39mfrom_str(openai\u001b[38;5;241m.\u001b[39mapi_type)\n\u001b[1;32m    135\u001b[0m     )\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_version \u001b[38;5;241m=\u001b[39m api_version \u001b[38;5;129;01mor\u001b[39;00m openai\u001b[38;5;241m.\u001b[39mapi_version\n",
      "File \u001b[0;32m~/miniconda3/envs/temp/lib/python3.9/site-packages/openai/util.py:186\u001b[0m, in \u001b[0;36mdefault_api_key\u001b[0;34m()\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m openai\u001b[38;5;241m.\u001b[39mapi_key\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m openai\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mAuthenticationError(\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo API key provided. You can set your API key in code using \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenai.api_key = <API-KEY>\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenai.api_key_path = <PATH>\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. You can generate API keys in the OpenAI web interface. See https://onboard.openai.com for details, or email support@openai.com if you have any questions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    188\u001b[0m     )\n",
      "\u001b[0;31mAuthenticationError\u001b[0m: No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://onboard.openai.com for details, or email support@openai.com if you have any questions."
     ]
    }
   ],
   "source": [
    "prompt = \"\"\" What is the most populated city in the state of Hawaii. \n",
    "Provide city name and no additional information.\"\"\"\n",
    "\n",
    "import os\n",
    "import openai\n",
    "\n",
    "# openai.api_key = \"ADD API KEY HERE\"\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": prompt\n",
    "    }\n",
    "  ],\n",
    "  temperature=0,\n",
    "  max_tokens=128,\n",
    ")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8dbc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "response[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74e5863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06094074",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1cdfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_str = \"\"\"What is the most populated city in the state of Hawaii. \n",
    "Provide city name and no additional information.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(prompt_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57db3165",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2614340b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8813f9dd",
   "metadata": {},
   "source": [
    "### Prompts Are First Class objects in LangChain\n",
    "\n",
    "* Prompts can be easily tailored to incorporate runtime variables.\n",
    "* They can also be customized with examples for more precise and context-relevant responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9791241d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_str = \"\"\"What is the most populated city in the state of {state}.\n",
    "\n",
    "Provide city name and no additional information.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(prompt_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c52da1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5c20ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke({\"state\": \"Hawaii\"})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99901955",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke({\"state\": \"California\"})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6045f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke({\"state\": \"Georgia\"})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8a5d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_str = \"\"\"What is the most populated city in the state provided below.\n",
    "\n",
    "Provide city name and no additional information. \n",
    "\n",
    "Examples:\n",
    "\n",
    "State: Hawaii\n",
    "City: Honolulu\n",
    "\n",
    "State: California\n",
    "City: Los Angeles\n",
    "\n",
    "State: {state}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(prompt_str)\n",
    "\n",
    "chain = prompt | model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f4e2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke({\"state\": \"Georgia\"})\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcb73f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38054a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_str = \"\"\"What is the most populated city in the state provided below.\n",
    "\n",
    "Provide city name and no additional information. \n",
    "\n",
    "Examples:\n",
    "\n",
    "State: Hawaii\n",
    "{{\"City\": \"Honolulu\"}}\n",
    "\n",
    "State: California\n",
    "{{\"City\": \"Los Angeles\"}}\n",
    "\n",
    "State: {state}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(prompt_str)\n",
    "\n",
    "chain = prompt | model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6034cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke({\"state\": \"Georgia\"})\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cfc21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c313d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = json.loads(response.content)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71089bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"City\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6f587c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_prefix = \"\"\"What is the most populated city in the state provided below. \n",
    "Provide city name and no additional information. \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f910569",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_examples = [\n",
    "    {\"ExampleState\": \"Hawaii\", \"ExampleCity\": \"Honolulu\"},\n",
    "    {\"ExampleState\": \"California\", \"ExampleCity\": \"Los Angeles\"}   \n",
    "]\n",
    "prompt_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eabaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt_str =\"State: {ExampleState}\\nCity: {ExampleCity}\"\n",
    "print(example_prompt_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652ecbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = PromptTemplate(input_variables=[\"ExampleState\", \"ExampleCity\"], template = example_prompt_str)\n",
    "\n",
    "example_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ae69fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example_prompt.format(**prompt_examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f35ecc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example_prompt.format(**prompt_examples[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab28255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "\n",
    "execute_fewshot_prompt = FewShotPromptTemplate(\n",
    "    prefix = prompt_prefix,\n",
    "    input_variables=[\"state\"],\n",
    "    examples= prompt_examples,\n",
    "    example_prompt = example_prompt,\n",
    "    example_separator=\"\\n\\n\",\n",
    "    suffix = \"State: {state}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827de395",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"state\": \"Georgia\"}\n",
    "print(execute_fewshot_prompt.format(**data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c04532",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = execute_fewshot_prompt | model\n",
    "chain.invoke(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0508d9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt_str_json = \"\"\" State: {ExampleState}\\n  {open_curly} \"City\": \"{ExampleCity}\" {close_curly} \"\"\"\n",
    "print(example_prompt_str_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d893123",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"ExampleState\", \"ExampleCity\"],  \n",
    "    partial_variables={\"open_curly\": \"{{\", \"close_curly\": \"}}\"},\n",
    "    template = example_prompt_str_json)\n",
    "example_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6539f41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_examples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568d9004",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example_prompt.format(**prompt_examples[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7751cf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe47bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "\n",
    "execute_fewshot_prompt = FewShotPromptTemplate(\n",
    "    prefix = prompt_prefix,\n",
    "    input_variables=[\"state\"], \n",
    "\n",
    "    examples= prompt_examples,\n",
    "    example_prompt = example_prompt,\n",
    "    example_separator=\"\\n\\n\",\n",
    "    suffix = \"State: {state}\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f170354",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"state\": \"Georgia\"}\n",
    "print(execute_fewshot_prompt.format(**data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ec6cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = execute_fewshot_prompt | model\n",
    "response = chain.invoke(data)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6946f5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cb6f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.loads(response.content)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38e3edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['City']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465de8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fec0252",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CityParser(BaseModel):\n",
    "    \"\"\"\n",
    "    this object holds information about the most populated city in the \n",
    "    given state.\n",
    "    \"\"\"\n",
    "    City: str = Field(..., description=\"The name of the most populous city\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b471bd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "cityParser = PydanticOutputParser(pydantic_object=CityParser)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6a63fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cityParser.parse(\"\"\"{\"City\": \"Atlanta\"}\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40c4f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = cityParser.parse(\"\"\"{\"City\": \"Atlanta\"}\"\"\")\n",
    "output.City\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526f5be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(execute_fewshot_prompt.format(**data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f655c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"state\": \"Georgia\"}\n",
    "chain = execute_fewshot_prompt | model | cityParser\n",
    "reponse = chain.invoke(data)\n",
    "reponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a778559",
   "metadata": {},
   "outputs": [],
   "source": [
    "reponse.City\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcc3013",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"state\": \"Georgia\"}\n",
    "print(execute_fewshot_prompt.format(**data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc303d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aabdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_prefix = \"\"\"What is the most populated city in the state provided below. \n",
    "Provide city name and no additional information. \n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9048f48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607ada21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cityParser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22463617",
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_fewshot_prompt = FewShotPromptTemplate(\n",
    "    prefix = prompt_prefix,\n",
    "    input_variables=[\"state\"], \n",
    "    partial_variables={\"format_instructions\": cityParser.get_format_instructions()},\n",
    "    examples= prompt_examples,\n",
    "    example_prompt = example_prompt,\n",
    "    example_separator=\"\\n\\n\",\n",
    "    suffix = \"State: {state}\\n\"\n",
    ")\n",
    "data = {\"state\": \"Georgia\"}\n",
    "print(execute_fewshot_prompt.format(**data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e479422",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"state\": \"Georgia\"}\n",
    "chain = execute_fewshot_prompt | model | cityParser\n",
    "reponse = chain.invoke(data)\n",
    "reponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765b0d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774bec82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "HUGGINGFACEHUB_API_TOKEN = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfcc546",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFaceHub\n",
    "repo_id_flan = \"google/flan-t5-xxl\" \n",
    "\n",
    "\n",
    "llm_google_flan = HuggingFaceHub(\n",
    "    repo_id= repo_id_flan, model_kwargs={\"temperature\": 1, \"max_length\": 64},\n",
    "    huggingfacehub_api_token = HUGGINGFACEHUB_API_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcaf5717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': 'Georgia'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\"state\": \"Georgia\"}\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9db2ca4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'execute_fewshot_prompt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mexecute_fewshot_prompt\u001b[49m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'execute_fewshot_prompt' is not defined"
     ]
    }
   ],
   "source": [
    "print(execute_fewshot_prompt.format(**data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b5e44dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = execute_fewshot_prompt | llm_google_flan \n",
    "reponse = chain.invoke(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0057d8d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"City\": \"Atlanta\"'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9562ce4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{ \"City\": \"Atlanta\" } \\n\\nState: Texas\\n{ \"City'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import HuggingFaceHub\n",
    "# repo_id_llama_2 = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "repo_id_mistral = \"mistralai/Mistral-7B-Instruct-v0.1\" \n",
    "\n",
    "\n",
    "llm_mistral = HuggingFaceHub(\n",
    "    repo_id= repo_id_mistral, model_kwargs={\"temperature\": 1, \"max_length\": 64},\n",
    "    huggingfacehub_api_token = HUGGINGFACEHUB_API_TOKEN\n",
    ")\n",
    "\n",
    "chain = execute_fewshot_prompt | llm_mistral\n",
    "\n",
    "reponse = chain.invoke(data)\n",
    "\n",
    "reponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "67a26958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{ \"City\": \"Atlanta\" } '"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = execute_fewshot_prompt | llm_mistral.bind(stop=\"\\n\")\n",
    "\n",
    "reponse = chain.invoke(data)\n",
    "\n",
    "reponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1288cbcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CityParser(City='Atlanta')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = execute_fewshot_prompt | llm_mistral.bind(stop=\"\\n\") | cityParser\n",
    "\n",
    "reponse = chain.invoke(data)\n",
    "reponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fae18d17",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for Ollama\nmodel_kwargs\n  extra fields not permitted (type=value_error.extra)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [87], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmanager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CallbackManager\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstreaming_stdout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StreamingStdOutCallbackHandler\n\u001b[0;32m----> 5\u001b[0m ollama_llama_llm \u001b[38;5;241m=\u001b[39m \u001b[43mOllama\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllama2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCallbackManager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mStreamingStdOutCallbackHandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/temp/lib/python3.9/site-packages/langchain/load/serializable.py:74\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/miniconda3/envs/temp/lib/python3.9/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for Ollama\nmodel_kwargs\n  extra fields not permitted (type=value_error.extra)"
     ]
    }
   ],
   "source": [
    "# from langchain.llms import Ollama\n",
    "# from langchain.callbacks.manager import CallbackManager\n",
    "# from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "# ollama_llama_llm = Ollama(\n",
    "#     model=\"llama2\", callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),    \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cdd76eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': 'Georgia'}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d411c5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, I can help you with that! The most populated city in the state provided is:\n",
      "\n",
      "{ \"City\": \"Los Angeles\" }"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sure, I can help you with that! The most populated city in the state provided is:\\n\\n{ \"City\": \"Los Angeles\" }'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = execute_fewshot_prompt | ollama_llama_llm\n",
    "\n",
    "reponse = chain.invoke(data)\n",
    "reponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32db999c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9f51bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
