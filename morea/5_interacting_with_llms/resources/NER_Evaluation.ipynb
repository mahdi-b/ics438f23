{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a516443f",
   "metadata": {},
   "source": [
    "### Token-Level Evaluation for Named Entity Recognition (NER)\n",
    "\n",
    "* Token-level evaluation is a common practice in NER model assessment. However, this approach has limitations as it doesn't consider full-entity accuracy.\n",
    "* A named entity can span multiple tokens, and token-level evaluation may not capture this adequately.\n",
    "* The followin is based on the [nerevalaute](https://github.com/MantisAI/nervaluate/) library documentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6805540-54c6-4b09-9ff3-aeaa9d92953f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nervaluate\n",
      "  Downloading nervaluate-0.1.8-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: nervaluate\n",
      "Successfully installed nervaluate-0.1.8\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nervaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e8170dc-6870-460c-9f31-178a926ce148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ent_type': {'correct': 3,\n",
       "  'incorrect': 0,\n",
       "  'partial': 0,\n",
       "  'missed': 0,\n",
       "  'spurious': 0,\n",
       "  'possible': 3,\n",
       "  'actual': 3,\n",
       "  'precision': 1.0,\n",
       "  'recall': 1.0,\n",
       "  'f1': 1.0},\n",
       " 'partial': {'correct': 3,\n",
       "  'incorrect': 0,\n",
       "  'partial': 0,\n",
       "  'missed': 0,\n",
       "  'spurious': 0,\n",
       "  'possible': 3,\n",
       "  'actual': 3,\n",
       "  'precision': 1.0,\n",
       "  'recall': 1.0,\n",
       "  'f1': 1.0},\n",
       " 'strict': {'correct': 3,\n",
       "  'incorrect': 0,\n",
       "  'partial': 0,\n",
       "  'missed': 0,\n",
       "  'spurious': 0,\n",
       "  'possible': 3,\n",
       "  'actual': 3,\n",
       "  'precision': 1.0,\n",
       "  'recall': 1.0,\n",
       "  'f1': 1.0},\n",
       " 'exact': {'correct': 3,\n",
       "  'incorrect': 0,\n",
       "  'partial': 0,\n",
       "  'missed': 0,\n",
       "  'spurious': 0,\n",
       "  'possible': 3,\n",
       "  'actual': 3,\n",
       "  'precision': 1.0,\n",
       "  'recall': 1.0,\n",
       "  'f1': 1.0}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nervaluate import Evaluator\n",
    "true = [\n",
    "    ['O', 'O', 'B-PER', 'I-PER', 'O'],\n",
    "    ['O', 'B-LOC', 'I-LOC', 'B-LOC', 'I-LOC', 'O'],\n",
    "]\n",
    "\n",
    "pred = [\n",
    "    ['O', 'O', 'B-PER', 'I-PER', 'O'],\n",
    "    ['O', 'B-LOC', 'I-LOC', 'B-LOC', 'I-LOC', 'O'],\n",
    "]\n",
    "\n",
    "evaluator = Evaluator(true, pred, tags=['LOC', 'PER'], loader=\"list\")\n",
    "\n",
    "results, results_by_tag = evaluator.evaluate()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f92b3558-8dad-4690-98be-d1e9d8b04353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LOC': {'ent_type': {'correct': 2,\n",
       "   'incorrect': 0,\n",
       "   'partial': 0,\n",
       "   'missed': 0,\n",
       "   'spurious': 0,\n",
       "   'possible': 2,\n",
       "   'actual': 2,\n",
       "   'precision': 1.0,\n",
       "   'recall': 1.0,\n",
       "   'f1': 1.0},\n",
       "  'partial': {'correct': 2,\n",
       "   'incorrect': 0,\n",
       "   'partial': 0,\n",
       "   'missed': 0,\n",
       "   'spurious': 0,\n",
       "   'possible': 2,\n",
       "   'actual': 2,\n",
       "   'precision': 1.0,\n",
       "   'recall': 1.0,\n",
       "   'f1': 1.0},\n",
       "  'strict': {'correct': 2,\n",
       "   'incorrect': 0,\n",
       "   'partial': 0,\n",
       "   'missed': 0,\n",
       "   'spurious': 0,\n",
       "   'possible': 2,\n",
       "   'actual': 2,\n",
       "   'precision': 1.0,\n",
       "   'recall': 1.0,\n",
       "   'f1': 1.0},\n",
       "  'exact': {'correct': 2,\n",
       "   'incorrect': 0,\n",
       "   'partial': 0,\n",
       "   'missed': 0,\n",
       "   'spurious': 0,\n",
       "   'possible': 2,\n",
       "   'actual': 2,\n",
       "   'precision': 1.0,\n",
       "   'recall': 1.0,\n",
       "   'f1': 1.0}},\n",
       " 'PER': {'ent_type': {'correct': 1,\n",
       "   'incorrect': 0,\n",
       "   'partial': 0,\n",
       "   'missed': 0,\n",
       "   'spurious': 0,\n",
       "   'possible': 1,\n",
       "   'actual': 1,\n",
       "   'precision': 1.0,\n",
       "   'recall': 1.0,\n",
       "   'f1': 1.0},\n",
       "  'partial': {'correct': 1,\n",
       "   'incorrect': 0,\n",
       "   'partial': 0,\n",
       "   'missed': 0,\n",
       "   'spurious': 0,\n",
       "   'possible': 1,\n",
       "   'actual': 1,\n",
       "   'precision': 1.0,\n",
       "   'recall': 1.0,\n",
       "   'f1': 1.0},\n",
       "  'strict': {'correct': 1,\n",
       "   'incorrect': 0,\n",
       "   'partial': 0,\n",
       "   'missed': 0,\n",
       "   'spurious': 0,\n",
       "   'possible': 1,\n",
       "   'actual': 1,\n",
       "   'precision': 1.0,\n",
       "   'recall': 1.0,\n",
       "   'f1': 1.0},\n",
       "  'exact': {'correct': 1,\n",
       "   'incorrect': 0,\n",
       "   'partial': 0,\n",
       "   'missed': 0,\n",
       "   'spurious': 0,\n",
       "   'possible': 1,\n",
       "   'actual': 1,\n",
       "   'precision': 1.0,\n",
       "   'recall': 1.0,\n",
       "   'f1': 1.0}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_by_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5befcf25",
   "metadata": {},
   "source": [
    "### Challenges in Token-Level Evaluation: I\n",
    "\n",
    "**Scenario I**: Full Match\n",
    "\n",
    "| Token   | Gold   | Prediction |\n",
    "| ------- | ------ | ---------- |\n",
    "| in      | O      | O          |\n",
    "| New     | B-LOC  | B-LOC      |\n",
    "| York    | I-LOC  | I-LOC      |\n",
    "| .       | O      | O          |\n",
    "\n",
    "- In this scenario, the gold standard and the prediction match both in terms of the surface string and entity type.\n",
    "- Token-level metrics like precision and recall work well in this case.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a0ab7a",
   "metadata": {},
   "source": [
    "### Challenges in Token-Level Evaluation: II\n",
    "\n",
    "**Scenario II**: System Hypothesized an Incorrect Entity\n",
    "\n",
    "| Token   | Gold   | Prediction |\n",
    "| ------- | ------ | ---------- |\n",
    "| an      | O      | O          |\n",
    "| Awful   | O      | B-ORG      |\n",
    "| Headache| O      | I-ORG      |\n",
    "| in      | O      | O          |\n",
    "\n",
    "- Here, the NER system incorrectly predicts the entity as an organization (B-ORG, I-ORG) instead of recognizing it as an ordinary word (O).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb443db",
   "metadata": {},
   "source": [
    "### Challenges in Token-Level Evaluation: III\n",
    "\n",
    "**Scenario III**: System Misses an Entity\n",
    "\n",
    "| Token   | Gold   | Prediction |\n",
    "| ------- | ------ | ---------- |\n",
    "| in      | O      | O          |\n",
    "| Palo    | B-LOC  | O          |\n",
    "| Alto    | I-LOC  | O          |\n",
    "| ,       | O      | O          |\n",
    "\n",
    "- In this scenario, the NER system completely misses the named entity (Palo Alto) that exists in the gold standard.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8020cb03",
   "metadata": {},
   "source": [
    "### Example Receipt\n",
    "\n",
    "```\n",
    "XYZ Mart\n",
    "1234 Main Lp.\n",
    "Anytown, USA\n",
    "\n",
    "Receipt: #123456789\n",
    "Date: 2023-10-21 16:30:00\n",
    "\n",
    "--------------------------------------------------\n",
    "Description        |  Qty   |  Price   |  Total\n",
    "--------------------------------------------------\n",
    "\n",
    "Apples (2 lb)       |   1    |  $2.99   |  $2.99\n",
    "Milk (1 gal)        |   2    |  $3.49   |  $6.98\n",
    "Bread (Whole Wheat) |   1    |  $2.29   |  $2.29\n",
    "Toilet Paper (6 pk) |   2    | $4.99    |  $9.98\n",
    "Dish Soap (16 oz)   |   1    |  $1.99   |  $1.99\n",
    "Charging Cable      |   3    |  $5.99   | $17.97\n",
    "\n",
    "--------------------------------------------------\n",
    "Subtotal:                      $41.20\n",
    "Sales Tax (7%):                $2.88\n",
    "Total:                         $44.08\n",
    "\n",
    "Payment Method: Credit Card\n",
    "Card Ending In: **** 1234\n",
    "Authorization Code: 987654\n",
    "\n",
    "Thank you for shopping with us!\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db443e33",
   "metadata": {},
   "source": [
    "### Example Receipt: Annotation\n",
    "\n",
    "```\n",
    "[ORG]: XYZ Mart\n",
    "[ADDRESS]: 1234 Main Lp.\n",
    "[ADDRESS]: Anytown, USA\n",
    "\n",
    "Receipt: #123456789\n",
    "[RECEIPT_NUMBER]: #123456789\n",
    "[DATE]: 2023-10-21 16:30:00\n",
    "\n",
    "--------------------------------------------------\n",
    "Description        |  Qty   |  Price   |  Total\n",
    "--------------------------------------------------\n",
    "\n",
    "Apples (2 lb)       |   1    |  $2.99   |  $2.99\n",
    "[PRODUCT]: Apples\n",
    "[QUANTITY]: (2 lb)\n",
    "[PRICE]: $2.99\n",
    "[TOTAL_PRICE]: $2.99\n",
    "\n",
    "Milk (1 gal)        |   2    |  $3.49   |  $6.98\n",
    "[PRODUCT]: Milk\n",
    "[QUANTITY]: (1 gal)\n",
    "[PRICE]: $3.49\n",
    "[TOTAL_PRICE]: $6.98\n",
    "\n",
    "Bread (Whole Wheat) |   1    |  $2.29   |  $2.29\n",
    "[PRODUCT]: Bread (Whole Wheat)\n",
    "[QUANTITY]: 1\n",
    "[PRICE]: $2.29\n",
    "[TOTAL_PRICE]: $2.29\n",
    "\n",
    "Toilet Paper (6 pk) |   2    |  $4.99   |  $9.98\n",
    "[PRODUCT]: Toilet Paper (6 pk)\n",
    "[QUANTITY]: 2\n",
    "[PRICE]: $4.99\n",
    "[TOTAL_PRICE]: $9.98\n",
    "\n",
    "Dish Soap (16 oz)   |   1    |  $1.99   |  $1.99\n",
    "[PRODUCT]: Dish Soap (16 oz)\n",
    "[QUANTITY]: 1\n",
    "[PRICE]: $1.99\n",
    "[TOTAL_PRICE]: $1.99\n",
    "\n",
    "Charging Cable      |   3    |  $5.99   | $17.97\n",
    "[PRODUCT]: Charging Cable\n",
    "[QUANTITY]: 3\n",
    "[PRICE]: $5.99\n",
    "[TOTAL_PRICE]: $17.97\n",
    "\n",
    "--------------------------------------------------\n",
    "[SUBTOTAL]:                       $41.20\n",
    "[SALES_TAX]: (7%):                $2.88\n",
    "[TOTAL_AMOUNT]:                   $44.08\n",
    "\n",
    "[PAYMENT_METHOD]: Credit Card\n",
    "[CARD_ENDING]: **** 1234\n",
    "[AUTHORIZATION_CODE]: 987654\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9a0180",
   "metadata": {},
   "source": [
    "### Example Receipt: Annotation Comparison\n",
    "| Token        | Gold                | Prediction     |\n",
    "|------------|-------------------|--------------------|\n",
    "| MART         | I-ORG               | I-ORG               |\n",
    "| 1234         | B-ADDRESS           | B-ADDRESS           |\n",
    "| Main         | I-ADDRESS           | I-ADDRESS           |\n",
    "| Lp           | I-ADDRESS           | O                   |\n",
    "| .            | O                   | O                   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc09848d",
   "metadata": {},
   "source": [
    "### Precision and Recall in NER\n",
    "\n",
    "- **Precision**: The percentage of named entities completely identified by the system that are correct.\n",
    "- **Recall**: The percentage of named entities in the corpus found by the system.\n",
    "- **F1-score**: The harmonic mean of precision and recall.\n",
    "    \n",
    "* The metrics are valuable for NER evaluation, but do have importnat limitations:\n",
    "\n",
    "* Ignores Partial Matches: they only considers exact matches, overlooking partial matches or overlapping entities.\n",
    " * No entity-level evaluation , which is crucial in real-world applications.\n",
    "* Don't address other scenarios, such as nested entities or entities that cross sentence boundaries.\n",
    "*  We may need to adapt evaluation schemas based on specific NER tasks and requirements beyond the CoNLL-2003 metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c87f42",
   "metadata": {},
   "source": [
    "### Challenges in Token-Level Evaluation: IV\n",
    "\n",
    "**class IV**: System assigns the wrong entity type\n",
    "\n",
    "|Token|Gold|Prediction|\n",
    "|---|---|---|\n",
    "|I|O|O|\n",
    "|live|O|O|\n",
    "|in|O|O|\n",
    "|Palo|B-LOC|B-ORG|\n",
    "|Alto|I-LOC|I-ORG|\n",
    "|,|O|O|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdb80ca",
   "metadata": {},
   "source": [
    "### Challenges in Token-Level Evaluation: V\n",
    "\n",
    "**class V**: System gets the boundaries wrong\n",
    "\n",
    "|Token|Gold|Prediction|\n",
    "|---|---|---|\n",
    "|Unless|O|B-PER|\n",
    "|Karl|B-PER|I-PER|\n",
    "|Smith|I-PER|I-PER|\n",
    "|resigns|O|O|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348709f8",
   "metadata": {},
   "source": [
    "### Challenges in Token-Level Evaluation: VI\n",
    "\n",
    "**Class VI**: System gets the boundaries and entity type wrong\n",
    "\n",
    "|Token|Gold|Prediction|\n",
    "|---|---|---|\n",
    "|Unless|O|B-ORG|\n",
    "|Karl|B-PER|I-ORG|\n",
    "|Smith|I-PER|I-ORG|\n",
    "|resigns|O|O|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21052bd",
   "metadata": {},
   "source": [
    "### MUC Evaluation Metrics for NER\n",
    "\n",
    "* MUC (Message Understanding Conference) introduced comprehensive metrics for evaluating Named Entity Recognition (NER) systems.\n",
    "* These metrics assess different categories of errors by comparing system output to golden annotations.\n",
    "\n",
    "* **Correct (COR)**: Both system output and golden annotation are identical.\n",
    "* **Incorrect (INC)**: System output and golden annotation don't match.\n",
    "* **Partial (PAR)**: System and golden annotation are somewhat similar but not identical.\n",
    "* **Missing (MIS)**: A golden annotation is not captured by the system.\n",
    "* **Spurious (SPU)**: The system produces a response that doesn't exist in the golden annotation.\n",
    "\n",
    "* These metrics go beyond strict classification and allow for partial matching, offering a more nuanced evaluation of NER systems.\n",
    "* They cover various scenarios encountered in NER, including recognizing differences in surface strings and entity types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b363c7e",
   "metadata": {},
   "source": [
    "### Variants of Precision/Recall/F1-Score\n",
    "\n",
    "* The workshop [SemEval’13](https://www.aclweb.org/portal/content/semeval-2013-international-workshop-semantic-evaluation) introduced four variants of precision, recall, and F1-score metrics based on the MUC framework.\n",
    "  * These variants provide different ways to evaluate NER performance.\n",
    "\n",
    "\n",
    "|Evaluation schema|Explanation|\n",
    "|:---|:---|\n",
    "|Strict|exact boundary string match and entity type|\n",
    "|Exact|exact boundary match over the string, regardless of the type|\n",
    "|Partial|partial boundary match over the string, regardless of the type|\n",
    "|Type|some overlap between the system tagged entity and the gold annotation is required|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fa840e",
   "metadata": {},
   "source": [
    "### Understanding the New Variants\n",
    "\n",
    "* Each variant of precision, recall, and F1-score assesses NER performance differently.\n",
    "  * They account for correct, incorrect, partial, missed, and spurious entity recognition in unique ways.\n",
    "\n",
    "| Scenario | Golden Standard           |  | System Prediction       |  | Evaluation Schema       |  |  |  |\n",
    "|:----------|:---------------------------|:----|------------------------|:----|------------------------|----|----|----|\n",
    "|          | Entity Type               | Surface String           | Entity Type              | Surface String        | Type | Partial | Exact | Strict |\n",
    "| III      | brand                     | TIKOSYN                  |                          |                       | MIS  | MIS     | MIS   | MIS    |\n",
    "| II       |                           |                          | brand                    | healthy               | SPU  | SPU     | SPU   | SPU    |\n",
    "| V        | drug                      | warfarin                 | drug                     | of warfarin           | COR  | PAR     | INC   | INC    |\n",
    "| IV       | drug                      | propranolol              | brand                    | propranolol           | INC  | COR     | COR   | INC    |\n",
    "| I        | drug                      | phenytoin                | drug                     | phenytoin             | COR  | COR     | COR   | COR    |\n",
    "| I        | drug                      | theophylline             | drug                     | theophylline          | COR  | COR     | COR   | COR    |\n",
    "| VI       | group                     | contraceptives           | drug                     | oral contraceptives   | INC  | PAR     | INC   | INC    |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188687cc",
   "metadata": {},
   "source": [
    "### Computing the Actual Precision/Recall/F1-Score\n",
    "```\n",
    "ACTUAL (ACT) = COR + INC + PAR + SPU = TP + FP\n",
    "POSSIBLE (POS) = COR + INC + PAR + MIS = TP + FN\n",
    "```\n",
    "\n",
    "* Precision: percentage of correctly identified named entities by the NER system\n",
    "* Recall: percentage of named entities in the gold standard annotations that the NER system correctly retrieves. \n",
    "* Depending on whether we seek an exact match (strict and exact) or a partial match (partial and type) scenario.\n",
    "\n",
    "__Exact Match (i.e., strict and exact )__\n",
    "```\n",
    "Precision = (COR / ACT) = TP / (TP + FP)\n",
    "Recall = (COR / POS) = TP / (TP+FN)\n",
    "```\n",
    "__Partial Match (i.e., partial and type)__\n",
    "```\n",
    "Precision = (COR + 0.5 × PAR) / ACT = TP / (TP + FP)\n",
    "Recall = (COR + 0.5 × PAR)/POS = COR / ACT = TP / (TP + FN)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a775b76",
   "metadata": {},
   "source": [
    "### Putting it All Together\n",
    "|Measure|Type|Partial|Exact|Strict|\n",
    "|---|---|---|---|---|\n",
    "|Correct|3|3|3|2|\n",
    "|Incorrect|2|0|2|3|\n",
    "|Partial|0|2|0|0|\n",
    "|Missed|1|1|1|1|\n",
    "|Spurious|1|1|1|1|\n",
    "|Precision|0.5|0.66|0.5|0.33|\n",
    "|Recall|0.5|0.66|0.5|0.33|\n",
    "|F1|0.5|0.66|0.5|0.33|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
