{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4190ed11",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Nearest Neighbor Search\n",
    "\n",
    "- Nearest neighbor search is a method to find data points closest to a given point.\n",
    "- Uses distance measures (e.g., Euclidean) to identify 'neighbors'.\n",
    "- Applicable in diverse domains: recommendation systems, classification, data retrieval.\n",
    "\n",
    "- Unfortunately, it can be computationally expensive for large datasets.\n",
    "\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899e3cf6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Finding the k-Closest Instances\n",
    "\n",
    "* Given a set $P$ of n points with $D$ features\n",
    "  * The points are in $R^D$ space\n",
    "* Given a query $x^*$, find the nearest neighbors of $x^*$ in $P$\n",
    "\n",
    "* This needs to be carried out efficiently.\n",
    "  * The naive solution would require inspecting all $n$ points to label each query $x^*$ \n",
    "\n",
    "![](https://www.dropbox.com/s/36yxn97bz11yird/nearest_p.png?dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfb3a1d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### k-Nearest Neighbor Algorithm: Exhaustive Search\n",
    "\n",
    "- Uses a similarity score or a distance metric to keep track of nearest neighbors  \n",
    "- Neighbor count: $k \\geq 1$ \n",
    "- Search Method: Linear scan\n",
    "  * Evaluate query against every database sequence.\n",
    "  * Compute similarity score for each.\n",
    "  * Update list of the top $k$ scores after each comparison.\n",
    "- Complexity:  $O(N \\times d)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbdedd6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### k-Dimensional Tree Overview\n",
    "\n",
    "- Purpose: Organizes points within a k-dimensional space using a space-partitioning data structure.\n",
    "- Structure: A binary tree where each node represents a k-dimensional point.\n",
    "- Internal Nodes: Act as splitting hyperplanes, dividing space into two half-spaces.\n",
    "  * For instance, if splitting on feature $z$:\n",
    "    - Points with \"z\" value less than the node's go to the left subtree.\n",
    "    - Points with \"z\" value greater than the node's go to the right subtree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5affeb49-9fcb-4f0c-b304-9c7bf3d52504",
   "metadata": {},
   "source": [
    "### Spatial representation of a 2-dimensional k-d tree.\n",
    "\n",
    "![](https://www.dropbox.com/s/qv61pltvk6sibql/kd_tree.png?dl=1)\n",
    "\n",
    "\n",
    "* The root node of the k-d tree is (3,5), which splits the space based on the y-coordinate.\n",
    "  * Everything with a y-value below 5 goes to the left subtree and everything with a y-value above 5 goes to the right subtree.\n",
    "* Moving one level down, the nodes (4,2) and (5,9) split based on the x-coordinate.\n",
    "  * For the node (4,2), points with an x-value less than 4 go to its left, and points with an x-value greater than 4 go to its right.\n",
    "  * Similarly, for the node (5,9), points with an x-value less than 5 go to its left, and points with an x-value greater than 5 go to its right.\n",
    "* Further, the tree keeps on partitioning until all points are included in the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade82d78",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### K-D Tree  Data Structure\n",
    "\n",
    "<img src=\"https://www.dropbox.com/s/tj0y0m4mebiyijv/3dtree.png?dl=1\" style=\"width:350px;\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c489c0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Handling Points Close to a Boundary\n",
    "\n",
    "* Start from the root and thread the nodes to find the region containing a query $x^*$\n",
    "\n",
    "* Search all the regions that intersect the region containing $x^*$\n",
    "  * In the unlikely event that we are unable to find K nearest neighbors, we can expand the search to other regions recursively\n",
    "  \n",
    "![](https://www.dropbox.com/s/1dn9oqmuxyzg2ky/kdd_search.png?dl=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e08fd9-1ce7-41ab-a74b-cae719962659",
   "metadata": {},
   "source": [
    "###  KD-Tree Search Pros and Cons\n",
    "* Pros\n",
    "  * KD-Tree supports exact nearest neighbor search.\n",
    "  * Efficient when data is in lower dimension space.\n",
    "  * Search time of $O(log~n)$ in best case \n",
    "\n",
    "* Cons: \n",
    "  * Not efficient in high-dimensional spaces.\n",
    "    * kd-tree to degrade in performance, making the search nearly as slow as a brute-force search.\n",
    "    * For high-dimensional datasets, approximate nearest neighbor methods are more efficient.\n",
    "    * $O(n^2)$ construction time.\n",
    "    * Search time of $O(log~n)$ in worst case \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98a052b",
   "metadata": {},
   "source": [
    "### Apprixmate Nearest Neighbors: Random Partitioning\n",
    "\n",
    "\n",
    "* The basic algorithm to find neighbors involves scanning all data points. How can we optimize this to find the nearest neighbor more efficiently?\n",
    "* In many applications, getting an approximate nearest neighbor is sufficient.\n",
    "  * Especially suitable for large, dense datasets.\n",
    "* What is build the tre representation of the data randomly\n",
    "  * To compensate for the inaccurarie resulting from this, we could build multiple such trees.\n",
    "* One notable tool for this is \"Annoy\" (Approximate Nearest Neighbors Oh Yeah!).\n",
    "  * [Annoy GitHub Repository](https://github.com/spotify/annoy)\n",
    "* It offers logarithmic time complexity for search operations.\n",
    "* The storage requirement is influenced by the number of trees in the forest.\n",
    "  * Each tree stores only labels, while actual data values are centralized in one location.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6f5800-f79a-4065-9675-b20bde285b64",
   "metadata": {},
   "source": [
    "### Locality Sensitive Hashing (LSH)\n",
    "\n",
    "- LSH uses hash functions to map data points to hash buckets.\n",
    "- Similar points have a higher probability of falling into the same bucket.\n",
    "- Reduce the search space by only comparing points in the same bucket.\n",
    "- Hash functions are designed to amplify the differences between dissimilar points.\n",
    "- Multiple hash tables (or functions) are used to increase accuracy.\n",
    "- The more hash tables, the higher the probability of finding true neighbors\n",
    "\n",
    "<img src=\"media/lsh.png\" style=\"width:800px;\"> \n",
    "form https://randorithms.com/2019/09/19/Visual-LSH.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bcd8b0-8ba8-41af-85c0-25fc11fad06a",
   "metadata": {},
   "source": [
    "### Product Quantization\n",
    "\n",
    "<img src=\"media/quanitzation.jpg\" style=\"width:800px;\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be2615f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Curse of Dimensionality\n",
    "\n",
    "* The distance between points increase as the number of dimension increases\n",
    "\n",
    "<img src=\"https://www.dropbox.com/s/3z8jh9esppvadrw/curse_dim.png?dl=1\" style=\"width:550px;\"> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8da0f907-f083-4d5e-b0e9-295dc8eab3a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'faiss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfaiss\u001b[39;00m\n\u001b[1;32m      2\u001b[0m d \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'faiss'"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "d = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3444ac-e185-4690-a551-7c2b57c463b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
