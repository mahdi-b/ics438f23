{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13c7eb0c-63e9-4908-a108-2c4f16fbd8a2",
   "metadata": {},
   "source": [
    "### Analyzing  Large Collection of Documents\n",
    "\n",
    "* Identifying Common Content: Finding overlap or common content among multiple documents.\n",
    "  * Word Handling: Specifically, it looks at non-stop words (words that carry significant meaning) to determine the relationship and content of the documents.\n",
    "    * Raw Word Counting: It considers the sheer occurrence of words without diving deep into the semantics or the context in which the word is used.\n",
    "\n",
    "* While this approach offers a straightforward way to analyze documents, there are certain limitations we need to be aware of.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf652b4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Overlapping Issues in Large Document Sets\n",
    "\n",
    "* As we include more diverse documents, the common content between them becomes smaller reduces.\n",
    "  * \"Sparses matches\"\n",
    "  * This can negatively impact \"similarity\" and subsequent tasks like clustering and classification.\n",
    "\n",
    "* Taking all non-stop words from combined documents results in significantly large datasets.\n",
    "\n",
    "\n",
    "* Words with similar beginnings like \"leave,\" \"leaving,\" and \"left\" are counted differently, even though they might express similar ideas.\n",
    "Ignoring Word Semantics\n",
    "  * We don't account for the different meanings a word can have based on context. For example:\n",
    "    * \"She works at the bank across the street\" versus \"houses on the bank of the river flooded due to a storm surge.\"\n",
    "* Accurately understanding word semantics in context is a challenging task and we will delve into this later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890cf84a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Distribution of Words in a Text \n",
    "\n",
    "* The frequency distribution of words in a language follows Zipf's law\n",
    "  * Just FYI: this makes computign statistics rather difficult or impossible\n",
    "  \n",
    "![](https://www.dropbox.com/s/neydq8wi2kqqof3/zipf_law.png?dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bbf775",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Understanding Document Similarity: How We Measure a Match\n",
    "\n",
    "* When we search for documents in Information Retrieval (IR), the goal is often to rank the results based on their relevance:\n",
    "  * We want to find documents that are similar to our search criteria.\n",
    "\n",
    "* The ideal result would rank these documents by how closely they match the search term.\n",
    "* The aim is to list the most relevant documents first, making it easier for the searcher to find what they're looking for.\n",
    "  * So, how do we decide the order of these documents in relation to a search term?\n",
    "  * We give each document a score between 0 and 1.\n",
    "* This score tells us how closely the document aligns with the search query.\n",
    "  * Over the years, experts have come up with many innovative solutions in the Document Retrieval field to address these challenges.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c78ec3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Measuring Document Similarity: Calculating a Match Score\n",
    "\n",
    "* Think about a simple search using just one term.\n",
    "\n",
    "* If the document doesn't have the search term, the score is 0.\n",
    "\n",
    "* If the search term appears often in the document, the score should increase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2723aaa2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Understanding the Jaccard Coefficient\n",
    "\n",
    "* Jaccard Coefficient measures the overlap between two sets, A and B.\n",
    "  * It calculates the overlap by considering all the terms in both A and B.\n",
    "\n",
    "* It works even if A and B are of different sizes.\n",
    "\n",
    "* The result is always a value between 0 and 1.\n",
    "\n",
    "* Limitations:\n",
    "  * It doesn't account for how often a term appears.\n",
    "  * It doesn't recognize that rare terms can be more valuable than common ones.\n",
    "    * This is why simply looking at the intersection might not always be best.\n",
    "\n",
    "* A better method is needed to adjust for length, rather than just using $|A \\cup B|$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6049f2f6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Understanding Term-Document Count Matrices\n",
    "\n",
    "* A count matrix displays the frequency of each word within a document.\n",
    "  * This approach, known as the \"bag of words\" model\n",
    "* The sequence of words in the document is not taken into account.\n",
    "* For instance, the phrases `John is quicker than Mary` and `Mary is quicker than John` would produce identical vectors in this model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf88fb4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Understanding Term Frequency (`tf`)\n",
    "\n",
    "* The term frequency, denoted as $tf_{t,d}$, represents how many times a term $t$ appears in a document $d$.\n",
    "* While a higher $tf$ can indicate a better match, it's not always directly tied to the significance of that match. For instance:\n",
    "  \n",
    "  * A document where the term appears 10 times is more relevant than one where it appears just once. However, it's not necessarily 10 times more relevant.\n",
    "  \n",
    "  * This means the relevance doesn't scale linearly with the term frequency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec1e540",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Understanding Log-Frequency Weighting\n",
    "\n",
    "* The weight of term $t$ in document $d$ can be determined using log-frequency as:\n",
    "\n",
    "$$\n",
    "w_{t,d} = \\begin{cases} \n",
    "1+\\log_{10}\\mbox{tf}_{t,d} & \\text{if } \\mbox{tf}_{t,d} > 0 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "* As a reference: \n",
    "  * 0 maps to 0\n",
    "  * 1 maps to 1\n",
    "  * 2 maps to 1.3\n",
    "  * 10 maps to 2\n",
    "  * 1000 maps to 4, and so on.\n",
    "\n",
    "* To calculate the score for a document-query pair, sum over terms `t` present in both the query (`q`) and the document (`d`):\n",
    "\n",
    "$$\n",
    "\\mbox{score} = \\sum_{t\\in q \\cap d}(1+\\log_{10}\\mbox{tf}_{t,d})\n",
    "$$\n",
    "\n",
    "* A score of 0 indicates that none of the terms from the query are found in the document.\n",
    "* While there might be different formulas, the core idea behind this calculation remains consistent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e78dad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Importance of Document Frequency\n",
    "\n",
    "* The challenge of rare terms remains:\n",
    "  * Rare terms often provide more valuable information than common ones.\n",
    "    * Think of stop words as an example.\n",
    "* Take the term 'arachnid' in a query, which is seldom found in the collection:\n",
    "  * A document that includes this term is highly probable to be pertinent to the query 'arachnid'.\n",
    "  * This term significantly aids in contrasting documents effectively.\n",
    "  * Therefore, it's beneficial to assign a higher weight to infrequent terms like 'arachnid'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae0aaf1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Continuing with Document Frequency\n",
    "\n",
    "* Common terms often offer less unique information than their rarer counterparts.\n",
    "* Think of a query term that's widely seen in the collection, like `high`, `increase`, or `true`:\n",
    "  * Solely using the $tf$ score, a document with these terms seems more relevant compared to one without.\n",
    "  * However, this doesn't guarantee its significance.\n",
    "* To evaluate how often a term appears across documents, we'll determine (or normalize using) its document frequency, denoted as `df`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16410223",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Inverse Document Frequency (`idf`)\n",
    "\n",
    "- $\\mbox{df}_{t,d}$ represents the frequency of term $t$ within document $d$.\n",
    "- $\\mbox{df}_t$ serves as an inverse gauge of the term $t$'s informativeness.\n",
    "  * Note: $\\mbox{df}_t \\le N$, with $N$ being the entire document count.\n",
    "\n",
    "- The inverse document frequency ($\\mbox{idf}$) of term $t$ is defined as:\n",
    "$$\n",
    "idf_t = log_{10}(N/\\mbox{df}_t)\n",
    "$$\n",
    "\n",
    "- We opt for the inverse since it's more practical than handling minuscule numbers, especially when $N$ is much larger than $\\mbox{df}_t$.\n",
    "- The logarithm (`log`) is incorporated to temper the `idf` effect, which becomes especially vital when managing vast document sets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55756d6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Term Frequency-Inverse Document Frequency (`tf-idf`) Scheme\n",
    "\n",
    "- The `tf-idf` weight of a term is derived from multiplying its term frequency (`tf`) and inverse document frequency (`idf`).\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "w_{t,d} &= \\mbox{tf}_{t,d} \\times \\mbox{idf}_t \\\\\n",
    "&= log(1+\\mbox{tf}_{t,d}) \\times log(N/\\mbox{df}_t)\n",
    "\\end{split}\n",
    "$$\n",
    "- This weighting method is a well-accepted strategy in the realm of information retrieval.\n",
    "  * Other references: tf.idf, tf x idf\n",
    "\n",
    "- The weight:\n",
    "  * Rises as a term's occurrence within a document increases.\n",
    "  * Also grows with the term's scarcity across the entire document set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4955c6d6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Score for a Document Given a Query\n",
    "\n",
    "\n",
    "$$\n",
    "Score(Q, T) = \\sum_{t\\in Q\\cap T} \\mbox{tf}.\\mbox{idf}_{t,d}\n",
    "$$\n",
    "\n",
    "* There are many variants\n",
    "  * How `tf` is computed (with/without logs)\n",
    "  * Whether the terms in the query are also weighted\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bfc8d6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using `tf-idf` for Feature Engineering\n",
    "* Each document is represented by a real-valued vector of $\\mbox{tf-idf}$ weights $\\in R^{|V|}$\n",
    "\n",
    "![](https://www.dropbox.com/s/1bx77e488ee6wek/count_tf_idf.png?dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce64927",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vector Representation in Information Retrieval (IR)\n",
    "\n",
    "- Concept 1: Represent both queries and documents as vectors within a defined space.\n",
    "- Concept 2: Sort documents based on how close their vectors are to the query vector in this space.\n",
    "  * Here, closeness refers to vector similarity.\n",
    "    * Using Euclidean distance might be misleading especially if vectors vary in length.\n",
    "      * Large Euclidean distances can arise between vectors of dissimilar lengths.\n",
    "- Concept 3: Order documents based on the angle they form with the query vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f294685",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "doc_1 = \"\"\"The king hath happily received, Macbeth,\n",
    "The news of thy success; and when he reads\n",
    "Thy personal venture in the rebels' fight,\n",
    "His wonders and his praises do contend\n",
    "Which should be thine or his: silenced with that,\n",
    "In viewing o'er the rest o' the selfsame day,\n",
    "He finds thee in the stout Norweyan ranks,\n",
    "Nothing afeard of what thyself didst make,\n",
    "Strange images of death. As thick as hail\n",
    "Came post with post; and every one did bear\n",
    "Thy praises in his kingdom's great defence,\n",
    "And pour'd them down before him.\"\"\"\n",
    "\n",
    "doc_1 = doc_1.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2503ca76",
   "metadata": {},
   "outputs": [],
   "source": [
    "lorem_ipsum = \"\"\"\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do \n",
    "eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut \n",
    "enim ad minim veniam, quis nostrud exercitation ullamco laboris \n",
    "nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor \n",
    "in reprehenderit in voluptate velit esse cillum dolore eu fugiat\n",
    "nulla pariatur.\n",
    "\"\"\".upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "63fe0278",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "corpus = [doc_1, doc_1 + lorem_ipsum]\n",
    "vocabulary = ['king', 'happily', 'and', \"thy\", \"ipsum\"]\n",
    "c_vec = CountVectorizer(vocabulary=vocabulary)\n",
    "tfidf_vec = TfidfVectorizer(vocabulary=vocabulary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "435336a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1, 4, 3, 0],\n",
       "        [1, 1, 4, 3, 1]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_vec.fit_transform(corpus).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "be1c10f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.19245009, 0.19245009, 0.76980036, 0.57735027, 0.        ],\n",
       "        [0.18577437, 0.18577437, 0.74309746, 0.5573231 , 0.26109939]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vec.fit_transform(corpus).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954ef71d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### From Angles to Cosines\n",
    "\n",
    "* In information retrieval, the following two notions are equivalent.\n",
    "  * Rank documents in decreasing order of the angle between query and hit\n",
    "  * Rank documents in increasing order of cosine(query,hit)\n",
    "\n",
    "* Cosine is a monotonically decreasing function for the interval [0o, 180o]\n",
    "\n",
    "![](https://www.dropbox.com/s/lpq4vvnlnmz0oxw/cosine.png?dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f75b10",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Length Normalization\n",
    "\n",
    "* A vector can be (length-) normalized by dividing each of its components by its length \n",
    "  * We commonly use the $L2$ norm:\n",
    "\n",
    "* Dividing a vector by its $L2$ norm makes it a unit (length) vector\n",
    "\n",
    "  * Effect on the two documents $d$ and $d′$ (d appended to itself) have identical vectors after length-normalization.\n",
    "  * Thus, long and short documents now have comparable weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915d7d3a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cosine Similairity\n",
    "\n",
    "* $q_i$ is the `tf-idf` weight of term `i` in the query\n",
    "* $d_i$ is the `tf-idf` weight of term `i` in the document\n",
    "\n",
    "![](https://www.dropbox.com/s/4x1fb50xiqidmnf/cos_equation.png?dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9036e0ff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cosine Similarity Illustrated\n",
    "\n",
    "![](https://www.dropbox.com/s/4inqt6nf9mfz6h9/cosine_similarity.png?dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef0fac1",
   "metadata": {},
   "source": [
    "### Example \n",
    "* Books: \"Sense and Sensibility\", \"Pride and Prejudice\", \"Wuthering Heights?\".\n",
    "\n",
    "<img src=\"https://www.dropbox.com/s/z28xu8xxhuv8ll5/example_books.png?dl=1\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "```\n",
    "cos(SaS,PaP) ≈ 0.789 × 0.832 + 0.515 × 0.555 + 0.335 × 0.0 + 0.0 × 0.0 ≈ 0.94\n",
    "cos(SaS,WH) ≈ 0.79\n",
    "cos(PaP,WH) ≈ 0.69\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a65d3d",
   "metadata": {},
   "source": [
    "### `tf-idf` Weighing  Variants\n",
    "\n",
    "* Just an FYI\n",
    "<div align=\"center\">\n",
    "<img src=\"https://www.dropbox.com/s/r88cmbmaqyk7hcp/weighting_schemes.png?dl=1\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "</div>\n",
    "* identifies components of the SMART notation: combination in use in a search engine (ddd.qqq)\n",
    "  * e.g.: lnc.ltc\n",
    "  * ```To the legacy of the SMART system belongs the so-called SMART triple notation, a mnemonic scheme for denoting tf-idf weighting variants in the vector space model. The mnemonic for representing a combination of weights takes the form ddd.qqq, where the first three letters represents the term weighting of the collection document vector and the second three letters represents the term weighting for the query document vector. For example, ltc.lnn represents the ltc weighting applied to a collection document and the lnn weighting applied to a query document.```  https://en.wikipedia.org/wiki/SMART_Information_Retrieval_System\n",
    "  \n",
    "* See [Introduction to Information Retrieval](https://nlp.stanford.edu/IR-book/) for more info if you're interested in the topic\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84b261d-517b-4ace-91fd-fb3d18489efe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
